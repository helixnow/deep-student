//! æ¨¡å‹äºŒç®¡çº¿ï¼ˆæ ¸å¿ƒè§£æ/å¯¹è¯ï¼‰
//!
//! ä» llm_manager.rs æ‹†åˆ†çš„æµå¼å’Œéæµå¼å¯¹è¯ç®¡çº¿

use crate::models::{
    AppError, ChatMessage, StandardModel2Output,
    StreamChunk,
};
use crate::providers::ProviderAdapter;
use crate::reasoning_policy::{
    get_passback_policy, requires_reasoning_passback, ReasoningPassbackPolicy,
};
use crate::utils::chat_timing;
use futures_util::StreamExt;
use log::{debug, error, info, warn};
use serde_json::{json, Value};
use std::collections::HashMap;
use tauri::{Emitter, Window};
use url::Url;
use uuid::Uuid;

use super::{
    adapters::get_adapter, parser, ApiConfig, ImagePayload, LLMManager, MergedChatMessage, Result,
};

/// è®¡ç®—æœ‰æ•ˆçš„ max_tokensï¼Œåº”ç”¨ä¾›åº”å•†çº§åˆ«çš„é™åˆ¶
/// DeepSeek ç­‰ä¾›åº”å•†æœ‰ max_tokens ä¸Šé™ï¼ˆå¦‚ 8192ï¼‰ï¼Œè¶…å‡ºä¼šè¿”å› 400 é”™è¯¯
#[inline]
fn effective_max_tokens(max_output_tokens: u32, max_tokens_limit: Option<u32>) -> u32 {
    match max_tokens_limit {
        Some(limit) => max_output_tokens.min(limit),
        None => max_output_tokens,
    }
}

/// â˜… 2026-02-13: ç»Ÿä¸€ LLM è¯·æ±‚ä½“å®¡è®¡æ—¥å¿—
///
/// å¯¹è¯·æ±‚ä½“è¿›è¡Œè„±æ•åä»¥ info çº§åˆ«è¾“å‡ºï¼Œä¾¿äºå®¡è®¡æ‰€æœ‰ LLM è°ƒç”¨ã€‚
/// - base64 å›¾ç‰‡å†…å®¹æ›¿æ¢ä¸º `[base64:é•¿åº¦]`
/// - tools æ•°ç»„ç®€åŒ–ä¸ºå·¥å…·åç§°åˆ—è¡¨ + è®¡æ•°
/// - å…¶ä½™å­—æ®µå®Œæ•´ä¿ç•™
pub(crate) fn sanitize_request_body_for_audit(body: &serde_json::Value) -> serde_json::Value {
    let mut sanitized = body.clone();

    // 1. éšè— messages ä¸­çš„ base64 å›¾ç‰‡
    if let Some(messages) = sanitized.get_mut("messages").and_then(|m| m.as_array_mut()) {
        for message in messages.iter_mut() {
            if let Some(content) = message.get_mut("content").and_then(|c| c.as_array_mut()) {
                for part in content.iter_mut() {
                    if part.get("type").and_then(|t| t.as_str()) == Some("image_url") {
                        if let Some(url_val) =
                            part.get_mut("image_url").and_then(|iu| iu.get_mut("url"))
                        {
                            if let Some(url_str) = url_val.as_str() {
                                if url_str.starts_with("data:") {
                                    let len = url_str.len();
                                    *url_val = json!(format!("[base64:{}bytes]", len));
                                }
                            }
                        }
                    }
                }
            }
        }
    }

    // 2. ç®€åŒ– tools æ•°ç»„ï¼šåªä¿ç•™ function.name + æ€»æ•°
    if let Some(tools) = sanitized.get_mut("tools").and_then(|t| t.as_array_mut()) {
        let count = tools.len();
        let names: Vec<String> = tools
            .iter()
            .filter_map(|t| {
                t.get("function")
                    .and_then(|f| f.get("name"))
                    .and_then(|n| n.as_str())
                    .map(|s| s.to_string())
            })
            .collect();
        // æ›¿æ¢ä¸ºæ‘˜è¦
        *tools = vec![json!({
            "_audit_summary": format!("{} tools: [{}]", count, names.join(", "))
        })];
    }

    sanitized
}

/// è¾“å‡ºå®¡è®¡æ—¥å¿—ï¼ˆinfo çº§åˆ«ï¼‰
pub(crate) fn log_llm_request_audit(tag: &str, url: &str, model: &str, body: &serde_json::Value) {
    let sanitized = sanitize_request_body_for_audit(body);
    match serde_json::to_string_pretty(&sanitized) {
        Ok(pretty) => info!(
            "[LLM_AUDIT:{}] model={} url={}\n{}",
            tag, model, url, pretty
        ),
        Err(e) => warn!(
            "[LLM_AUDIT:{}] model={} url={} (åºåˆ—åŒ–å¤±è´¥: {})",
            tag, model, url, e
        ),
    }
}

/// â˜… 2026-02-14: å®¡è®¡æ—¥å¿— + å°†è„±æ•åçš„ LLM è¯·æ±‚ä½“æ¨é€ç»™å‰ç«¯æ˜¾ç¤ºï¼ˆåˆå¹¶ç‰ˆæœ¬ï¼Œåªåšä¸€æ¬¡ sanitizeï¼‰
///
/// 1. è¾“å‡º info çº§åˆ«å®¡è®¡æ—¥å¿—
/// 2. å¦‚æœ stream_event ä»¥ `chat_v2_event_` å¼€å¤´ï¼Œé€šè¿‡ Tauri äº‹ä»¶æ¨é€ç»™å‰ç«¯
pub(crate) fn log_and_emit_llm_request(
    tag: &str,
    window: &tauri::Window,
    stream_event: &str,
    model: &str,
    url: &str,
    body: &serde_json::Value,
) {
    let sanitized = sanitize_request_body_for_audit(body);

    // 1. å®¡è®¡æ—¥å¿—
    match serde_json::to_string_pretty(&sanitized) {
        Ok(pretty) => info!(
            "[LLM_AUDIT:{}] model={} url={}\n{}",
            tag, model, url, pretty
        ),
        Err(e) => warn!(
            "[LLM_AUDIT:{}] model={} url={} (åºåˆ—åŒ–å¤±è´¥: {})",
            tag, model, url, e
        ),
    }

    // 2. æ¨é€ç»™å‰ç«¯ï¼ˆä»… Chat V2 æµï¼‰
    let prefix = "chat_v2_event_";
    if !stream_event.starts_with(prefix) {
        return;
    }

    let payload = json!({
        "streamEvent": stream_event,
        "model": model,
        "url": url,
        "requestBody": sanitized,
    });

    if let Err(e) = window.emit("chat_v2_llm_request_body", &payload) {
        warn!("[LLM_AUDIT] Failed to emit llm_request_body event: {}", e);
    }
}

impl LLMManager {
    // ç»Ÿä¸€AIæ¥å£å±‚ - æ¨¡å‹äºŒï¼ˆæ ¸å¿ƒè§£æ/å¯¹è¯ï¼‰- æµå¼ç‰ˆæœ¬
    pub async fn call_unified_model_2_stream(
        &self,
        context: &HashMap<String, Value>,
        chat_history: &[ChatMessage],
        subject: &str,
        enable_chain_of_thought: bool,
        enable_thinking: bool,
        task_context: Option<&str>,
        window: Window,
        stream_event: &str,
        _trace_id: Option<&str>,
        disable_tools: bool,
        _max_input_tokens_override: Option<usize>,
        model_override_id: Option<String>,
        temp_override: Option<f32>,
        system_prompt_override: Option<String>,
        top_p_override: Option<f32>,
        frequency_penalty_override: Option<f32>,
        presence_penalty_override: Option<f32>,
        max_output_tokens_override: Option<u32>,
    ) -> Result<StandardModel2Output> {
        info!(
            "è°ƒç”¨ç»Ÿä¸€æ¨¡å‹äºŒæ¥å£(æµå¼): ç§‘ç›®={}, æ€ç»´é“¾={}, override_model={:?}",
            subject, enable_chain_of_thought, model_override_id
        );

        // è®°å½•å¼€å§‹æ—¶é—´å’Œç»Ÿè®¡ä¿¡æ¯
        let _start_instant = std::time::Instant::now();
        let mut request_bytes = 0usize;
        let _response_bytes = 0usize;
        let _chunk_count = 0usize;

        // è·å–æ¨¡å‹é…ç½®ï¼ˆæ”¯æŒ overrideï¼‰ï¼Œæ ¹æ®ä»»åŠ¡ä¸Šä¸‹æ–‡è·¯ç”±
        let task_key = match task_context {
            Some(tc) if tc.contains("review") => "review",
            Some(tc) if tc == "tag_generation" => "tag_generation",
            _ => "default",
        };
        let (config, _cot_by_model) = self
            .select_model_for(
                task_key,
                model_override_id.clone(),
                temp_override,
                top_p_override,
                frequency_penalty_override,
                presence_penalty_override,
                max_output_tokens_override,
            )
            .await?;

        // P1ä¿®å¤ï¼šå›¾ç‰‡ä¸Šä¸‹æ–‡ä¸¥æ ¼æ§åˆ¶ - å›¾ç‰‡ç”±æ¶ˆæ¯çº§å­—æ®µæä¾›ï¼Œç¦ç”¨ä¼šè¯çº§å›é€€
        let images_used_source = "per_message_only".to_string();
        debug!("[LLM] å›¾ç‰‡ä¸Šä¸‹æ–‡ç­–ç•¥: ä»…æ¶ˆæ¯çº§ï¼Œç¦ç”¨ä¼šè¯çº§å›é€€");
        let images_base64: Option<Vec<String>> = None; // ä¼šè¯çº§å›¾ç‰‡å›é€€ç¦ç”¨

        // è®°å½•ä¸€æ¬¡ API è°ƒç”¨ä¸Šä¸‹æ–‡ï¼ˆæ¨¡å‹ä¸å›¾ç‰‡ä½¿ç”¨æƒ…å†µï¼‰ï¼ˆç®€åŒ–ï¼šä»…æ§åˆ¶å°è¾“å‡ºä»¥é¿å… Send çº¦æŸï¼‰
        debug!(
            "[model2_stream] model={} provider={} adapter={} multi={} reasoning={} temp={} cot={} images={{source:{},count:{}}}",
            config.model, config.name, config.model_adapter, config.is_multimodal, config.is_reasoning, config.temperature,
            enable_chain_of_thought, images_used_source, images_base64.as_ref().map(|v| v.len()).unwrap_or(0)
        );

        // ç§»é™¤ä¸Šä¸‹æ–‡é¢„ç®—è£å‰ªï¼šæŒ‰ç…§ç”¨æˆ·å»ºè®®ï¼Œå®Œæ•´ä¿ç•™å†å²ï¼Œç”±å‰ç«¯å±•ç¤ºtokenä¼°ç®—å¹¶ç”±ç”¨æˆ·å†³å®š
        let chat_history = chat_history.to_vec();

        let mut messages = vec![];
        let mut pre_call_injection_texts: Vec<String> = Vec::new();
        if let Some(graph_inject) = Self::build_prefetched_graph_injection(context) {
            debug!(
                "[GraphInject] å·²æ„å»ºå›¾è°±å¬å›æ³¨å…¥å†…å®¹ï¼Œé•¿åº¦ {} å­—ç¬¦",
                graph_inject.chars().count()
            );
            pre_call_injection_texts.push(graph_inject);
        }

        // æ³¨æ„ï¼šCanvas ç¬”è®°ä¸Šä¸‹æ–‡å·²é€šè¿‡ prompt_builder ç»Ÿä¸€æ³¨å…¥åˆ° system_prompt_override ä¸­
        // ä¸å†åœ¨æ­¤å¤„å•ç‹¬æ³¨å…¥ canvas_note_context

        // ğŸ”§ P2é‡æ„ï¼šç§»é™¤æ—§ç‰ˆå›é€€è·¯å¾„ï¼Œæ‰€æœ‰æ³¨å…¥ç»Ÿä¸€ç”± prompt_builder ç®¡ç†
        // Chat V2 Pipeline å§‹ç»ˆä¼ å…¥ prompt_builder ç”Ÿæˆçš„ XML æ ¼å¼ system_prompt
        // å¦‚æœæ²¡æœ‰ä¼ å…¥ï¼Œä½¿ç”¨ç§‘ç›®é»˜è®¤ promptï¼ˆä»…ç”¨äºé Chat V2 çš„æ—§ç‰ˆè°ƒç”¨ï¼‰
        let system_content =
            system_prompt_override.unwrap_or_else(|| self.get_subject_prompt(subject, "model2"));

        // ç¦æ­¢åœ¨æ­¤æ‹¼æ¥ RAG/Memory æ–‡æœ¬ï¼Œç”±å·¥å…·é—­ç¯è´Ÿè´£

        // ä¸å†æ‹¼æ¥ latest_user_query ç­‰ä¼ª system æ³¨å…¥

        // ğŸ”§ P3ä¿®å¤ï¼šç»Ÿä¸€ä½¿ç”¨ system roleï¼Œä¸å†åŒºåˆ†æ¨ç†/éæ¨ç†æ¨¡å‹
        // æ‰€æœ‰å†…å®¹ç”± prompt_builder ç»Ÿä¸€ç®¡ç†ï¼Œç›´æ¥æ”¾å…¥ system message
        messages.push(json!({ "role": "system", "content": system_content }));

        // ğŸ”§ P1ä¿®å¤ï¼šé¢„å¤„ç†æ¶ˆæ¯ï¼Œåˆå¹¶è¿ç»­çš„å·¥å…·è°ƒç”¨
        // OpenAI åè®®æœŸæœ›ï¼šä¸€ä¸ª assistant æ¶ˆæ¯åŒ…å« tool_calls æ•°ç»„ï¼Œç„¶åè·Ÿç€å¤šä¸ª tool æ¶ˆæ¯
        // å½“å‰æ•°æ®æ¨¡å‹æ¯ä¸ªæ¶ˆæ¯åªæœ‰ä¸€ä¸ª tool_callï¼Œéœ€è¦åœ¨åºåˆ—åŒ–æ—¶åˆå¹¶
        let merged_history = Self::merge_consecutive_tool_calls(&chat_history);

        // æ·»åŠ èŠå¤©å†å²ï¼ˆé€æ¡å¤„ç†ç”¨æˆ·å›¾ç‰‡ä¸å·¥å…·è°ƒç”¨æ¶ˆæ¯çš„æ ‡å‡†åŒ–ï¼‰
        for (_index, merged_msg) in merged_history.iter().enumerate() {
            match merged_msg {
                // ğŸ”§ P1ä¿®å¤ï¼šå¤„ç†åˆå¹¶çš„å·¥å…·è°ƒç”¨æ¶ˆæ¯
                // ğŸ”§ Anthropic æœ€ä½³å®è·µï¼šå¿…é¡»ä¿ç•™ thinking_content
                MergedChatMessage::MergedToolCalls {
                    tool_calls,
                    content,
                    thinking_content,
                    thought_signature,
                } => {
                    // ç”Ÿæˆ tool_calls æ•°ç»„
                    let tool_calls_arr: Vec<_> = tool_calls
                        .iter()
                        .map(|tc| {
                            json!({
                                "id": tc.id,
                                "type": "function",
                                "function": {
                                    "name": tc.tool_name,
                                    "arguments": tc.args_json.to_string()
                                }
                            })
                        })
                        .collect();

                    // ğŸ”§ è¾…åŠ©é—­åŒ…ï¼šå°† thought_signature æ³¨å…¥åˆ° assistant æ¶ˆæ¯ä¸­
                    // Gemini 3 è¦æ±‚åœ¨åŒ…å« functionCall çš„ model content ä¸­å›ä¼  thoughtSignature
                    let inject_thought_signature = |msg: &mut Value| {
                        if let Some(ref sig) = thought_signature {
                            msg["thought_signature"] = json!(sig);
                        }
                    };

                    // ğŸ”§ ä½¿ç”¨é€‚é…å™¨ç³»ç»Ÿå¤„ç†å·¥å…·è°ƒç”¨æ¶ˆæ¯æ ¼å¼
                    let has_thinking = thinking_content
                        .as_ref()
                        .map(|s| !s.is_empty())
                        .unwrap_or(false);
                    let adapter =
                        get_adapter(config.provider_type.as_deref(), &config.model_adapter);

                    // å°è¯•ä½¿ç”¨é€‚é…å™¨çš„è‡ªå®šä¹‰æ ¼å¼
                    let tool_calls_json: Vec<Value> = tool_calls_arr.clone();
                    if has_thinking {
                        if let Some(formatted_content) = adapter
                            .format_tool_call_message(&tool_calls_json, thinking_content.as_deref())
                        {
                            let mut msg = json!({
                                "role": "assistant",
                                "content": formatted_content
                            });
                            inject_thought_signature(&mut msg);
                            messages.push(msg);

                            debug!(
                                "[LLMManager] Adapter {} format: {} tool_calls with thinking block (len={})",
                                adapter.id(),
                                tool_calls.len(),
                                thinking_content.as_ref().map(|s| s.len()).unwrap_or(0)
                            );
                        } else if requires_reasoning_passback(&config) {
                            // å…¶ä»–æ¨ç†æ¨¡å‹ï¼ˆDeepSeek ç­‰ï¼‰ï¼šä½¿ç”¨ reasoning_content å­—æ®µ
                            let policy = get_passback_policy(&config);
                            let mut assistant_msg = json!({
                                "role": "assistant",
                                "content": content,
                                "tool_calls": tool_calls_arr
                            });

                            if let Some(ref thinking) = thinking_content {
                                match policy {
                                    ReasoningPassbackPolicy::DeepSeekStyle => {
                                        assistant_msg["reasoning_content"] = json!(thinking);
                                    }
                                    ReasoningPassbackPolicy::ReasoningDetails => {
                                        assistant_msg["reasoning_details"] = json!([{
                                            "type": "thinking",
                                            "text": thinking
                                        }]);
                                    }
                                    ReasoningPassbackPolicy::NoPassback => {}
                                }
                            }

                            inject_thought_signature(&mut assistant_msg);
                            messages.push(assistant_msg);

                            debug!(
                                "[LLMManager] Reasoning model: {} tool_calls with thinking (policy={:?})",
                                tool_calls.len(),
                                policy
                            );
                        } else {
                            // æ— æ€ç»´é“¾æˆ–ä¸éœ€è¦å›ä¼ ï¼ˆé€‚é…å™¨æœªæä¾›è‡ªå®šä¹‰æ ¼å¼ï¼‰
                            let mut msg = json!({
                                "role": "assistant",
                                "content": content,
                                "tool_calls": tool_calls_arr
                            });
                            inject_thought_signature(&mut msg);
                            messages.push(msg);

                            debug!(
                                "[LLMManager] Merged {} tool_calls into single assistant message (no custom format)",
                                tool_calls.len()
                            );
                        }
                    } else {
                        // æ— æ€ç»´é“¾
                        let mut msg = json!({
                            "role": "assistant",
                            "content": content,
                            "tool_calls": tool_calls_arr
                        });
                        inject_thought_signature(&mut msg);
                        messages.push(msg);

                        debug!(
                            "[LLMManager] Merged {} tool_calls into single assistant message",
                            tool_calls.len()
                        );
                    }
                }

                MergedChatMessage::Regular(msg) => {
                    // å¤„ç†ç”¨æˆ·æ¶ˆæ¯
                    if msg.role == "user" {
                        // â˜… æ–‡æ¡£25ï¼šä¼˜å…ˆæ£€æŸ¥ multimodal_contentï¼ˆå›¾æ–‡äº¤æ›¿æ¨¡å¼ï¼‰
                        if config.is_multimodal
                            && msg
                                .multimodal_content
                                .as_ref()
                                .map(|v| !v.is_empty())
                                .unwrap_or(false)
                        {
                            // ä½¿ç”¨ multimodal_content æ„å»ºäº¤æ›¿çš„ content æ•°ç»„
                            // â˜… P0 æ¶æ„æ”¹é€ ï¼šç§»é™¤å‘é€æ—¶å‹ç¼©ï¼Œå›¾ç‰‡å·²åœ¨é¢„å¤„ç†é˜¶æ®µå‹ç¼©å®Œæˆ
                            // æ³¨æ„ï¼švision_quality å‚æ•°ä¸å†ä½¿ç”¨ï¼Œä¿ç•™ä»£ç ä»¥ä¾¿è°ƒè¯•
                            let _vq = context
                                .get("vision_quality")
                                .and_then(|v| v.as_str())
                                .unwrap_or("");

                            let empty_multimodal: Vec<crate::models::MultimodalContentPart> =
                                Vec::new();
                            let content: Vec<serde_json::Value> = msg.multimodal_content.as_ref().unwrap_or(&empty_multimodal)
                                .iter()
                                .map(|part| {
                                    use crate::models::MultimodalContentPart;
                                    match part {
                                        MultimodalContentPart::Text { text } => {
                                            json!({
                                                "type": "text",
                                                "text": text
                                            })
                                        }
                                        MultimodalContentPart::ImageUrl { media_type, base64 } => {
                                            // â˜… P0 æ¶æ„æ”¹é€ ï¼šç›´æ¥ä½¿ç”¨é¢„å¤„ç†åçš„å›¾ç‰‡ï¼Œä¸å†å®æ—¶å‹ç¼©
                                            // é¢„å¤„ç†é˜¶æ®µï¼ˆpdf_processing_service.rsï¼‰å·²ç»å®Œæˆå‹ç¼©
                                            json!({
                                                "type": "image_url",
                                                "image_url": { "url": format!("data:{};base64,{}", media_type, base64) }
                                            })
                                        }
                                    }
                                })
                                .collect();

                            info!(
                                "[LLMManager] Using multimodal_content mode with {} parts (interleaved text/image)",
                                content.len()
                            );

                            messages.push(json!({
                                "role": msg.role,
                                "content": content
                            }));
                        } else {
                            // ä¼ ç»Ÿæ¨¡å¼ï¼šä½¿ç”¨ content + image_base64
                            let mut message_content = msg.content.clone();

                            // å¦‚æœæœ‰æ–‡æ¡£é™„ä»¶ï¼Œå°†å…¶å†…å®¹æ·»åŠ åˆ°æ¶ˆæ¯ä¸­
                            if let Some(doc_attachments) = &msg.doc_attachments {
                                if !doc_attachments.is_empty() {
                                    message_content.push_str("\n\n--- é™„ä»¶å†…å®¹ ---");
                                    for doc in doc_attachments {
                                        message_content
                                            .push_str(&format!("\n\nã€æ–‡æ¡£: {}ã€‘", doc.name));
                                        if let Some(text_content) = &doc.text_content {
                                            message_content
                                                .push_str(&format!("\n{}", text_content));
                                        }
                                    }
                                }
                            }

                            // ğŸ¯ æ”¹é€ ï¼šæ¯æ¡ç”¨æˆ·æ¶ˆæ¯æºå¸¦å„è‡ªçš„å›¾ç‰‡
                            if config.is_multimodal
                                && msg
                                    .image_base64
                                    .as_ref()
                                    .map(|v| !v.is_empty())
                                    .unwrap_or(false)
                            {
                                let mut content = vec![json!({
                                    "type": "text",
                                    "text": message_content
                                })];

                                if let Some(images) = &msg.image_base64 {
                                    // â˜… P0 æ¶æ„æ”¹é€ ï¼šç§»é™¤å‘é€æ—¶å‹ç¼©ï¼Œå›¾ç‰‡å·²åœ¨é¢„å¤„ç†é˜¶æ®µå‹ç¼©å®Œæˆ
                                    for image_base64 in images {
                                        // ç›´æ¥ä½¿ç”¨é¢„å¤„ç†åçš„å›¾ç‰‡ï¼Œä¸å†å®æ—¶å‹ç¼©
                                        let image_format =
                                            Self::detect_image_format_from_base64(image_base64);
                                        content.push(json!({
                                            "type": "image_url",
                                            "image_url": { "url": format!("data:image/{};base64,{}", image_format, image_base64) }
                                        }));
                                    }
                                }

                                messages.push(json!({
                                    "role": msg.role,
                                    "content": content
                                }));
                            } else {
                                messages.push(json!({
                                    "role": msg.role,
                                    "content": message_content
                                }));
                            }
                        }
                    } else if msg.role == "assistant" {
                        // æ™®é€š assistant æ¶ˆæ¯ï¼ˆæ²¡æœ‰å·¥å…·è°ƒç”¨ï¼‰
                        // ğŸ”§ ä½¿ç”¨é€‚é…å™¨ç³»ç»Ÿå¤„ç†å†å²æ¶ˆæ¯æ ¼å¼
                        let has_thinking = msg
                            .thinking_content
                            .as_ref()
                            .map(|s| !s.is_empty())
                            .unwrap_or(false);
                        let adapter =
                            get_adapter(config.provider_type.as_deref(), &config.model_adapter);

                        if has_thinking && adapter.requires_thinking_in_history(&config) {
                            // é€‚é…å™¨è¦æ±‚åœ¨å†å²æ¶ˆæ¯ä¸­ä¿ç•™ thinking å—
                            // ä½¿ç”¨é€‚é…å™¨çš„è‡ªå®šä¹‰æ ¼å¼ï¼ˆå¦‚ Anthropicï¼‰
                            let empty_tool_calls: Vec<Value> = vec![];
                            if let Some(formatted_content) = adapter.format_tool_call_message(
                                &empty_tool_calls,
                                msg.thinking_content.as_deref(),
                            ) {
                                // é€‚é…å™¨æä¾›äº†è‡ªå®šä¹‰æ ¼å¼ï¼Œæ·»åŠ  text å—
                                let mut content_blocks: Vec<Value> =
                                    if let Some(arr) = formatted_content.as_array() {
                                        arr.clone()
                                    } else {
                                        vec![formatted_content]
                                    };
                                if !msg.content.is_empty() {
                                    content_blocks.push(json!({
                                        "type": "text",
                                        "text": msg.content
                                    }));
                                }
                                messages.push(json!({
                                    "role": "assistant",
                                    "content": content_blocks
                                }));
                            } else {
                                // é€‚é…å™¨æœªæä¾›è‡ªå®šä¹‰æ ¼å¼ï¼Œä½¿ç”¨é€šç”¨æ ¼å¼
                                let mut content_blocks = Vec::new();
                                if let Some(ref thinking) = msg.thinking_content {
                                    content_blocks.push(json!({
                                        "type": "thinking",
                                        "thinking": thinking
                                    }));
                                }
                                if !msg.content.is_empty() {
                                    content_blocks.push(json!({
                                        "type": "text",
                                        "text": msg.content
                                    }));
                                }
                                messages.push(json!({
                                    "role": "assistant",
                                    "content": content_blocks
                                }));
                            }
                        } else if has_thinking && requires_reasoning_passback(&config) {
                            // ğŸ”§ æ€ç»´é“¾å›ä¼ ç­–ç•¥ï¼ˆæ–‡æ¡£ 29 ç¬¬ 7 èŠ‚ï¼‰
                            // ä½¿ç”¨ç»Ÿä¸€çš„ reasoning_policy æ¨¡å—åˆ¤æ–­æ˜¯å¦éœ€è¦å›ä¼ 
                            let policy = get_passback_policy(&config);
                            let mut assistant_msg = json!({
                                "role": "assistant",
                                "content": msg.content
                            });
                            if let Some(ref thinking) = msg.thinking_content {
                                match policy {
                                    ReasoningPassbackPolicy::DeepSeekStyle => {
                                        // DeepSeek/xAI/Perplexity ç­‰ä½¿ç”¨ reasoning_content å­—ç¬¦ä¸²
                                        assistant_msg["reasoning_content"] = json!(thinking);
                                    }
                                    ReasoningPassbackPolicy::ReasoningDetails => {
                                        // Gemini 3/OpenAI o1 ç­‰ä½¿ç”¨ reasoning_details æ•°ç»„
                                        // å¯¹äº Gemini 3ï¼Œéœ€è¦åŒ…å« thoughtSignatureï¼ˆå·¥å…·è°ƒç”¨å¿…éœ€ï¼‰
                                        let mut detail = json!({
                                            "type": "thinking",
                                            "text": thinking
                                        });
                                        // å¦‚æœå­˜åœ¨ thought_signatureï¼Œæ·»åŠ åˆ° detail ä¸­
                                        if let Some(ref signature) = msg.thought_signature {
                                            detail["signature"] = json!(signature);
                                        }
                                        assistant_msg["reasoning_details"] = json!([detail]);
                                    }
                                    ReasoningPassbackPolicy::NoPassback => {
                                        // ä¸åº”è¯¥åˆ°è¾¾è¿™é‡Œï¼Œä½†ä¿æŒå®‰å…¨
                                    }
                                }
                            }
                            messages.push(assistant_msg);
                        } else {
                            // å…¶ä»–æ¨¡å‹ï¼šåªä¼ é€’ contentï¼ˆthinking ä¸éœ€è¦åœ¨å†å²ä¸­ä¼ é€’ï¼‰
                            messages.push(json!({
                                "role": "assistant",
                                "content": msg.content
                            }));
                        }
                    } else if msg.role == "tool" {
                        // æ ‡å‡†åŒ–ï¼šå·¥å…·ç»“æœæ¶ˆæ¯å¿…é¡»åŒ…å« tool_call_id ä»¥å…³è”åˆ°ä¸Šä¸€æ¡assistantçš„tool_calls
                        if let Some(tr) = &msg.tool_result {
                            messages.push(json!({
                                "role": "tool",
                                "tool_call_id": tr.call_id,
                                // æŒ‰OpenAIè§„èŒƒï¼Œcontentä¸ºå­—ç¬¦ä¸²ï¼ˆé€šå¸¸ä¸ºJSONå­—ç¬¦ä¸²ï¼‰
                                "content": msg.content
                            }));
                        } else {
                            // é¿å…å‘é€ä¸åˆæ³•çš„toolæ¶ˆæ¯ï¼ˆç¼ºå°‘tool_call_idï¼‰ï¼Œé™çº§ä¸ºassistantæ–‡æœ¬ä»¥ä¿è¯ä¸æŠ¥é”™
                            messages.push(json!({
                                "role": "assistant",
                                "content": msg.content
                            }));
                        }
                    }
                }
            }
        }

        // ğŸ”§ é˜²å¾¡æ€§åˆå¹¶ï¼šè¿ç»­ user æ¶ˆæ¯åˆå¹¶ï¼Œé¿å…éƒ¨åˆ† APIï¼ˆAnthropic/ERNIEï¼‰æŠ¥é”™
        Self::merge_consecutive_user_messages(&mut messages);

        // è¿‘ä¼¼è¾“å…¥tokenç»Ÿè®¡ï¼ˆç”¨äºç”¨é‡/äº‹ä»¶ï¼‰
        let _approx_tokens_in = {
            let mut s = 0usize;
            // ä½¿ç”¨ system_content ä¼°ç®—ç³»ç»Ÿæç¤ºçš„ token æ•°é‡
            s += crate::utils::token_budget::estimate_tokens(&system_content);
            if !context.is_empty() {
                for (k, v) in context {
                    let _ = k;
                    s += crate::utils::token_budget::estimate_tokens(&v.to_string());
                }
            }
            for m in &chat_history {
                s += crate::utils::token_budget::estimate_tokens(&m.content);
            }
            s
        };

        let mut request_body = json!({
            "model": config.model,
            "messages": messages,
            "stream": true
        });

        // ğŸ†• åº”ç”¨æ¨ç†é…ç½®ï¼Œä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„enable_thinkingå‚æ•°
        Self::apply_reasoning_config(&mut request_body, &config, Some(enable_thinking));

        // æ£€æŸ¥æ˜¯å¦å¯ç”¨å·¥å…·ï¼ˆå…¨å±€ + æ¨¡å‹èƒ½åŠ›ï¼‰
        let mut tools_enabled = self
            .db
            .get_setting("tools.enabled")
            .ok()
            .flatten()
            .map(|v| v.to_lowercase())
            .map(|v| v != "0" && v != "false")
            .unwrap_or(true); // é»˜è®¤å¯ç”¨
        if disable_tools {
            tools_enabled = false;
        }

        // ğŸ†• æ£€æŸ¥ context ä¸­æ˜¯å¦æœ‰è‡ªå®šä¹‰å·¥å…·ï¼ˆç”¨äº Pipeline æ³¨å…¥ Canvas ç­‰å·¥å…·ï¼‰
        // å³ä½¿ disable_tools = trueï¼Œä¹Ÿå…è®¸é€šè¿‡ context æ³¨å…¥å·¥å…· schema
        // è¿™æ · Pipeline å¯ä»¥æ¥ç®¡å·¥å…·æ‰§è¡Œï¼Œä½† LLM ä»ç„¶çŸ¥é“æœ‰å“ªäº›å·¥å…·å¯ç”¨
        let custom_tools = context
            .get("custom_tools")
            .and_then(|v| v.as_array())
            .cloned();
        let has_custom_tools = custom_tools
            .as_ref()
            .map(|arr| !arr.is_empty())
            .unwrap_or(false);

        // ğŸ” è°ƒè¯•æ—¥å¿—ï¼šæ£€æŸ¥ custom_tools åœ¨ LLM è°ƒç”¨æ—¶çš„çŠ¶æ€
        debug!(
            "[LLM] custom_tools check: has_custom_tools={}, count={}, disable_tools={}, tools_enabled={}, supports_tools={}",
            has_custom_tools,
            custom_tools.as_ref().map(|arr| arr.len()).unwrap_or(0),
            disable_tools,
            tools_enabled,
            config.supports_tools
        );
        debug!(
            "[LLM] custom_tools check: has_custom_tools={}, count={}, disable_tools={}, tools_enabled={}, supports_tools={}",
            has_custom_tools,
            custom_tools.as_ref().map(|arr| arr.len()).unwrap_or(0),
            disable_tools,
            tools_enabled,
            config.supports_tools
        );

        if has_custom_tools && config.supports_tools {
            // ä½¿ç”¨è‡ªå®šä¹‰å·¥å…·ï¼ˆPipeline æ¥ç®¡æ‰§è¡Œï¼Œä½†éœ€è¦ LLM çŸ¥é“å·¥å…· schemaï¼‰
            let tools = Value::Array(custom_tools.unwrap_or_default());
            debug!(
                "[LLM] ä½¿ç”¨ context æ³¨å…¥çš„è‡ªå®šä¹‰å·¥å…·ï¼Œæ•°é‡: {}",
                tools.as_array().map(|a| a.len()).unwrap_or(0)
            );
            request_body["tools"] = tools;
            request_body["tool_choice"] = json!("auto");
        } else if !disable_tools && tools_enabled && config.supports_tools {
            // æ„å»ºå·¥å…·åˆ—è¡¨ï¼ŒåŒ…å«æœ¬åœ°å·¥å…·å’Œ MCP å·¥å…·
            let tools = self.build_tools_with_mcp(&window).await;

            // åªæœ‰åœ¨å·¥å…·åˆ—è¡¨éç©ºæ—¶æ‰è®¾ç½® tools å’Œ tool_choice
            if tools.as_array().map(|arr| !arr.is_empty()).unwrap_or(false) {
                request_body["tools"] = tools;
                request_body["tool_choice"] = json!("auto");
            } else {
                warn!("[LLM] å·¥å…·åˆ—è¡¨ä¸ºç©ºï¼Œè·³è¿‡ tool_choice è®¾ç½®");
            }

            // è¿è¡Œæ—¶äº’æ–¥ä¿®æ­£ï¼šæŸäº›æ¨¡å‹ï¼ˆå¦‚ DeepSeek V3.1ï¼‰ä½¿ç”¨å‡½æ•°è°ƒç”¨æ—¶éœ€ç¦ç”¨æ€ç»´æ¨¡å¼å­—æ®µ
            // ä½¿ç”¨é€‚é…å™¨ç³»ç»Ÿåˆ¤æ–­æ˜¯å¦éœ€è¦ç¦ç”¨
            let adapter = get_adapter(config.provider_type.as_deref(), &config.model_adapter);
            if let Some(body_map) = request_body.as_object() {
                if adapter.should_disable_thinking_for_tools(&config, body_map) {
                    if let Some(m) = request_body.as_object_mut() {
                        m.remove("enable_thinking");
                        m.remove("include_thoughts");
                        m.remove("thinking_budget");
                        m.remove("thinking"); // V3.2 æ ¼å¼
                        debug!(
                            "[LLMManager] Adapter {} disabled thinking for tool calls",
                            adapter.id()
                        );
                    }
                }
            }
        } else {
            if !config.supports_tools {
                debug!("è·³è¿‡å·¥å…·æ³¨å…¥ï¼šæ¨¡å‹ä¸æ”¯æŒå‡½æ•°è°ƒç”¨ (supports_tools=false)");
                // ä¸ºä¸æ”¯æŒå·¥å…·çš„æ¨¡å‹ä¸»åŠ¨è°ƒç”¨RAG/æ™ºèƒ½è®°å¿†å·¥å…·å¹¶æ³¨å…¥ä¸Šä¸‹æ–‡ï¼ˆnotes assistantç¦ç”¨æ­¤å›é€€ï¼‰
                // å·¥å…·è°ƒç”¨ç¦ç”¨ä¸åº”å½±å“æ–‡æœ¬é™çº§æ³¨å…¥ï¼›ä»…åœ¨æ˜¾å¼çš„å—é™é˜¶æ®µæ‰è·³è¿‡
                // ç»Ÿä¸€ç®¡çº¿å·²åœ¨ä¸Šæ¸¸æ§åˆ¶ï¼šnotes/summary/summary_request æ‰é˜»æ–­
                {
                    info!("[Fallback] æ¨¡å‹ä¸æ”¯æŒå·¥å…·è°ƒç”¨ï¼Œå¯åŠ¨é™çº§æ³¨å…¥æ¨¡å¼");
                    let mut inject_texts = Vec::new();

                    let mut reuse_prefetched_web_search = false;
                    if let Some(prefetched) = context
                        .get("prefetched_web_search_sources")
                        .and_then(|v| v.as_array())
                    {
                        // å…¼å®¹ä¸¤ç§æ ¼å¼ï¼š
                        // - RagSourceInfo: document_id, file_name, chunk_text
                        // - SourceInfo (Chat V2): title, url, snippet
                        let mut rows = Vec::new();
                        for (idx, item) in prefetched.iter().enumerate() {
                            // å°è¯•è·å–æ ‡é¢˜ï¼šfile_name æˆ– title
                            let title = item
                                .get("file_name")
                                .or_else(|| item.get("title"))
                                .and_then(|v| v.as_str())
                                .unwrap_or("æœç´¢ç»“æœ");
                            // å°è¯•è·å–å†…å®¹ï¼šchunk_text æˆ– snippet
                            let content = item
                                .get("chunk_text")
                                .or_else(|| item.get("snippet"))
                                .and_then(|v| v.as_str())
                                .unwrap_or("");
                            // å°è¯•è·å– URLï¼šdocument_id æˆ– url
                            let url = item
                                .get("document_id")
                                .or_else(|| item.get("url"))
                                .and_then(|v| v.as_str())
                                .unwrap_or("");

                            if !content.trim().is_empty() {
                                if !url.is_empty() {
                                    rows.push(format!(
                                        "[å¤–éƒ¨æœç´¢ {}] {}\n{}\nURL: {}",
                                        idx + 1,
                                        title,
                                        content,
                                        url
                                    ));
                                } else {
                                    rows.push(format!(
                                        "[å¤–éƒ¨æœç´¢ {}] {}\n{}",
                                        idx + 1,
                                        title,
                                        content
                                    ));
                                }
                            }
                            if rows.len() >= 5 {
                                break;
                            }
                        }
                        if !rows.is_empty() {
                            reuse_prefetched_web_search = true;
                            debug!(
                                "[Fallback] å¤ç”¨é¢„å–çš„ web_search ç»“æœï¼Œå…± {} æ¡",
                                rows.len()
                            );
                            let joined = format!("ã€å¤–éƒ¨æœç´¢ç»“æœã€‘\n{}\n\n", rows.join("\n\n"));
                            inject_texts.push(joined);
                        }
                    }

                    let mcp_client = None;
                    // ğŸ”§ P1-36: ä¼˜å…ˆè¯»å–ç»Ÿä¸€ç®¡çº¿æ³¨å…¥çš„ memory_enabled
                    let memory_enabled_from_context =
                        context.get("memory_enabled").and_then(|v| v.as_bool());
                    let tool_ctx = crate::tools::ToolContext {
                        db: Some(&self.db),
                        mcp_client,
                        supports_tools: false, // ä¸“é—¨ä¸ºé™çº§æ³¨å…¥åœºæ™¯
                        window: Some(&window),
                        stream_event: Some(stream_event),
                        stage: Some("fallback"),
                        memory_enabled: memory_enabled_from_context,
                        llm_manager: None, // fallback åœºæ™¯ä¸éœ€è¦é‡æ’å™¨
                    };

                    if let Some(last_user_msg) =
                        chat_history.iter().filter(|m| m.role == "user").last()
                    {
                        let memory_enabled_effective = memory_enabled_from_context.unwrap_or(true);
                        if memory_enabled_effective {
                            let _ = window.emit(
                                &format!("{}_memory_sources", stream_event),
                                &serde_json::json!({"stage":"disabled"}),
                            );
                        }

                        let rag_enabled = context
                            .get("rag_enabled")
                            .and_then(|v| v.as_bool())
                            .unwrap_or(true);
                        let _rag_library_ids: Option<Vec<String>> = context
                            .get("rag_library_ids")
                            .and_then(|v| v.as_array())
                            .map(|arr| {
                                arr.iter()
                                    .filter_map(|v| v.as_str().map(|s| s.to_string()))
                                    .collect::<Vec<String>>()
                            })
                            .filter(|v| !v.is_empty());
                        let _rag_note_subjects: Option<Vec<String>> = context
                            .get("rag_note_subjects")
                            .and_then(|v| v.as_array())
                            .map(|arr| {
                                arr.iter()
                                    .filter_map(|v| v.as_str().map(|s| s.to_string()))
                                    .collect::<Vec<String>>()
                            })
                            .filter(|v| !v.is_empty());
                        if rag_enabled {
                            // Legacy RAG removed; VFS RAG is used via builtin:rag_search tool
                            debug!(
                                "[Fallback] Legacy RAG removed, skipping knowledge base injection"
                            );
                        } else {
                            debug!("[Fallback] RAG å·²å…³é—­ï¼Œè·³è¿‡çŸ¥è¯†åº“æ³¨å…¥");
                        }
                        // è°ƒç”¨ WebSearch å·¥å…·ç”Ÿæˆæ³¨å…¥æ–‡æœ¬
                        if !reuse_prefetched_web_search {
                            let web_registry =
                                crate::tools::ToolRegistry::new_with(vec![std::sync::Arc::new(
                                    crate::tools::WebSearchTool,
                                )]);
                            let web_args = json!({
                                "query": last_user_msg.content,
                                "top_k": 3
                            });
                            debug!(
                                "[Fallback] å‡†å¤‡è°ƒç”¨ web_search å·¥å…·ï¼ŒæŸ¥è¯¢: {}",
                                &last_user_msg.content
                            );
                            let (_ok, _data, _err, _usage, _citations, web_inject_text) =
                                web_registry
                                    .call_tool("web_search", &web_args, &tool_ctx)
                                    .await;
                            debug!("[Fallback] web_search è°ƒç”¨å®Œæˆï¼Œok={}, citationsæ•°é‡={}, inject_texté•¿åº¦={}",
                                _ok,
                                _citations.as_ref().map(|c| c.len()).unwrap_or(0),
                                web_inject_text.as_ref().map(|t| t.len()).unwrap_or(0)
                            );
                            if let Some(ref err) = _err {
                                warn!("[Fallback] web_search è¿”å›é”™è¯¯: {}", err);
                            }
                            if let Some(text) = web_inject_text {
                                debug!(
                                    "[Fallback] å°† web_search æ³¨å…¥æ–‡æœ¬åŠ å…¥é˜Ÿåˆ—ï¼Œé•¿åº¦: {} å­—ç¬¦",
                                    text.len()
                                );
                                inject_texts.push(text);
                            } else {
                                warn!("[Fallback] web_search è¿”å›çš„ inject_text ä¸º Noneï¼");
                            }
                        }
                    }

                    if !inject_texts.is_empty() {
                        debug!(
                            "[Fallback] æ”¶é›†æ³¨å…¥æ–‡æœ¬ï¼Œå…± {} æ®µï¼Œç¨åç»Ÿä¸€æ³¨å…¥ç³»ç»Ÿæç¤º",
                            inject_texts.len()
                        );
                        pre_call_injection_texts.extend(inject_texts);
                    } else {
                        warn!("[Fallback] inject_texts é˜Ÿåˆ—ä¸ºç©ºï¼Œæ²¡æœ‰ä»»ä½•å†…å®¹å¯æ³¨å…¥ï¼");
                    }
                } // end disable_tools guard
            }
        }

        if let Some(inject_content) = Self::coalesce_injection_texts(&pre_call_injection_texts) {
            Self::append_injection_to_system_message(&mut messages, &inject_content);
        }

        // æ³¨å…¥é˜¶æ®µå¯èƒ½ä¿®æ”¹ messagesï¼Œæ­¤å¤„ç¡®ä¿è¯·æ±‚ä½“æºå¸¦æœ€æ–°å‰¯æœ¬
        request_body["messages"] = serde_json::Value::Array(messages.clone());

        // è®¡ç®—è¯·æ±‚ä½“å¤§å°
        request_bytes = serde_json::to_string(&request_body)
            .unwrap_or_default()
            .len();

        // ç®€åŒ–ï¼šä¸å†åœ¨æ­¤å¤„ä¼°ç®—è¾“å…¥token

        // æ ¹æ®æ¨¡å‹é€‚é…å™¨ç±»å‹å’Œæ˜¯å¦ä¸ºæ¨ç†æ¨¡å‹è®¾ç½®ä¸åŒçš„å‚æ•°
        if cfg!(debug_assertions) {
            // debug removed: adapter type & reasoning flag
        }

        if config.is_reasoning {
            // ä½¿ç”¨é…ç½®åŒ–çš„ max_tokens_limit é™åˆ¶ max_completion_tokens
            let max_tokens = match config.max_tokens_limit {
                Some(limit) => config.max_output_tokens.min(limit),
                None => config.max_output_tokens,
            };
            match config.model_adapter.as_str() {
                _ => {
                    request_body["max_completion_tokens"] = json!(max_tokens);
                }
            }
        } else {
            // éæ¨ç†æ¨¡å‹èµ°é€šç”¨å‚æ•°
            // ä½¿ç”¨é…ç½®åŒ–çš„ max_tokens_limit é™åˆ¶ max_tokens
            let max_tokens = match config.max_tokens_limit {
                Some(limit) => config.max_output_tokens.min(limit),
                None => config.max_output_tokens,
            };
            request_body["max_tokens"] = json!(max_tokens);
            request_body["temperature"] = json!(config.temperature);
            // å…³é”®ï¼šå¦‚æœæ¨¡å‹æ˜¯éæ¨ç†æ¨¡å‹ï¼Œå³ä½¿å‰ç«¯è¯·æ±‚äº†æ€ç»´é“¾ï¼Œ
            // ä¹Ÿä¸è¦å‘APIå‘é€ç‰¹å®šäºæ€ç»´é“¾çš„å‚æ•°ï¼Œé™¤éè¯¥æ¨¡å‹æ˜ç¡®æ”¯æŒã€‚
            // å¯¹äºé€šç”¨æ¨¡å‹ï¼Œé€šå¸¸ä¸éœ€è¦ä¸º"æ€ç»´é“¾"ä¼ é€’ç‰¹æ®Šå‚æ•°ï¼Œæ¨¡å‹ä¼šè‡ªç„¶åœ°æŒ‰æŒ‡ä»¤å›å¤ã€‚
            // å¦‚æœ enable_chain_of_thought å¯¹éæ¨ç†æ¨¡å‹æ„å‘³ç€ä¸åŒçš„å¤„ç†ï¼ˆä¾‹å¦‚ï¼Œæ›´è¯¦ç»†çš„å›å¤ï¼‰ï¼Œ
            // è¿™é‡Œçš„é€»è¾‘å¯èƒ½éœ€è¦è°ƒæ•´ï¼Œä½†é€šå¸¸æ˜¯Promptå·¥ç¨‹çš„ä¸€éƒ¨åˆ†ï¼Œè€Œä¸æ˜¯APIå‚æ•°ã€‚
            if enable_chain_of_thought {
                warn!(
                    "å‰ç«¯ä¸ºéæ¨ç†æ¨¡å‹ {} è¯·æ±‚äº†æ€ç»´é“¾ã€‚é€šå¸¸è¿™ç”±Promptæ§åˆ¶ï¼Œè€Œéç‰¹å®šAPIå‚æ•°ã€‚",
                    config.model
                );
            }
        }
        // ğŸ†• æ£€æµ‹åˆæˆçš„ load_skills å·¥å…·äº¤äº’æ˜¯å¦å‡ºç°åœ¨è¯·æ±‚æ¶ˆæ¯ä¸­
        {
            let synthetic_count = messages.iter().filter(|m| {
                // æ£€æµ‹ assistant æ¶ˆæ¯ä¸­åŒ…å« load_skills tool_call
                if let Some(tool_calls) = m.get("tool_calls").and_then(|v| v.as_array()) {
                    tool_calls.iter().any(|tc| {
                        tc.get("function")
                            .and_then(|f| f.get("name"))
                            .and_then(|n| n.as_str())
                            .map_or(false, |name| name == "load_skills")
                    })
                } else if m.get("role").and_then(|r| r.as_str()) == Some("tool") {
                    // æ£€æµ‹ tool æ¶ˆæ¯ä¸­åŒ…å« skill_loaded æ ‡è®°
                    m.get("content")
                        .and_then(|c| c.as_str())
                        .map_or(false, |c| c.contains("<skill_loaded"))
                } else {
                    false
                }
            }).count();
            if synthetic_count > 0 {
                info!(
                    "[LLM_AUDIT] è¯·æ±‚ä½“åŒ…å« {} æ¡åˆæˆ load_skills å·¥å…·æ¶ˆæ¯ï¼ˆæ€»æ¶ˆæ¯æ•°: {}ï¼‰",
                    synthetic_count,
                    messages.len()
                );
            }
        }

        // è¾“å‡ºå®Œæ•´è¯·æ±‚ä½“ç”¨äºè°ƒè¯•ï¼ˆéšè—å›¾ç‰‡å†…å®¹ä¿æŠ¤éšç§ï¼‰
        let debug_body = {
            let mut debug = request_body.clone();
            if let Some(messages) = debug["messages"].as_array_mut() {
                for message in messages {
                    if let Some(content) = message["content"].as_array_mut() {
                        for part in content {
                            if part["type"] == "image_url" {
                                part["image_url"]["url"] = json!("data:image/jpeg;base64,[hidden]");
                            }
                        }
                    }
                }
            }
            debug
        };
        debug!("[LLM_REVIEW_DEBUG] ==> å®Œæ•´è¯·æ±‚ä½“å¼€å§‹ <==");
        debug!(
            "{}",
            serde_json::to_string_pretty(&debug_body).unwrap_or_default()
        );
        debug!("[LLM_REVIEW_DEBUG] ==> å®Œæ•´è¯·æ±‚ä½“ç»“æŸ <==");

        // è®°å½•è¯·æ±‚ä½“å¤§å°ä¸èµ·å§‹æ—¶é—´ï¼ˆç®€åŒ–ï¼‰
        let request_json_str = serde_json::to_string(&request_body).unwrap_or_default();
        let request_bytes = request_json_str.len();
        let start_instant = std::time::Instant::now();

        // Provider é€‚é…ï¼šæ„å»ºè¯·æ±‚
        let adapter: Box<dyn ProviderAdapter> = if self.should_use_openai_responses(&config) {
            Box::new(crate::providers::OpenAIResponsesAdapter)
        } else {
            match config.model_adapter.as_str() {
                "google" | "gemini" => Box::new(crate::providers::GeminiAdapter::new()),
                "anthropic" | "claude" => Box::new(crate::providers::AnthropicAdapter::new()),
                _ => Box::new(crate::providers::OpenAIAdapter),
            }
        };
        let preq = adapter
            .build_request(
                &config.base_url,
                &config.api_key,
                &config.model,
                &request_body,
            )
            .map_err(|e| Self::provider_error("å¯¹è¯è¯·æ±‚æ„å»ºå¤±è´¥", e))?;

        // â˜… ä½¿ç”¨ preq.bodyï¼ˆé€‚é…å™¨è½¬æ¢åçš„å®é™…è¯·æ±‚ä½“ï¼‰è€Œé request_bodyï¼ˆè½¬æ¢å‰ï¼‰ï¼Œ
        // ç¡®ä¿ Anthropic/Gemini ç­‰é OpenAI æä¾›å•†çš„é¢„è§ˆä¸å®é™…å‘é€å†…å®¹ä¸€è‡´
        log_and_emit_llm_request(
            "CHAT_STREAM",
            &window,
            stream_event,
            &config.model,
            &preq.url,
            &preq.body,
        );

        // å‘å‡ºå¼€å§‹äº‹ä»¶
        let request_id = Uuid::new_v4().to_string();
        if let Err(e) = window.emit(
            &format!("{}_start", stream_event),
            &json!({
                "id": request_id,
                "model": config.model,
                "request_bytes": request_bytes
            }),
        ) {
            warn!("å‘é€å¼€å§‹äº‹ä»¶å¤±è´¥: {}", e);
        }

        // ERR-01 ä¿®å¤ï¼šHTTP é”™è¯¯ç åŒºåˆ†å¤„ç†ä¸æŒ‡æ•°é€€é¿é‡è¯•
        const MAX_RETRIES: u32 = 3;
        const INITIAL_BACKOFF_MS: u64 = 1000;
        let mut retry_count = 0u32;
        let mut backoff_ms = INITIAL_BACKOFF_MS;

        let response = loop {
            // æ¯æ¬¡é‡è¯•éƒ½éœ€è¦é‡æ–°æ„å»º request_builderï¼ˆå› ä¸º send() ä¼šæ¶ˆè€—å®ƒï¼‰
            let mut request_builder = self.client
                .post(&preq.url)
                .header("Accept", "text/event-stream, application/json, text/plain, */*")
                .header("Accept-Encoding", "identity")  // ç¦ç”¨å‹ç¼©ï¼Œé¿å…äºŒè¿›åˆ¶å“åº”
                .header("Accept-Language", "zh-CN,zh;q=0.9,en;q=0.8")
                // Connection å¤´ç”± reqwest è‡ªåŠ¨ç®¡ç†ï¼šHTTP/1.1 ä½¿ç”¨ keep-aliveï¼ŒHTTP/2 ä½¿ç”¨å¤šè·¯å¤ç”¨
                .header("User-Agent", "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36");
            for (k, v) in &preq.headers {
                request_builder = request_builder.header(k.clone(), v.clone());
            }
            if let Ok(parsed_url) = Url::parse(&config.base_url) {
                if (parsed_url.scheme() == "http" || parsed_url.scheme() == "https")
                    && parsed_url.host_str().is_some()
                {
                    let origin_val = format!(
                        "{}://{}",
                        parsed_url.scheme(),
                        parsed_url.host_str().unwrap_or_default()
                    );
                    let referer_val = format!(
                        "{}://{}/",
                        parsed_url.scheme(),
                        parsed_url.host_str().unwrap_or_default()
                    );
                    request_builder = request_builder
                        .header("Origin", origin_val)
                        .header("Referer", referer_val);
                }
            }

            let resp = request_builder
                .json(&preq.body)
                .send()
                .await
                .map_err(|e| AppError::network(format!("æ¨¡å‹äºŒAPIè¯·æ±‚å¤±è´¥: {}", e)))?;

            if resp.status().is_success() {
                break resp;
            }

            let status = resp.status();
            let status_code = status.as_u16();

            match status_code {
                // 429 Rate Limitï¼šä½¿ç”¨æŒ‡æ•°é€€é¿é‡è¯•
                429 => {
                    // å°è¯•è§£æ Retry-After å¤´
                    let retry_after = resp
                        .headers()
                        .get("Retry-After")
                        .and_then(|v| v.to_str().ok())
                        .and_then(|s| s.parse::<u64>().ok());

                    let wait_ms = retry_after.map(|s| s * 1000).unwrap_or(backoff_ms);

                    if retry_count < MAX_RETRIES {
                        retry_count += 1;
                        warn!(
                            "[æ¨¡å‹äºŒAPI] é‡åˆ°é€Ÿç‡é™åˆ¶(429)ï¼Œç­‰å¾… {}ms åé‡è¯• ({}/{})",
                            wait_ms, retry_count, MAX_RETRIES
                        );
                        tokio::time::sleep(tokio::time::Duration::from_millis(wait_ms)).await;
                        backoff_ms = (backoff_ms * 2).min(30000); // æŒ‡æ•°é€€é¿ï¼Œæœ€å¤§30ç§’
                        continue;
                    } else {
                        let error_text = resp.text().await.unwrap_or_default();
                        let error_msg = format!(
                            "æ¨¡å‹äºŒAPIè¯·æ±‚å¤±è´¥: é€Ÿç‡é™åˆ¶(429)ï¼Œå·²é‡è¯•{}æ¬¡ä»å¤±è´¥ - {}",
                            MAX_RETRIES, error_text
                        );
                        error!("{}", error_msg);
                        return Err(AppError::llm(error_msg));
                    }
                }
                // 401/403 è®¤è¯é”™è¯¯ï¼šç›´æ¥è¿”å›æ˜ç¡®é”™è¯¯
                401 | 403 => {
                    let error_text = resp.text().await.unwrap_or_default();
                    let error_msg = format!(
                        "æ¨¡å‹äºŒAPIè®¤è¯å¤±è´¥: API Key æ— æ•ˆæˆ–å·²è¿‡æœŸ (HTTP {}) - {}",
                        status_code, error_text
                    );
                    error!("{}", error_msg);
                    return Err(AppError::configuration(error_msg));
                }
                // 5xx æœåŠ¡ç«¯é”™è¯¯ï¼šå¯é‡è¯•
                500..=599 => {
                    if retry_count < MAX_RETRIES {
                        retry_count += 1;
                        warn!(
                            "[æ¨¡å‹äºŒAPI] æœåŠ¡ç«¯é”™è¯¯({})ï¼Œç­‰å¾… {}ms åé‡è¯• ({}/{})",
                            status_code, backoff_ms, retry_count, MAX_RETRIES
                        );
                        tokio::time::sleep(tokio::time::Duration::from_millis(backoff_ms)).await;
                        backoff_ms = (backoff_ms * 2).min(30000);
                        continue;
                    } else {
                        let error_text = resp.text().await.unwrap_or_default();
                        let error_msg = format!(
                            "æ¨¡å‹äºŒAPIæœåŠ¡ç«¯é”™è¯¯: HTTP {} - å·²é‡è¯•{}æ¬¡ä»å¤±è´¥ - {}",
                            status_code, MAX_RETRIES, error_text
                        );
                        error!("{}", error_msg);
                        return Err(AppError::llm(error_msg));
                    }
                }
                // å…¶ä»–é”™è¯¯ï¼šç›´æ¥è¿”å›
                _ => {
                    let error_text = resp.text().await.unwrap_or_default();
                    let error_msg =
                        format!("æ¨¡å‹äºŒAPIè¯·æ±‚å¤±è´¥: HTTP {} - {}", status_code, error_text);
                    error!("æ¨¡å‹äºŒAPIè¯·æ±‚å¤±è´¥: {}", error_msg);
                    return Err(AppError::llm(error_msg));
                }
            }
        };

        let mut stream = response.bytes_stream();
        let mut full_content = String::new();
        let mut reasoning_content = String::new(); // æ”¶é›†æ€ç»´é“¾å†…å®¹
        let mut chunk_counter = 0;
        // å·²æœ‰ request_id
        let mut response_bytes: usize = 0;
        // æ•è·å·¥å…·è°ƒç”¨é›†åˆ
        let mut captured_tool_calls: Vec<crate::models::ToolCall> = Vec::new();
        // æ•è· API è¿”å›çš„ usage ä¿¡æ¯ï¼ˆç”¨äºå‡†ç¡®è®°å½• token ä½¿ç”¨é‡ï¼‰
        let mut captured_usage: Option<serde_json::Value> = None;

        // å·¥å…·è°ƒç”¨èšåˆçŠ¶æ€ - ç”¨äºå¤„ç†æµå¼åˆ†å—çš„å·¥å…·è°ƒç”¨
        let mut pending_tool_calls: std::collections::HashMap<i32, (String, String, String)> =
            std::collections::HashMap::new(); // index -> (id, name, accumulated_args)

        let mut stream_ended = false;
        // åˆå§‹åŒ–SSEè¡Œç¼“å†²å™¨
        let mut sse_buffer = crate::utils::sse_buffer::SseLineBuffer::new();
        // Proactively clear any stale cancel flags from previous runs for this stream_event
        // This avoids immediately cancelling a brand-new stream due to a leftover registry flag
        let _ = self.take_cancellation_if_any(stream_event).await;

        // Register cancel channel for this stream_event
        let cancel_rx = self.register_cancel_channel(stream_event).await;

        debug!(
            "{}[æµå¼è¯·æ±‚] å¼€å§‹å¤„ç†ï¼Œè¯·æ±‚ID: {}, äº‹ä»¶å: {}",
            chat_timing::format_elapsed_prefix(stream_event),
            request_id,
            stream_event
        );

        // é™å™ªä¸éšç§ï¼šä¸æ‰“å°å®Œæ•´è¯·æ±‚å†…å®¹ï¼Œä»…è¾“å‡ºå…³é”®ä¿¡æ¯
        debug!(
            "{}è¯·æ±‚ -> (ç»é€‚é…å™¨) base={} | model={} | stream=true",
            chat_timing::format_elapsed_prefix(stream_event),
            config.base_url,
            config.model
        );
        // P1ä¿®å¤ï¼šç”Ÿå‘½å‘¨æœŸå¯¹é½ - å‘é€startå’Œidäº‹ä»¶
        if let Err(e) = window.emit(
            &format!("{}_start", stream_event),
            &json!({
                "id": stream_event,
                "model": config.model,
                "request_bytes": request_bytes
            }),
        ) {
            warn!("å‘é€å¼€å§‹äº‹ä»¶å¤±è´¥: {}", e);
        }

        if let Err(e) = window.emit(
            &format!("{}_id", stream_event),
            &json!({
                "request_id": stream_event,
                "stream_event": stream_event,
                "timestamp": chrono::Utc::now().format("%Y-%m-%d %H:%M:%S%.3f").to_string()
            }),
        ) {
            warn!("å‘é€IDäº‹ä»¶å¤±è´¥: {}", e);
        }
        // ç”¨é‡æ—¥å¿—ï¼šå¼€å§‹ï¼ˆä½¿ç”¨ FileManager çš„ app_data_dirï¼‰
        {
            let dir = self.file_manager.get_app_data_dir().to_path_buf();
            let logger = crate::debug_logger::DebugLogger::new(dir);
            let _ = logger
                .log_llm_usage(
                    "start",
                    &config.name,
                    &config.model,
                    &config.model_adapter,
                    request_bytes,
                    0,
                    0,
                    0,
                    None,
                    None,
                )
                .await;
        }
        let mut was_cancelled = false;
        while let Some(chunk_result) = stream.next().await {
            // Hard cancel check (best-effort): proactively drain registry then check channel
            let registry_cancelled = self.take_cancellation_if_any(stream_event).await;
            let cancel_flag = *cancel_rx.borrow();

            if registry_cancelled {
                debug!("[Stream Loop] æ£€æµ‹åˆ° registry å–æ¶ˆæ ‡è®°: {}", stream_event);
            }
            if cancel_flag {
                debug!("[Stream Loop] æ£€æµ‹åˆ° cancel_channel ä¿¡å·: {}", stream_event);
            }

            if cancel_flag || registry_cancelled {
                info!(
                    "{}[Cancel] æµå¾ªç¯æ£€æµ‹åˆ°å–æ¶ˆä¿¡å·ï¼Œå‡†å¤‡ä¸­æ–­: {} (registry={}, channel={})",
                    chat_timing::format_elapsed_prefix(stream_event),
                    stream_event,
                    registry_cancelled,
                    cancel_flag
                );
                // P1ä¿®å¤ï¼šç”Ÿå‘½å‘¨æœŸå¯¹é½ - å‘é€cancelledäº‹ä»¶
                if let Err(e) = window.emit(
                    &format!("{}_cancelled", stream_event),
                    &json!({
                        "id": request_id,
                        "reason": "user_cancelled",
                        "timestamp": chrono::Utc::now().format("%Y-%m-%d %H:%M:%S%.3f").to_string()
                    }),
                ) {
                    warn!("å‘é€å–æ¶ˆäº‹ä»¶å¤±è´¥: {}", e);
                } else {
                    debug!("[Cancel] å·²å‘é€ {}_cancelled äº‹ä»¶", stream_event);
                }
                was_cancelled = true;
                debug!("[Cancel] æµå¾ªç¯å·²ä¸­æ–­ï¼Œé€€å‡º while å¾ªç¯");
                break;
            }
            match chunk_result {
                Ok(chunk) => {
                    response_bytes += chunk.len();
                    let chunk_str = String::from_utf8_lossy(&chunk);

                    // ä½¿ç”¨SSEç¼“å†²å™¨å¤„ç†chunkï¼Œè·å–å®Œæ•´çš„è¡Œ
                    let complete_lines = sse_buffer.process_chunk(&chunk_str);
                    for line in complete_lines {
                        // ä½¿ç”¨é€‚é…å™¨è§£ææµäº‹ä»¶ï¼ˆåŒ…æ‹¬[DONE]æ ‡è®°ï¼‰
                        let events = adapter.parse_stream(&line);

                        // æ£€æŸ¥æ˜¯å¦æ˜¯ç»“æŸæ ‡è®°ï¼ˆä¿ç•™ä¸ºåå¤‡æœºåˆ¶ï¼‰
                        if crate::utils::sse_buffer::SseLineBuffer::check_done_marker(&line) {
                            debug!(
                                "{}æ£€æµ‹åˆ°SSEç»“æŸæ ‡è®°: [DONE]",
                                chat_timing::format_elapsed_prefix(stream_event)
                            );
                            if events.is_empty() {
                                // å¦‚æœé€‚é…å™¨æ²¡æœ‰ç”ŸæˆDoneäº‹ä»¶ï¼Œæˆ‘ä»¬æ‰‹åŠ¨æ·»åŠ ä¸€ä¸ª
                                debug!(
                                    "{}é€‚é…å™¨æœªç”ŸæˆDoneäº‹ä»¶ï¼Œæ‰‹åŠ¨æ·»åŠ ",
                                    chat_timing::format_elapsed_prefix(stream_event)
                                );
                                stream_ended = true;
                                break;
                            }
                        }
                        for event in events {
                            match event {
                                crate::providers::StreamEvent::ContentChunk(content) => {
                                    full_content.push_str(&content);
                                    chunk_counter += 1;

                                    let stream_chunk = StreamChunk {
                                        content: content.clone(),
                                        is_complete: false,
                                        chunk_id: format!("{}_chunk_{}", request_id, chunk_counter),
                                    };

                                    // ğŸ”§ ä¿®å¤ï¼šå½“ hook å­˜åœ¨æ—¶ç”± hook è´Ÿè´£å‘é€äº‹ä»¶ï¼ˆæ­£ç¡®çš„ BackendEvent æ ¼å¼ï¼‰
                                    // å¦åˆ™ç›´æ¥ emit StreamChunkï¼ˆå…¼å®¹æ—§è°ƒç”¨æ–¹ï¼‰
                                    if let Some(h) = self.get_hook(stream_event).await {
                                        h.on_content_chunk(&content);
                                    } else if let Err(e) = window.emit(stream_event, &stream_chunk)
                                    {
                                        warn!("å‘é€å†…å®¹å—å¤±è´¥: {}", e);
                                    }
                                }
                                crate::providers::StreamEvent::ReasoningChunk(reasoning) => {
                                    reasoning_content.push_str(&reasoning);

                                    let reasoning_chunk = StreamChunk {
                                        content: reasoning.clone(),
                                        is_complete: false,
                                        chunk_id: format!(
                                            "{}_reasoning_chunk_{}",
                                            request_id, chunk_counter
                                        ),
                                    };

                                    // ğŸ”§ ä¿®å¤ï¼šå½“ hook å­˜åœ¨æ—¶ç”± hook è´Ÿè´£å‘é€äº‹ä»¶
                                    if let Some(h) = self.get_hook(stream_event).await {
                                        h.on_reasoning_chunk(&reasoning);
                                    } else if let Err(e) = window.emit(
                                        &format!("{}_reasoning", stream_event),
                                        &reasoning_chunk,
                                    ) {
                                        warn!("å‘é€æ€ç»´é“¾å—å¤±è´¥: {}", e);
                                    }
                                }
                                crate::providers::StreamEvent::ThoughtSignature(signature) => {
                                    // Gemini 3 æ€ç»´ç­¾åï¼šå·¥å…·è°ƒç”¨åœºæ™¯ä¸‹éœ€è¦ç¼“å­˜å¹¶å›ä¼ 
                                    debug!(
                                        "[ThoughtSignature] æ”¶åˆ° Gemini 3 æ€ç»´ç­¾å: é•¿åº¦={}",
                                        signature.len()
                                    );
                                    // é€šè¿‡ hook ä¼ é€’ç­¾åç»™è°ƒç”¨æ–¹ç¼“å­˜
                                    if let Some(h) = self.get_hook(stream_event).await {
                                        h.on_thought_signature(&signature);
                                    }
                                }
                                crate::providers::StreamEvent::ToolCall(tool_call_value) => {
                                    // èšåˆåˆ†å—çš„å·¥å…·è°ƒç”¨ï¼ˆä¸å†å‘é€åŸå§‹åˆ†å—äº‹ä»¶ï¼‰
                                    if let Some(index) = tool_call_value
                                        .get("index")
                                        .and_then(|v| v.as_i64())
                                        .map(|v| v as i32)
                                    {
                                        if let Some(id) =
                                            tool_call_value.get("id").and_then(|v| v.as_str())
                                        {
                                            // è¿™æ˜¯ä¸€ä¸ªæ–°çš„å·¥å…·è°ƒç”¨çš„å¼€å§‹ï¼ˆæœ‰å®Œæ•´çš„idï¼‰
                                            let name = tool_call_value
                                                .get("function")
                                                .and_then(|f| f.get("name"))
                                                .and_then(|n| n.as_str())
                                                .unwrap_or("unknown");
                                            // ğŸ”§ ä¿®å¤ï¼šæŸäº› OpenAI å…¼å®¹ API è¿”å› arguments ä¸º JSON å¯¹è±¡è€Œéå­—ç¬¦ä¸²
                                            // æ­¤æ—¶ as_str() è¿”å› Noneï¼Œå¯¼è‡´å‚æ•°è¢«é™é»˜ä¸¢å¼ƒä¸º ""
                                            let args = tool_call_value
                                                .get("function")
                                                .and_then(|f| f.get("arguments"))
                                                .map(|a| {
                                                    if let Some(s) = a.as_str() {
                                                        s.to_string()
                                                    } else if a.is_null() {
                                                        String::new()
                                                    } else {
                                                        // arguments æ˜¯ JSON å¯¹è±¡/æ•°ç»„ï¼Œåºåˆ—åŒ–ä¸ºå­—ç¬¦ä¸²
                                                        warn!("[llm_manager] å·¥å…·è°ƒç”¨ arguments ä¸æ˜¯å­—ç¬¦ä¸²è€Œæ˜¯ JSON å€¼ (tool={}), è‡ªåŠ¨åºåˆ—åŒ–", name);
                                                        serde_json::to_string(a).unwrap_or_default()
                                                    }
                                                })
                                                .unwrap_or_default();

                                            pending_tool_calls.insert(
                                                index,
                                                (
                                                    id.to_string(),
                                                    name.to_string(),
                                                    args,
                                                ),
                                            );
                                            // ğŸ†• 2026-01-15: å·¥å…·è°ƒç”¨å‚æ•°å¼€å§‹ç´¯ç§¯æ—¶é€šçŸ¥å‰ç«¯
                                            // è®©å‰ç«¯ç«‹å³æ˜¾ç¤º"æ­£åœ¨å‡†å¤‡å·¥å…·è°ƒç”¨"çŠ¶æ€
                                            if let Some(h) = self.get_hook(stream_event).await {
                                                h.on_tool_call_start(id, name);
                                            }
                                            // ç®€åŒ–æ—¥å¿—ï¼šå·¥å…·è°ƒç”¨å¼€å§‹æ—¶è¾“å‡ºä¸€æ¬¡
                                            print!("ğŸ”§");
                                            use std::io::Write;
                                            let _ = std::io::stdout().flush();
                                        } else if let Some((id, name, mut accumulated_args)) =
                                            pending_tool_calls.get(&index).cloned()
                                        {
                                            // è¿™æ˜¯å·¥å…·è°ƒç”¨çš„åç»­å—ï¼ˆæ²¡æœ‰idï¼Œåªæœ‰argumentsç‰‡æ®µï¼‰
                                            // ğŸ”§ ä¿®å¤ï¼šåŒæ ·å¤„ç† arguments ä¸º JSON å¯¹è±¡çš„æƒ…å†µ
                                            let args_fragment_opt = tool_call_value
                                                .get("function")
                                                .and_then(|f| f.get("arguments"))
                                                .and_then(|a| {
                                                    if let Some(s) = a.as_str() {
                                                        Some(s.to_string())
                                                    } else if a.is_null() {
                                                        None
                                                    } else {
                                                        Some(serde_json::to_string(a).unwrap_or_default())
                                                    }
                                                });
                                            if let Some(args_fragment) = args_fragment_opt {
                                                accumulated_args.push_str(&args_fragment);
                                                pending_tool_calls.insert(
                                                    index,
                                                    (id.clone(), name, accumulated_args.clone()),
                                                );
                                                // ğŸ†• è½¬å‘ args delta ç»™å‰ç«¯å®æ—¶é¢„è§ˆ
                                                if let Some(h) = self.get_hook(stream_event).await {
                                                    h.on_tool_call_args_delta(&id, &args_fragment);
                                                }
                                                // ç®€åŒ–æ—¥å¿—ï¼šæ¯ 200 å­—ç¬¦è¾“å‡ºä¸€ä¸ª / ä»£è¡¨ç´¯ç§¯
                                                if accumulated_args.len() % 200
                                                    < args_fragment.len()
                                                {
                                                    print!("/");
                                                    use std::io::Write;
                                                    let _ = std::io::stdout().flush();
                                                }
                                            }
                                        }
                                    }
                                }
                                crate::providers::StreamEvent::Usage(usage_value) => {
                                    // å­˜å‚¨ usage æ•°æ®ä»¥ä¾¿æœ€ç»ˆè®°å½•åˆ°æ•°æ®åº“
                                    captured_usage = Some(usage_value.clone());
                                    // emit usage äº‹ä»¶
                                    if let Err(e) = window
                                        .emit(&format!("{}_usage", stream_event), &usage_value)
                                    {
                                        error!("å‘é€ç”¨é‡äº‹ä»¶å¤±è´¥: {}", e);
                                    }
                                    if let Some(h) = self.get_hook(stream_event).await {
                                        h.on_usage(&usage_value);
                                    }
                                }
                                crate::providers::StreamEvent::SafetyBlocked(safety_info) => {
                                    // emit safety_blocked äº‹ä»¶
                                    if let Err(e) = window.emit(
                                        &format!("{}_safety_blocked", stream_event),
                                        &safety_info,
                                    ) {
                                        error!("å‘é€å®‰å…¨é˜»æ–­äº‹ä»¶å¤±è´¥: {}", e);
                                    }
                                    // åŒæ—¶å‘é€é€šç”¨é”™è¯¯äº‹ä»¶
                                    let error_event = json!({
                                        "type": "safety_error",
                                        "message": "Request blocked due to safety policies",
                                        "details": safety_info
                                    });
                                    if let Err(e) = window
                                        .emit(&format!("{}_error", stream_event), &error_event)
                                    {
                                        error!("å‘é€å®‰å…¨é”™è¯¯äº‹ä»¶å¤±è´¥: {}", e);
                                    }
                                }
                                crate::providers::StreamEvent::Done => {
                                    stream_ended = true;

                                    // å®Œæˆå¾…èšåˆçš„å·¥å…·è°ƒç”¨ï¼ˆåªåœ¨æœ‰å·¥å…·è°ƒç”¨æ—¶è¾“å‡ºç®€æ´æ—¥å¿—ï¼‰
                                    if !pending_tool_calls.is_empty() {
                                        debug!("å·¥å…·è°ƒç”¨åºåˆ—ç»“æŸ");
                                    }
                                    for (_index, (id, name, accumulated_args)) in
                                        pending_tool_calls.iter()
                                    {
                                        let complete_tool_call = serde_json::json!({
                                            "id": id,
                                            "type": "function",
                                            "function": {
                                                "name": name,
                                                "arguments": accumulated_args
                                            }
                                        });

                                        match Self::convert_openai_tool_call(&complete_tool_call) {
                                            Ok(tc) => {
                                                captured_tool_calls.push(tc);
                                            }
                                            Err(e) => {
                                                warn!("[llm_manager] å·¥å…·è°ƒç”¨è§£æå¤±è´¥: {}, args_len={}", e, accumulated_args.len());
                                                // æ„é€ å¸¦æˆªæ–­é”™è¯¯æ ‡è®°çš„ ToolCallï¼Œè®© pipeline å±‚åé¦ˆç»™ LLM é‡è¯•
                                                captured_tool_calls.push(crate::models::ToolCall {
                                                    id: id.clone(),
                                                    tool_name: name.clone(),
                                                    args_json: json!({
                                                        "_truncation_error": true,
                                                        "_error_message": format!(
                                                            "å·¥å…·è°ƒç”¨å‚æ•° JSON è¢«æˆªæ–­ï¼ˆå·²ç”Ÿæˆ {} å­—ç¬¦ä½†æœªå®Œæˆï¼‰ã€‚åŸå› ï¼šæ¨¡å‹è¾“å‡º token è¾¾åˆ°ä¸Šé™ã€‚",
                                                            accumulated_args.len()
                                                        ),
                                                        "_args_len": accumulated_args.len(),
                                                    }),
                                                });
                                            }
                                        }
                                    }
                                    // è¾“å‡ºä¸€æ¡ç®€æ´çš„å·¥å…·è°ƒç”¨æ€»ç»“
                                    if !captured_tool_calls.is_empty() {
                                        let names: Vec<_> = captured_tool_calls
                                            .iter()
                                            .map(|tc| tc.tool_name.as_str())
                                            .collect();
                                        debug!("å·¥å…·è°ƒç”¨èšåˆå®Œæˆ: {:?}", names);
                                    }
                                    pending_tool_calls.clear();

                                    break;
                                }
                            }
                        }

                        if stream_ended {
                            break;
                        }
                    }
                }
                Err(e) => {
                    error!(
                        "{}æµè¯»å–é”™è¯¯: {}",
                        chat_timing::format_elapsed_prefix(stream_event),
                        e
                    );
                    debug!(
                        "{}å·²å¤„ç†å—æ•°: {}, ä¸»å†…å®¹é•¿åº¦: {}, æ€ç»´é“¾é•¿åº¦: {}",
                        chat_timing::format_elapsed_prefix(stream_event),
                        chunk_counter,
                        full_content.len(),
                        reasoning_content.len()
                    );

                    // å¦‚æœå·²ç»æœ‰å†…å®¹ï¼Œä¸æŠŠè¿™å½“ä½œå®Œå…¨å¤±è´¥
                    if !full_content.is_empty() || !reasoning_content.is_empty() {
                        warn!(
                            "{}éƒ¨åˆ†å†…å®¹å·²æ¥æ”¶ï¼Œæ ‡è®°ä¸ºéƒ¨åˆ†æˆåŠŸ",
                            chat_timing::format_elapsed_prefix(stream_event)
                        );
                        break;
                    } else {
                        error!(
                            "{}æ²¡æœ‰æ¥æ”¶åˆ°ä»»ä½•å†…å®¹ï¼Œè¿™æ˜¯å®Œå…¨å¤±è´¥",
                            chat_timing::format_elapsed_prefix(stream_event)
                        );
                        // å‘é€ä½œç”¨åŸŸé”™è¯¯äº‹ä»¶
                        let error_event = format!("{}_error", stream_event);
                        let error_payload = json!({
                            "error": format!("æµå¼è¯·æ±‚å¤±è´¥: {}", e),
                            "stream_event": stream_event,
                            "timestamp": chrono::Utc::now().format("%Y-%m-%d %H:%M:%S%.3f").to_string()
                        });
                        if let Err(emit_err) = window.emit(&error_event, &error_payload) {
                            error!("å‘é€ä½œç”¨åŸŸé”™è¯¯äº‹ä»¶å¤±è´¥: {}", emit_err);
                        }
                        // åŒæ—¶å‘é€å…¼å®¹æ€§å…¨å±€é”™è¯¯äº‹ä»¶
                        if let Err(emit_err) = window.emit("stream_error", &error_payload) {
                            error!("å‘é€å…¨å±€é”™è¯¯äº‹ä»¶å¤±è´¥: {}", emit_err);
                        }
                        return Err(AppError::network(format!("æµå¼è¯·æ±‚å¤±è´¥: {}", e)));
                    }
                }
            }

            // å¦‚æœæµå·²ç»“æŸï¼Œé€€å‡ºå¾ªç¯
            if stream_ended {
                break;
            }
        }

        // å¤„ç†SSEç¼“å†²å™¨ä¸­å‰©ä½™çš„ä¸å®Œæ•´è¡Œï¼ˆP1ä¿®å¤ï¼šSTREAM-3ï¼‰
        if let Some(remaining_line) = sse_buffer.flush() {
            if !remaining_line.trim().is_empty() {
                debug!(
                    "{}å¤„ç†SSEç¼“å†²å™¨ä¸­çš„å‰©ä½™æ•°æ®: {} å­—ç¬¦",
                    chat_timing::format_elapsed_prefix(stream_event),
                    remaining_line.len()
                );
                // ä½¿ç”¨é€‚é…å™¨è§£æå‰©ä½™çš„è¡Œ
                let events = adapter.parse_stream(&remaining_line);
                for event in events {
                    match event {
                        crate::providers::StreamEvent::ContentChunk(content) => {
                            full_content.push_str(&content);
                            chunk_counter += 1;

                            let stream_chunk = StreamChunk {
                                content: content.clone(),
                                is_complete: false,
                                chunk_id: format!("{}_chunk_{}", request_id, chunk_counter),
                            };

                            if let Some(h) = self.get_hook(stream_event).await {
                                h.on_content_chunk(&content);
                            } else if let Err(e) = window.emit(stream_event, &stream_chunk) {
                                warn!("å‘é€å‰©ä½™å†…å®¹å—å¤±è´¥: {}", e);
                            }
                        }
                        crate::providers::StreamEvent::ReasoningChunk(reasoning) => {
                            reasoning_content.push_str(&reasoning);

                            let reasoning_chunk = StreamChunk {
                                content: reasoning.clone(),
                                is_complete: false,
                                chunk_id: format!(
                                    "{}_reasoning_chunk_{}",
                                    request_id, chunk_counter
                                ),
                            };

                            if let Some(h) = self.get_hook(stream_event).await {
                                h.on_reasoning_chunk(&reasoning);
                            } else if let Err(e) = window
                                .emit(&format!("{}_reasoning", stream_event), &reasoning_chunk)
                            {
                                warn!("å‘é€å‰©ä½™æ€ç»´é“¾å—å¤±è´¥: {}", e);
                            }
                        }
                        _ => { /* å¿½ç•¥å…¶ä»–äº‹ä»¶ç±»å‹ï¼ˆDone/ToolCall/Usageç­‰å·²åœ¨ä¸»å¾ªç¯å¤„ç†ï¼‰ */
                        }
                    }
                }
            }
        }

        // ğŸ”§ P0ä¿®å¤ï¼šGemini åŸç”Ÿ SSE ä¸å‘é€ `data: [DONE]`ï¼Œæµç›´æ¥ç»“æŸã€‚
        // å¦‚æœ pending_tool_calls ä¸­ä»æœ‰æœªå¤„ç†çš„å·¥å…·è°ƒç”¨ï¼Œåœ¨æ­¤æ‰§è¡Œä¸ Done å¤„ç†å™¨ç›¸åŒçš„ finalize é€»è¾‘ã€‚
        if !pending_tool_calls.is_empty() {
            info!(
                "[llm_manager] Finalizing {} pending tool calls after stream end (no Done event received)",
                pending_tool_calls.len()
            );
            for (_index, (id, name, accumulated_args)) in pending_tool_calls.iter() {
                let complete_tool_call = serde_json::json!({
                    "id": id,
                    "type": "function",
                    "function": {
                        "name": name,
                        "arguments": accumulated_args
                    }
                });

                match Self::convert_openai_tool_call(&complete_tool_call) {
                    Ok(tc) => {
                        captured_tool_calls.push(tc);
                    }
                    Err(e) => {
                        warn!("[llm_manager] å·¥å…·è°ƒç”¨è§£æå¤±è´¥(fallback): {}, args_len={}", e, accumulated_args.len());
                        captured_tool_calls.push(crate::models::ToolCall {
                            id: id.clone(),
                            tool_name: name.clone(),
                            args_json: json!({
                                "_truncation_error": true,
                                "_error_message": format!(
                                    "å·¥å…·è°ƒç”¨å‚æ•° JSON è¢«æˆªæ–­ï¼ˆå·²ç”Ÿæˆ {} å­—ç¬¦ä½†æœªå®Œæˆï¼‰ã€‚åŸå› ï¼šæ¨¡å‹è¾“å‡º token è¾¾åˆ°ä¸Šé™ã€‚",
                                    accumulated_args.len()
                                ),
                                "_args_len": accumulated_args.len(),
                            }),
                        });
                    }
                }
            }
            if !captured_tool_calls.is_empty() {
                let names: Vec<_> = captured_tool_calls
                    .iter()
                    .map(|tc| tc.tool_name.as_str())
                    .collect();
                info!("[llm_manager] Fallback tool call finalize completed: {:?}", names);
            }
            pending_tool_calls.clear();
        }

        // Clear cancel channel for this stream
        self.clear_cancel_channel(stream_event).await;

        // è¾“å‡ºæœ€ç»ˆæ”¶é›†ç»Ÿè®¡ï¼ˆè„±æ•ï¼‰
        use sha2::{Digest, Sha256};
        let mut h1 = Sha256::new();
        h1.update(full_content.as_bytes());
        let mut h2 = Sha256::new();
        h2.update(reasoning_content.as_bytes());
        let full_hash = format!("{:x}", h1.finalize());
        let reasoning_hash = format!("{:x}", h2.finalize());
        debug!(
            "{}æµå¼å“åº”å®Œæˆç»Ÿè®¡ï¼ˆè„±æ•ï¼‰:",
            chat_timing::format_elapsed_prefix(stream_event)
        );
        debug!(
            "  - ä¸»å†…å®¹é•¿åº¦: {} å­—ç¬¦, hash: {}",
            full_content.len(),
            &full_hash[..8.min(full_hash.len())]
        );
        debug!(
            "  - æ€ç»´é“¾é•¿åº¦: {} å­—ç¬¦, hash: {}",
            reasoning_content.len(),
            &reasoning_hash[..8.min(reasoning_hash.len())]
        );

        // ğŸ”§ [REFACTOR] æ—§çš„å·¥å…·è°ƒç”¨æ‰§è¡Œé€»è¾‘å·²ç§»é™¤
        // å·¥å…·è°ƒç”¨ç°åœ¨ç”± Chat V2 Pipeline ç»Ÿä¸€å¤„ç†ï¼ˆsrc-tauri/src/chat_v2/pipeline.rsï¼‰
        // æ­¤å¤„åªè´Ÿè´£æµå¼å“åº”çš„æ”¶é›†ï¼Œå·¥å…·è°ƒç”¨é€šè¿‡ LLMStreamHooks å›è°ƒç»™ä¸Šå±‚

        // ğŸ”§ [CRITICAL] å°†æ”¶é›†åˆ°çš„å·¥å…·è°ƒç”¨é€šè¿‡ hooks å›è°ƒç»™ä¸Šå±‚ï¼ˆPipelineï¼‰
        if !captured_tool_calls.is_empty() {
            info!(
                "[llm_manager] Notifying hooks of {} tool calls",
                captured_tool_calls.len()
            );
            for tc in &captured_tool_calls {
                if let Some(h) = self.get_hook(stream_event).await {
                    let chat_msg = ChatMessage {
                        role: "assistant".to_string(),
                        content: String::new(),
                        timestamp: chrono::Utc::now(),
                        thinking_content: None,
                        thought_signature: None,
                        rag_sources: None,
                        memory_sources: None,
                        graph_sources: None,
                        web_search_sources: None,
                        image_paths: None,
                        image_base64: None,
                        doc_attachments: None,
                        multimodal_content: None,
                        tool_call: Some(tc.clone()),
                        tool_result: None,
                        overrides: None,
                        relations: None,
                        persistent_stable_id: None,
                        metadata: None,
                    };
                    h.on_tool_call(&chat_msg);
                }
            }
        }

        if !was_cancelled {
            // å‘é€æœ€ç»ˆå®Œæˆä¿¡å·åˆ°ä¸»å†…å®¹æµ
            let final_chunk = StreamChunk {
                content: full_content.clone(), // å‘é€å®Œæ•´å†…å®¹è€Œä¸æ˜¯ç©ºå­—ç¬¦ä¸²
                is_complete: true,
                chunk_id: format!("{}_final_chunk_{}", request_id, chunk_counter + 1),
            };

            // ğŸ¯ ç»Ÿä¸€å›é¡¾åˆ†æå®Œæˆæ—¥å¿—
            if stream_event.contains("review_analysis_stream")
                || stream_event.contains("review_chat_stream")
            {
                debug!("[ç»Ÿä¸€å›é¡¾-åç«¯å‘é€-ä¸»å†…å®¹å®Œæˆ] äº‹ä»¶å: {}", stream_event);
                debug!(
                    "   - æ—¶é—´æˆ³: {}",
                    chrono::Utc::now().format("%Y-%m-%d %H:%M:%S%.3f")
                );
                debug!("   - chunk_id: {}", final_chunk.chunk_id);
                debug!("   - å®Œæ•´å†…å®¹é•¿åº¦: {} å­—ç¬¦", final_chunk.content.len());
                debug!("   - is_complete: {}", final_chunk.is_complete);
                debug!("   - æ€»å—æ•°: {}", chunk_counter + 1);
            }

            // ğŸ”§ ä¿®å¤ï¼šå½“ hook å­˜åœ¨æ—¶ç”± hook è´Ÿè´£å‘é€å®Œæˆäº‹ä»¶ï¼Œè·³è¿‡ç›´æ¥ emit
            if let Some(h) = self.get_hook(stream_event).await {
                // hook å­˜åœ¨ï¼Œè°ƒç”¨ on_complete å¤„ç†å®Œæˆé€»è¾‘
                h.on_complete(
                    &full_content,
                    if reasoning_content.is_empty() {
                        None
                    } else {
                        Some(&reasoning_content)
                    },
                );
                debug!("é€šè¿‡ hook å¤„ç†å®Œæˆä¿¡å·ï¼Œå†…å®¹é•¿åº¦: {}", full_content.len());
            } else if let Err(e) = window.emit(stream_event, &final_chunk) {
                error!("å‘é€æœ€ç»ˆä¸»å†…å®¹å®Œæˆä¿¡å·å¤±è´¥: {}", e);
            } else {
                debug!("å‘é€ä¸»å†…å®¹å®Œæˆä¿¡å·æˆåŠŸï¼Œå†…å®¹é•¿åº¦: {}", full_content.len());
            }
        }
        // å¦‚æœæœ‰æ€ç»´é“¾å†…å®¹ï¼Œä¹Ÿå‘é€æ€ç»´é“¾å®Œæˆä¿¡å·
        if !was_cancelled && enable_chain_of_thought && !reasoning_content.is_empty() {
            let reasoning_final_chunk = StreamChunk {
                content: reasoning_content.clone(), // ä¹Ÿå‘é€å®Œæ•´çš„æ€ç»´é“¾å†…å®¹
                is_complete: true,
                chunk_id: format!("{}_reasoning_final_chunk_{}", request_id, chunk_counter + 1),
            };

            debug!(
                "[æ€ç»´é“¾æ€»ç»“] å‡†å¤‡å‘é€æœ€ç»ˆæ€ç»´é“¾: æ€»é•¿åº¦={}, å†…å®¹é¢„è§ˆ={}",
                reasoning_content.len(),
                &reasoning_content.chars().take(100).collect::<String>()
            );

            if let Err(e) = window.emit(
                &format!("{}_reasoning", stream_event),
                &reasoning_final_chunk,
            ) {
                error!("å‘é€æ€ç»´é“¾å®Œæˆä¿¡å·å¤±è´¥: {}", e);
            } else {
                debug!(
                    "å‘é€æ€ç»´é“¾å®Œæˆä¿¡å·æˆåŠŸï¼Œå†…å®¹é•¿åº¦: {}, äº‹ä»¶å: {}_reasoning",
                    reasoning_content.len(),
                    stream_event
                );
            }
        } else if !was_cancelled && enable_chain_of_thought && reasoning_content.is_empty() {
            warn!("[æ€ç»´é“¾æ€»ç»“] å¯ç”¨äº†æ€ç»´é“¾ä½† reasoning_content ä¸ºç©º!");
        }

        // å¦‚æœå¯ç”¨äº†æ€ç»´é“¾ï¼Œå°è¯•æå–æ€ç»´é“¾è¯¦æƒ…ï¼ˆæ–‡æ¡£ 29 ç¬¬ 7 èŠ‚ï¼‰
        let chain_of_thought_details = if enable_chain_of_thought {
            let needs_passback = requires_reasoning_passback(&config);
            if needs_passback {
                // æ¨ç†æ¨¡å‹è‡ªåŠ¨åŒ…å«æ€ç»´é“¾
                let reference = if !reasoning_content.is_empty() {
                    parser::extract_reasoning_sections(&reasoning_content)
                } else {
                    parser::extract_reasoning_sections(&full_content)
                };
                let policy = get_passback_policy(&config);
                Some(json!({
                    "full_response": full_content,
                    "reasoning_content": if reasoning_content.is_empty() { Value::Null } else { json!(reasoning_content) },
                    "enabled": true,
                    "is_reasoning_model": true,
                    "model_adapter": config.model_adapter,
                    "parsed_sections": reference,
                    "passback_policy": match policy {
                        ReasoningPassbackPolicy::DeepSeekStyle => "deepseek_style",
                        ReasoningPassbackPolicy::ReasoningDetails => "reasoning_details",
                        ReasoningPassbackPolicy::NoPassback => "no_passback",
                    }
                }))
            } else {
                Some(json!({
                    "full_response": full_content,
                    "enabled": true,
                    "is_reasoning_model": false,
                    "model_adapter": config.model_adapter
                }))
            }
        } else {
            None
        };

        // ç”¨é‡æ—¥å¿—ï¼šç»“æŸï¼ˆè„±æ•å†™å…¥ï¼Œä½¿ç”¨ FileManagerï¼‰
        {
            let approx_tokens_out = crate::utils::token_budget::estimate_tokens(&full_content);
            let dur = start_instant.elapsed().as_millis();
            let dir = self.file_manager.get_app_data_dir().to_path_buf();
            let logger = crate::debug_logger::DebugLogger::new(dir);

            // ä» API è¿”å›çš„ usage æ•°æ®ä¸­æå–å®é™… token æ•°é‡
            let (actual_prompt_tokens, actual_completion_tokens, reasoning_tokens) =
                Self::extract_usage_tokens(
                    &captured_usage,
                    approx_tokens_out,
                    (request_bytes / 4).max(1),
                );

            let _ = logger
                .log_llm_usage(
                    "end",
                    &config.name,
                    &config.model,
                    &config.model_adapter,
                    request_bytes,
                    response_bytes,
                    actual_prompt_tokens as usize,
                    actual_completion_tokens as usize,
                    Some(dur),
                    None,
                )
                .await;

            crate::llm_usage::record_llm_usage(
                crate::llm_usage::CallerType::ChatV2,
                &config.model,
                actual_prompt_tokens,
                actual_completion_tokens,
                reasoning_tokens,
                None,
                Some(stream_event.to_string()),
                Some(dur as u64),
                !was_cancelled,
                if was_cancelled {
                    Some("cancelled".to_string())
                } else {
                    None
                },
            );
        }

        Ok(StandardModel2Output {
            assistant_message: if was_cancelled {
                String::new()
            } else {
                full_content
            },
            raw_response: Some("stream_response".to_string()),
            chain_of_thought_details,
            cancelled: was_cancelled,
        })
    }
    // ğŸ¯ æ–°å¢ï¼šé€šç”¨æµå¼æ¥å£ï¼Œæ”¯æŒè‡ªå®šä¹‰æ¨¡å‹é…ç½®ï¼ˆç”¨äºæ€»ç»“è¯·æ±‚ç­‰ç‰¹æ®Šåœºæ™¯ï¼‰
    pub async fn call_unified_model_stream_with_config(
        &self,
        config: &ApiConfig,
        context: &HashMap<String, Value>,
        chat_history: &[ChatMessage],
        subject: &str,
        enable_chain_of_thought: bool,
        image_paths: Option<Vec<String>>,
        task_context: Option<&str>,
        window: Window,
        stream_event: &str,
        _max_input_tokens_override: Option<usize>,
    ) -> Result<StandardModel2Output> {
        info!(
            "è°ƒç”¨é€šç”¨æµå¼æ¥å£: æ¨¡å‹={}, ç§‘ç›®={}, æ€ç»´é“¾={}, å›¾ç‰‡æ•°é‡={}",
            config.model,
            subject,
            enable_chain_of_thought,
            image_paths.as_ref().map(|p| p.len()).unwrap_or(0)
        );

        // å·²ç§»é™¤ Google/Gemini ç‰¹æ®Šé€‚é…å™¨è·¯ç”±ï¼Œç»Ÿä¸€èµ°æ ‡å‡†æµå¼å®ç°

        // å›¾ç‰‡æ”¹ä¸ºæ¶ˆæ¯çº§æ¥æº
        let images_used_source = "per_message".to_string();
        let images_base64: Option<Vec<String>> = None;

        // ç§»é™¤ä¸Šä¸‹æ–‡é¢„ç®—è£å‰ªï¼šæŒ‰ç…§ç”¨æˆ·å»ºè®®ï¼Œå®Œæ•´ä¿ç•™å†å²ï¼Œç”±å‰ç«¯å±•ç¤ºtokenä¼°ç®—å¹¶ç”±ç”¨æˆ·å†³å®š
        let chat_history = chat_history.to_vec();

        debug!(
            "[model2_stream_with_config] model={} provider={} adapter={} multi={} reasoning={} temp={} cot={} images={{source:{},count:{}}}",
            config.model, config.name, config.model_adapter, config.is_multimodal, config.is_reasoning, config.temperature,
            enable_chain_of_thought, images_used_source, images_base64.as_ref().map(|v| v.len()).unwrap_or(0)
        );

        let mut messages = vec![];

        // æ–‡æ¡£31æ¸…ç†ï¼šç§‘ç›®é…ç½®ç³»ç»Ÿå·²åºŸå¼ƒï¼Œä½¿ç”¨é€šç”¨æç¤ºè¯
        let subject_prompt = format!("è¯·åŸºäº{}ç§‘ç›®çš„ç‰¹ç‚¹è¿›è¡Œåˆ†æã€‚\n\n", subject);

        // æ„å»ºç³»ç»Ÿæç¤ºè¯ï¼ˆä½¿ç”¨ä¸call_unified_model_2_streamç›¸åŒçš„é€»è¾‘ï¼‰
        if !context.is_empty() {
            let mut system_content = subject_prompt.clone();

            if let Some(task_ctx) = task_context {
                system_content.push_str(&format!("ã€ä»»åŠ¡èƒŒæ™¯ã€‘\n{}\n\n", task_ctx));
            }

            for (key, value) in context {
                match key.as_str() {
                    "ocr_text" => system_content.push_str(&format!(
                        "ã€é¢˜ç›®å†…å®¹ã€‘\n{}\n\n",
                        value.as_str().unwrap_or("")
                    )),
                    "user_question" => system_content.push_str(&format!(
                        "ã€å­¦ç”Ÿé—®é¢˜ã€‘\n{}\n\n",
                        value.as_str().unwrap_or("")
                    )),
                    "tags" => {
                        if let Some(tags_array) = value.as_array() {
                            let tags: Vec<String> = tags_array
                                .iter()
                                .filter_map(|v| v.as_str())
                                .map(|s| s.to_string())
                                .collect();
                            if !tags.is_empty() {
                                system_content
                                    .push_str(&format!("ã€ç›¸å…³æ ‡ç­¾ã€‘\n{}\n\n", tags.join(", ")));
                            }
                        }
                    }
                    "mistake_type" => system_content.push_str(&format!(
                        "ã€é¢˜ç›®ç±»å‹ã€‘\n{}\n\n",
                        value.as_str().unwrap_or("")
                    )),
                    _ => {}
                }
            }

            if !config.is_reasoning {
                messages.push(json!({
                    "role": "system",
                    "content": system_content
                }));
            }

            // å¦‚æœæ˜¯å¤šæ¨¡æ€æ¨¡å‹ä¸”æä¾›äº†å›¾ç‰‡ï¼Œæ·»åŠ å›¾ç‰‡åˆ°ç¬¬ä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
            if config.is_multimodal && images_base64.is_some() && chat_history.is_empty() {
                let mut content = vec![json!({
                    "type": "text",
                    "text": "è¯·åŸºäºä¸Šè¿°ä¿¡æ¯å’Œå›¾ç‰‡ï¼Œæä¾›è¯¦ç»†çš„è§£ç­”ã€‚"
                })];

                if let Some(images) = &images_base64 {
                    for image_base64 in images {
                        let image_format = Self::detect_image_format_from_base64(image_base64);
                        debug!("æ£€æµ‹åˆ°å›¾åƒæ ¼å¼: {}", image_format);
                        content.push(json!({
                            "type": "image_url",
                            "image_url": {
                                "url": format!("data:image/{};base64,{}", image_format, image_base64)
                            }
                        }));
                    }
                }

                messages.push(json!({
                    "role": "user",
                    "content": content
                }));
            } else if chat_history.is_empty() {
                // çº¯æ–‡æœ¬æ¨¡å‹æˆ–æ²¡æœ‰æä¾›å›¾ç‰‡
                messages.push(json!({
                    "role": "user",
                    "content": "è¯·åŸºäºä¸Šè¿°ä¿¡æ¯ï¼Œæä¾›è¯¦ç»†çš„è§£ç­”ã€‚"
                }));
            }
        }

        // æ·»åŠ èŠå¤©å†å²ï¼ˆåŒ…å«å·¥å…·è°ƒç”¨æ¶ˆæ¯çš„æ ‡å‡†åŒ–ï¼‰
        for (index, msg) in chat_history.iter().enumerate() {
            // å¤„ç†ç”¨æˆ·æ¶ˆæ¯
            if msg.role == "user" {
                let mut message_content = msg.content.clone();
                if config.is_reasoning && index == 0 {
                    // æ¨ç†æ¨¡å‹ï¼šé¦–æ¡ç”¨æˆ·æ¶ˆæ¯åˆå¹¶ç§‘ç›®æç¤ºï¼ˆç®€åŒ–å¤„ç†ï¼‰
                    message_content = format!("{}\n\n{}", subject_prompt, message_content);
                }

                // å¦‚æœæœ‰æ–‡æ¡£é™„ä»¶ï¼Œå°†å…¶å†…å®¹æ·»åŠ åˆ°æ¶ˆæ¯ä¸­
                if let Some(doc_attachments) = &msg.doc_attachments {
                    if !doc_attachments.is_empty() {
                        message_content.push_str("\n\n--- é™„ä»¶å†…å®¹ ---");
                        for doc in doc_attachments {
                            message_content.push_str(&format!("\n\nã€æ–‡æ¡£: {}ã€‘", doc.name));
                            if let Some(text_content) = &doc.text_content {
                                message_content.push_str(&format!("\n{}", text_content));
                            }
                        }
                    }
                }

                // ğŸ¯ æ”¹é€ ï¼šå¦‚æœæ˜¯å¤šæ¨¡æ€æ¨¡å‹ä¸”è¯¥æ¡æ¶ˆæ¯æœ‰å›¾ç‰‡ï¼Œä¸ºè¯¥æ¡æ¶ˆæ¯é™„å›¾
                if config.is_multimodal
                    && msg
                        .image_base64
                        .as_ref()
                        .map(|v| !v.is_empty())
                        .unwrap_or(false)
                {
                    let mut content = vec![json!({
                        "type": "text",
                        "text": message_content
                    })];

                    if let Some(images) = &msg.image_base64 {
                        for image_base64 in images {
                            let image_format = Self::detect_image_format_from_base64(image_base64);
                            content.push(json!({
                                "type": "image_url",
                                "image_url": { "url": format!("data:image/{};base64,{}", image_format, image_base64) }
                            }));
                        }
                    }

                    messages.push(json!({
                        "role": msg.role,
                        "content": content
                    }));
                } else {
                    messages.push(json!({
                        "role": msg.role,
                        "content": message_content
                    }));
                }
            } else if msg.role == "assistant" {
                // å¦‚æœassistantæ¶ˆæ¯ä¸­å­˜åœ¨å·¥å…·è°ƒç”¨ï¼Œåˆ™ä»¥tool_callsæ ‡å‡†ç»“æ„å‘å‡º
                if let Some(tc) = &msg.tool_call {
                    let tool_call_obj = json!({
                        "id": tc.id,
                        "type": "function",
                        "function": {
                            "name": tc.tool_name,
                            "arguments": tc.args_json.to_string()
                        }
                    });
                    messages.push(json!({
                        "role": "assistant",
                        "content": msg.content,
                        "tool_calls": [tool_call_obj]
                    }));
                } else {
                    messages.push(json!({
                        "role": "assistant",
                        "content": msg.content
                    }));
                }
            } else if msg.role == "tool" {
                // å·¥å…·ç»“æœæ¶ˆæ¯å¿…é¡»åŒ…å« tool_call_id
                if let Some(tr) = &msg.tool_result {
                    messages.push(json!({
                        "role": "tool",
                        "tool_call_id": tr.call_id,
                        "content": msg.content
                    }));
                } else {
                    // é™çº§å…œåº•ï¼Œé¿å…äº§ç”Ÿä¸è¢«APIæ¥å—çš„toolæ¶ˆæ¯
                    messages.push(json!({
                        "role": "assistant",
                        "content": msg.content
                    }));
                }
            }
        }

        // ğŸ”§ é˜²å¾¡æ€§åˆå¹¶ï¼šè¿ç»­ assistant tool_callsï¼ˆæç«¯æƒ…å†µä¸‹å¯èƒ½å‡ºç°ï¼‰
        // æ³¨æ„ï¼šhistory.rs è¾“å‡ºçš„æ˜¯äº¤å‰æ¨¡å¼ assistantâ†’toolâ†’assistantâ†’toolï¼Œ
        // æ‰€ä»¥æ­¤å‡½æ•°åœ¨æ­£å¸¸æµç¨‹ä¸­æ˜¯ no-opï¼Œä»…ä½œä¸ºé˜²å¾¡æ€§ä¿æŠ¤ã€‚
        Self::merge_consecutive_assistant_tool_calls(&mut messages);
        // ğŸ”§ é˜²å¾¡æ€§åˆå¹¶ï¼šè¿ç»­ user æ¶ˆæ¯åˆå¹¶
        Self::merge_consecutive_user_messages(&mut messages);

        let mut request_body = json!({
            "model": config.model,
            "messages": messages,
            "stream": true
        });

        Self::apply_reasoning_config(&mut request_body, &config, None);

        // æ£€æŸ¥æ˜¯å¦å¯ç”¨å·¥å…·ï¼ˆå…¨å±€ + æ¨¡å‹èƒ½åŠ›ï¼‰
        let tools_enabled = self
            .db
            .get_setting("tools.enabled")
            .ok()
            .flatten()
            .map(|v| v.to_lowercase())
            .map(|v| v != "0" && v != "false")
            .unwrap_or(true); // é»˜è®¤å¯ç”¨

        if tools_enabled && config.supports_tools {
            // æ„å»ºå·¥å…·åˆ—è¡¨ï¼ŒåŒ…å«æœ¬åœ°å·¥å…·å’Œ MCP å·¥å…·
            let tools = self.build_tools_with_mcp(&window).await;

            // åªæœ‰åœ¨å·¥å…·åˆ—è¡¨éç©ºæ—¶æ‰è®¾ç½® tools å’Œ tool_choice
            if tools.as_array().map(|arr| !arr.is_empty()).unwrap_or(false) {
                request_body["tools"] = tools;
                request_body["tool_choice"] = json!("auto");
            } else {
                warn!("[LLM] å·¥å…·åˆ—è¡¨ä¸ºç©ºï¼Œè·³è¿‡ tool_choice è®¾ç½®");
            }
        } else {
            if !config.supports_tools {
                debug!("è·³è¿‡å·¥å…·æ³¨å…¥ï¼šæ¨¡å‹ä¸æ”¯æŒå‡½æ•°è°ƒç”¨ (supports_tools=false)");
                // ä¸ºä¸æ”¯æŒå·¥å…·çš„æ¨¡å‹ä¸»åŠ¨è°ƒç”¨RAG/æ™ºèƒ½è®°å¿†å·¥å…·å¹¶æ³¨å…¥ä¸Šä¸‹æ–‡
                let inject_texts: Vec<String> = Vec::new();

                if let Some(_last_user_msg) = chat_history.iter().filter(|m| m.role == "user").last()
                {
                    let memory_enabled_effective = context
                        .get("memory_enabled")
                        .and_then(|v| v.as_bool())
                        .unwrap_or(true);
                    if memory_enabled_effective {
                        let _ = window.emit(
                            &format!("{}_memory_sources", stream_event),
                            &serde_json::json!({"stage":"disabled"}),
                        );
                    }

                    let rag_enabled = context
                        .get("rag_enabled")
                        .and_then(|v| v.as_bool())
                        .unwrap_or(true);
                    let _rag_library_ids: Option<Vec<String>> = context
                        .get("rag_library_ids")
                        .and_then(|v| v.as_array())
                        .map(|arr| {
                            arr.iter()
                                .filter_map(|v| v.as_str().map(|s| s.to_string()))
                                .collect::<Vec<String>>()
                        })
                        .filter(|v| !v.is_empty());
                    let _rag_note_subjects: Option<Vec<String>> = context
                        .get("rag_note_subjects")
                        .and_then(|v| v.as_array())
                        .map(|arr| {
                            arr.iter()
                                .filter_map(|v| v.as_str().map(|s| s.to_string()))
                                .collect::<Vec<String>>()
                        })
                        .filter(|v| !v.is_empty());
                    if rag_enabled {
                        // Legacy RAG removed; VFS RAG is used via builtin:rag_search tool
                        debug!("[Fallback] Legacy RAG removed, skipping knowledge base injection");
                    } else {
                        debug!("[Fallback] RAG å·²å…³é—­ï¼Œè·³è¿‡çŸ¥è¯†åº“æ³¨å…¥");
                    }
                }

                // å¦‚æœæœ‰æ³¨å…¥æ–‡æœ¬ï¼Œæ·»åŠ åˆ°ç³»ç»Ÿæç¤ºï¼ˆç»Ÿä¸€é•¿åº¦é—¨æ§ï¼‰
                if !inject_texts.is_empty() {
                    // å•é¡¹ä¸æ€»é‡é™é¢
                    let per_item_max = 1600usize; // æ¯æ®µæœ€å¤š1600å­—ç¬¦
                    let total_max = 20000usize; // æ€»æ³¨å…¥æœ€å¤š20000å­—ç¬¦
                    let mut acc = String::new();
                    for mut s in inject_texts {
                        if s.chars().count() > per_item_max {
                            s = s.chars().take(per_item_max).collect();
                        }
                        if acc.chars().count() + s.chars().count() > total_max {
                            break;
                        }
                        acc.push_str(&s);
                    }
                    let inject_content = acc;
                    // å°†æ³¨å…¥æ–‡æœ¬å®‰å…¨åœ°åˆå¹¶åˆ°"æ¨¡å‹å®é™…å¯è§"çš„æ¶ˆæ¯ï¼š
                    // - éæ¨ç†æ¨¡å‹ï¼šç³»ç»Ÿæ¶ˆæ¯å¯è§ â†’ è¿½åŠ åˆ° system
                    // - æ¨ç†æ¨¡å‹ï¼šç³»ç»Ÿæ¶ˆæ¯æœ€ç»ˆåˆå¹¶åˆ°é¦–æ¡ user â†’ ç›´æ¥åœ¨é¦–æ¡ user å‰é¢æ‹¼æ¥
                    if !config.is_reasoning {
                        // è¿½åŠ /åˆ›å»º system
                        if let Some(first_msg) = messages.get_mut(0) {
                            if first_msg["role"] == "system" {
                                let current_content = first_msg["content"].as_str().unwrap_or("");
                                first_msg["content"] =
                                    json!(format!("{}\n\n{}", current_content, inject_content));
                            } else {
                                messages.insert(
                                    0,
                                    json!({ "role": "system", "content": inject_content }),
                                );
                            }
                        } else {
                            messages.push(json!({ "role": "system", "content": inject_content }));
                        }
                    } else {
                        // æ¨ç†æ¨¡å‹ï¼šåˆå¹¶åˆ°é¦–æ¡ç”¨æˆ·æ¶ˆæ¯å¼€å¤´
                        if let Some(first_msg) = messages.get_mut(0) {
                            if first_msg["role"] == "user" {
                                let cur = first_msg["content"].as_str().unwrap_or("");
                                first_msg["content"] =
                                    json!(format!("{}\n\n{}", inject_content, cur));
                            } else {
                                // è‹¥é¦–æ¡ä¸æ˜¯ userï¼Œåˆ™åˆ›å»ºä¸€æ¡æ–°çš„ user æ¶ˆæ¯æ‰¿è½½
                                messages.insert(
                                    0,
                                    json!({ "role": "user", "content": format!("{}", inject_content) })
                                );
                            }
                        } else {
                            messages.push(
                                json!({ "role": "user", "content": format!("{}", inject_content) }),
                            );
                        }
                    }
                }
            }
        }

        // é™çº§æ³¨å…¥å¯èƒ½è°ƒæ•´ messagesï¼Œç¡®ä¿è¯·æ±‚ä½“ä½¿ç”¨æœ€æ–°æ¶ˆæ¯é›†åˆ
        request_body["messages"] = serde_json::Value::Array(messages.clone());

        // æ ¹æ®æ¨¡å‹é€‚é…å™¨æ·»åŠ ç‰¹å®šå‚æ•°ï¼ˆåº”ç”¨ä¾›åº”å•†çº§åˆ«çš„ max_tokens é™åˆ¶ï¼‰
        let max_tokens = effective_max_tokens(config.max_output_tokens, config.max_tokens_limit);
        request_body["max_tokens"] = json!(max_tokens);
        request_body["temperature"] = json!(config.temperature);

        // è®°å½•è¯·æ±‚ä½“å¤§å°ä¸èµ·å§‹æ—¶é—´
        let request_json_str = serde_json::to_string(&request_body).unwrap_or_default();
        let request_bytes = request_json_str.len();
        let start_instant = std::time::Instant::now();

        // è¾“å‡ºå®Œæ•´è¯·æ±‚ä½“ç”¨äºè°ƒè¯•
        debug!("[LLM_CONTINUE_DEBUG] ==> å®Œæ•´è¯·æ±‚ä½“å¼€å§‹ <==");
        debug!(
            "{}",
            serde_json::to_string_pretty(&request_body).unwrap_or_default()
        );
        debug!("[LLM_CONTINUE_DEBUG] ==> å®Œæ•´è¯·æ±‚ä½“ç»“æŸ <==");

        debug!("å‘é€è¯·æ±‚åˆ°: {}", config.base_url);
        // ä½¿ç”¨ ProviderAdapter ç»Ÿä¸€æ„å»ºè¯·æ±‚ï¼ˆé¿å…è¦†ç›–åˆ†æ”¯ç¡¬ç¼–ç /chat/completionsï¼‰
        let adapter: Box<dyn ProviderAdapter> = match config.model_adapter.as_str() {
            "google" | "gemini" => Box::new(crate::providers::GeminiAdapter::new()),
            "anthropic" | "claude" => Box::new(crate::providers::AnthropicAdapter::new()),
            _ => Box::new(crate::providers::OpenAIAdapter),
        };
        let preq = adapter
            .build_request(
                &config.base_url,
                &config.api_key,
                &config.model,
                &request_body,
            )
            .map_err(|e| Self::provider_error("ç»­å†™è¯·æ±‚æ„å»ºå¤±è´¥", e))?;

        // â˜… åŒä¸»è·¯å¾„ï¼šä½¿ç”¨é€‚é…å™¨è½¬æ¢åçš„å®é™…è¯·æ±‚ä½“
        log_and_emit_llm_request(
            "CONTINUE_STREAM",
            &window,
            stream_event,
            &config.model,
            &preq.url,
            &preq.body,
        );

        let mut request_builder = self.client
            .post(&preq.url)
            .header("Accept", "text/event-stream, application/json, text/plain, */*")
            .header("Accept-Encoding", "identity")
            .header("Accept-Language", "zh-CN,zh;q=0.9,en;q=0.8")
            .header("User-Agent", "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36");
        for (k, v) in preq.headers {
            request_builder = request_builder.header(k, v);
        }

        if let Ok(parsed_url) = Url::parse(&config.base_url) {
            // config is a parameter here
            if (parsed_url.scheme() == "http" || parsed_url.scheme() == "https")
                && parsed_url.host_str().is_some()
            {
                let origin_val = format!(
                    "{}://{}",
                    parsed_url.scheme(),
                    parsed_url.host_str().unwrap_or_default()
                );
                let referer_val = format!(
                    "{}://{}/",
                    parsed_url.scheme(),
                    parsed_url.host_str().unwrap_or_default()
                );
                request_builder = request_builder
                    .header("Origin", origin_val)
                    .header("Referer", referer_val);
            }
        }

        // è¿›å…¥å‰å…ˆæ¸…ç©ºä¸€æ¬¡å¯èƒ½æ®‹ç•™çš„å–æ¶ˆæ ‡å¿—ï¼Œé¿å…æ–°è¯·æ±‚è¢«ç«‹å³ç»ˆæ­¢
        let _ = self.consume_pending_cancel(stream_event).await;

        // æ³¨å†Œå–æ¶ˆé€šé“ï¼ˆé€šçŸ¥å¼ä¸­æ–­ï¼‰
        let cancel_rx = self.register_cancel_channel(stream_event).await;

        // å‘å‡ºå¼€å§‹äº‹ä»¶
        let request_id = Uuid::new_v4().to_string();
        if let Err(e) = window.emit(
            &format!("{}_start", stream_event),
            &json!({
                "id": request_id,
                "model": config.model,
                "request_bytes": request_bytes
            }),
        ) {
            warn!("å‘é€å¼€å§‹äº‹ä»¶å¤±è´¥: {}", e);
        }

        let response = request_builder
            .json(&preq.body)
            .send()
            .await
            .map_err(|e| AppError::network(format!("è¯·æ±‚å¤±è´¥: {}", e)))?;

        // æµå¼å¤„ç†å“åº”ï¼ˆä½¿ç”¨ä¸call_unified_model_2_streamç›¸åŒçš„é€»è¾‘ï¼‰
        let mut stream = response.bytes_stream();
        let mut full_content = String::new();
        let mut reasoning_content = String::new();
        let mut chunk_counter = 0;
        let mut response_bytes: usize = 0;
        let mut was_cancelled = false;
        let mut captured_tool_calls: Vec<crate::models::ToolCall> = Vec::new();
        // æ•è· API è¿”å›çš„ usage ä¿¡æ¯ï¼ˆç”¨äºå‡†ç¡®è®°å½• token ä½¿ç”¨é‡ï¼‰
        let mut captured_usage: Option<serde_json::Value> = None;

        // å·¥å…·è°ƒç”¨èšåˆçŠ¶æ€ - ç”¨äºå¤„ç†æµå¼åˆ†å—çš„å·¥å…·è°ƒç”¨
        let mut pending_tool_calls: std::collections::HashMap<i32, (String, String, String)> =
            std::collections::HashMap::new(); // index -> (id, name, accumulated_args)
        let mut stream_ended = false;
        // åˆå§‹åŒ–SSEè¡Œç¼“å†²å™¨
        let mut sse_buffer = crate::utils::sse_buffer::SseLineBuffer::new();

        while let Some(chunk_result) = stream.next().await {
            // å…ˆä¸»åŠ¨æ¸…ç†ä¸€æ¬¡æ³¨å†Œè¡¨ä¸­çš„å–æ¶ˆæ ‡å¿—ï¼Œå†æ£€æŸ¥é€šé“ä¸­çš„é€šçŸ¥
            let registry_cancelled = self.consume_pending_cancel(stream_event).await;
            if *cancel_rx.borrow() || registry_cancelled {
                info!(
                    "[Cancel] Breaking stream loop for {} (custom config)",
                    stream_event
                );
                // P1ä¿®å¤ï¼šç”Ÿå‘½å‘¨æœŸå¯¹é½ - å‘é€cancelledäº‹ä»¶
                if let Err(e) = window.emit(
                    &format!("{}_cancelled", stream_event),
                    &json!({
                        "id": request_id,
                        "reason": "user_cancelled",
                        "timestamp": chrono::Utc::now().format("%Y-%m-%d %H:%M:%S%.3f").to_string()
                    }),
                ) {
                    warn!("å‘é€å–æ¶ˆäº‹ä»¶å¤±è´¥: {}", e);
                }
                was_cancelled = true;
                break;
            }
            match chunk_result {
                Ok(chunk) => {
                    response_bytes += chunk.len();
                    let chunk_str = String::from_utf8_lossy(&chunk);

                    // ä½¿ç”¨SSEç¼“å†²å™¨å¤„ç†chunkï¼Œè·å–å®Œæ•´çš„è¡Œ
                    let complete_lines = sse_buffer.process_chunk(&chunk_str);
                    for line in complete_lines {
                        // ä½¿ç”¨é€‚é…å™¨è§£ææµäº‹ä»¶ï¼ˆåŒ…æ‹¬[DONE]æ ‡è®°ï¼‰
                        let events = adapter.parse_stream(&line);

                        // æ£€æŸ¥æ˜¯å¦æ˜¯ç»“æŸæ ‡è®°ï¼ˆä¿ç•™ä¸ºåå¤‡æœºåˆ¶ï¼‰
                        if crate::utils::sse_buffer::SseLineBuffer::check_done_marker(&line) {
                            debug!("æ£€æµ‹åˆ°SSEç»“æŸæ ‡è®°: [DONE]");
                            if events.is_empty() {
                                // å¦‚æœé€‚é…å™¨æ²¡æœ‰ç”ŸæˆDoneäº‹ä»¶ï¼Œæˆ‘ä»¬æ‰‹åŠ¨æ·»åŠ ä¸€ä¸ª
                                debug!("é€‚é…å™¨æœªç”ŸæˆDoneäº‹ä»¶ï¼Œæ‰‹åŠ¨æ·»åŠ ");
                                stream_ended = true;
                                break;
                            }
                        }
                        for event in events {
                            match event {
                                crate::providers::StreamEvent::ContentChunk(content) => {
                                    full_content.push_str(&content);
                                    chunk_counter += 1;

                                    let stream_chunk = StreamChunk {
                                        content: content.clone(),
                                        is_complete: false,
                                        chunk_id: format!("{}_chunk_{}", request_id, chunk_counter),
                                    };

                                    if let Err(e) = window.emit(stream_event, &stream_chunk) {
                                        error!("å‘é€å†…å®¹å—å¤±è´¥: {}", e);
                                    }
                                }
                                crate::providers::StreamEvent::ReasoningChunk(reasoning) => {
                                    reasoning_content.push_str(&reasoning);

                                    let reasoning_chunk = StreamChunk {
                                        content: reasoning.clone(),
                                        is_complete: false,
                                        chunk_id: format!(
                                            "{}_reasoning_chunk_{}",
                                            request_id, chunk_counter
                                        ),
                                    };

                                    if let Err(e) = window.emit(
                                        &format!("{}_reasoning", stream_event),
                                        &reasoning_chunk,
                                    ) {
                                        warn!("å‘é€æ€ç»´é“¾å—å¤±è´¥: {}", e);
                                    }
                                }
                                crate::providers::StreamEvent::ThoughtSignature(_signature) => {
                                    // Gemini 3 æ€ç»´ç­¾åï¼ˆæ­¤å‡½æ•°ä¸ä½¿ç”¨ hookï¼Œç›´æ¥å¿½ç•¥ï¼‰
                                    // ç­¾ååœ¨å·¥å…·è°ƒç”¨åœºæ™¯ä¸‹éœ€è¦ç¼“å­˜ï¼Œä½†æ­¤å‡½æ•°ç”¨äº v2 pipeline
                                }
                                crate::providers::StreamEvent::ToolCall(tool_call_value) => {
                                    // èšåˆåˆ†å—çš„å·¥å…·è°ƒç”¨ï¼ˆä¸å†å‘é€åŸå§‹åˆ†å—äº‹ä»¶ï¼‰
                                    if let Some(index) = tool_call_value
                                        .get("index")
                                        .and_then(|v| v.as_i64())
                                        .map(|v| v as i32)
                                    {
                                        if let Some(id) =
                                            tool_call_value.get("id").and_then(|v| v.as_str())
                                        {
                                            // è¿™æ˜¯ä¸€ä¸ªæ–°çš„å·¥å…·è°ƒç”¨çš„å¼€å§‹ï¼ˆæœ‰å®Œæ•´çš„idï¼‰
                                            let name = tool_call_value
                                                .get("function")
                                                .and_then(|f| f.get("name"))
                                                .and_then(|n| n.as_str())
                                                .unwrap_or("unknown");
                                            // ğŸ”§ ä¿®å¤ï¼šæŸäº› OpenAI å…¼å®¹ API è¿”å› arguments ä¸º JSON å¯¹è±¡è€Œéå­—ç¬¦ä¸²
                                            let args = tool_call_value
                                                .get("function")
                                                .and_then(|f| f.get("arguments"))
                                                .map(|a| {
                                                    if let Some(s) = a.as_str() {
                                                        s.to_string()
                                                    } else if a.is_null() {
                                                        String::new()
                                                    } else {
                                                        warn!("[llm_manager] å·¥å…·è°ƒç”¨ arguments ä¸æ˜¯å­—ç¬¦ä¸²è€Œæ˜¯ JSON å€¼ (tool={}), è‡ªåŠ¨åºåˆ—åŒ–", name);
                                                        serde_json::to_string(a).unwrap_or_default()
                                                    }
                                                })
                                                .unwrap_or_default();

                                            pending_tool_calls.insert(
                                                index,
                                                (
                                                    id.to_string(),
                                                    name.to_string(),
                                                    args,
                                                ),
                                            );
                                            // ç®€åŒ–æ—¥å¿—ï¼šå·¥å…·è°ƒç”¨å¼€å§‹æ—¶è¾“å‡ºä¸€æ¬¡
                                            print!("ğŸ”§");
                                            use std::io::Write;
                                            let _ = std::io::stdout().flush();
                                        } else if let Some((id, name, mut accumulated_args)) =
                                            pending_tool_calls.get(&index).cloned()
                                        {
                                            // è¿™æ˜¯å·¥å…·è°ƒç”¨çš„åç»­å—ï¼ˆæ²¡æœ‰idï¼Œåªæœ‰argumentsç‰‡æ®µï¼‰
                                            // ğŸ”§ ä¿®å¤ï¼šåŒæ ·å¤„ç† arguments ä¸º JSON å¯¹è±¡çš„æƒ…å†µ
                                            let args_fragment_opt = tool_call_value
                                                .get("function")
                                                .and_then(|f| f.get("arguments"))
                                                .and_then(|a| {
                                                    if let Some(s) = a.as_str() {
                                                        Some(s.to_string())
                                                    } else if a.is_null() {
                                                        None
                                                    } else {
                                                        Some(serde_json::to_string(a).unwrap_or_default())
                                                    }
                                                });
                                            if let Some(args_fragment) = args_fragment_opt {
                                                accumulated_args.push_str(&args_fragment);
                                                pending_tool_calls.insert(
                                                    index,
                                                    (id, name, accumulated_args.clone()),
                                                );
                                                // ç®€åŒ–æ—¥å¿—ï¼šæ¯ 200 å­—ç¬¦è¾“å‡ºä¸€ä¸ª / ä»£è¡¨ç´¯ç§¯
                                                if accumulated_args.len() % 200
                                                    < args_fragment.len()
                                                {
                                                    print!("/");
                                                    use std::io::Write;
                                                    let _ = std::io::stdout().flush();
                                                }
                                            }
                                        }
                                    }
                                }
                                crate::providers::StreamEvent::Usage(usage_value) => {
                                    // å­˜å‚¨ usage æ•°æ®
                                    captured_usage = Some(usage_value.clone());
                                    if let Err(e) = window
                                        .emit(&format!("{}_usage", stream_event), &usage_value)
                                    {
                                        error!("å‘é€ç”¨é‡äº‹ä»¶å¤±è´¥: {}", e);
                                    }
                                }
                                crate::providers::StreamEvent::SafetyBlocked(safety_info) => {
                                    // emit safety_blocked äº‹ä»¶
                                    if let Err(e) = window.emit(
                                        &format!("{}_safety_blocked", stream_event),
                                        &safety_info,
                                    ) {
                                        error!("å‘é€å®‰å…¨é˜»æ–­äº‹ä»¶å¤±è´¥: {}", e);
                                    }
                                    // åŒæ—¶å‘é€é€šç”¨é”™è¯¯äº‹ä»¶
                                    let error_event = json!({
                                        "type": "safety_error",
                                        "message": "Request blocked due to safety policies",
                                        "details": safety_info
                                    });
                                    if let Err(e) = window
                                        .emit(&format!("{}_error", stream_event), &error_event)
                                    {
                                        error!("å‘é€å®‰å…¨é”™è¯¯äº‹ä»¶å¤±è´¥: {}", e);
                                    }
                                }
                                crate::providers::StreamEvent::Done => {
                                    stream_ended = true;

                                    // å®Œæˆå¾…èšåˆçš„å·¥å…·è°ƒç”¨ï¼ˆåªåœ¨æœ‰å·¥å…·è°ƒç”¨æ—¶è¾“å‡ºç®€æ´æ—¥å¿—ï¼‰
                                    if !pending_tool_calls.is_empty() {
                                        debug!("å·¥å…·è°ƒç”¨åºåˆ—ç»“æŸ");
                                    }
                                    for (_index, (id, name, accumulated_args)) in
                                        pending_tool_calls.iter()
                                    {
                                        let complete_tool_call = serde_json::json!({
                                            "id": id,
                                            "type": "function",
                                            "function": {
                                                "name": name,
                                                "arguments": accumulated_args
                                            }
                                        });

                                        match Self::convert_openai_tool_call(&complete_tool_call) {
                                            Ok(tc) => {
                                                captured_tool_calls.push(tc);
                                            }
                                            Err(e) => {
                                                warn!("[llm_manager] å·¥å…·è°ƒç”¨è§£æå¤±è´¥: {}, args_len={}", e, accumulated_args.len());
                                                // æ„é€ å¸¦æˆªæ–­é”™è¯¯æ ‡è®°çš„ ToolCallï¼Œè®© pipeline å±‚åé¦ˆç»™ LLM é‡è¯•
                                                captured_tool_calls.push(crate::models::ToolCall {
                                                    id: id.clone(),
                                                    tool_name: name.clone(),
                                                    args_json: json!({
                                                        "_truncation_error": true,
                                                        "_error_message": format!(
                                                            "å·¥å…·è°ƒç”¨å‚æ•° JSON è¢«æˆªæ–­ï¼ˆå·²ç”Ÿæˆ {} å­—ç¬¦ä½†æœªå®Œæˆï¼‰ã€‚åŸå› ï¼šæ¨¡å‹è¾“å‡º token è¾¾åˆ°ä¸Šé™ã€‚",
                                                            accumulated_args.len()
                                                        ),
                                                        "_args_len": accumulated_args.len(),
                                                    }),
                                                });
                                            }
                                        }
                                    }
                                    // è¾“å‡ºä¸€æ¡ç®€æ´çš„å·¥å…·è°ƒç”¨æ€»ç»“
                                    if !captured_tool_calls.is_empty() {
                                        let names: Vec<_> = captured_tool_calls
                                            .iter()
                                            .map(|tc| tc.tool_name.as_str())
                                            .collect();
                                        debug!("å·¥å…·è°ƒç”¨èšåˆå®Œæˆ: {:?}", names);
                                    }
                                    pending_tool_calls.clear();

                                    break;
                                }
                            }
                        }

                        if stream_ended {
                            break;
                        }
                    }
                }
                Err(e) => {
                    error!("æµå¼å“åº”é”™è¯¯: {}", e);
                    break;
                }
            }
        }

        // å¤„ç†SSEç¼“å†²å™¨ä¸­å‰©ä½™çš„ä¸å®Œæ•´è¡Œï¼ˆP1ä¿®å¤ï¼šSTREAM-3ï¼‰
        if let Some(remaining_line) = sse_buffer.flush() {
            if !remaining_line.trim().is_empty() {
                debug!("å¤„ç†SSEç¼“å†²å™¨ä¸­çš„å‰©ä½™æ•°æ®: {} å­—ç¬¦", remaining_line.len());
                // ä½¿ç”¨é€‚é…å™¨è§£æå‰©ä½™çš„è¡Œ
                let events = adapter.parse_stream(&remaining_line);
                for event in events {
                    match event {
                        crate::providers::StreamEvent::ContentChunk(content) => {
                            full_content.push_str(&content);
                            chunk_counter += 1;

                            let stream_chunk = StreamChunk {
                                content: content.clone(),
                                is_complete: false,
                                chunk_id: format!("{}_chunk_{}", request_id, chunk_counter),
                            };

                            if let Err(e) = window.emit(stream_event, &stream_chunk) {
                                error!("å‘é€å‰©ä½™å†…å®¹å—å¤±è´¥: {}", e);
                            }
                        }
                        crate::providers::StreamEvent::ReasoningChunk(reasoning) => {
                            reasoning_content.push_str(&reasoning);

                            let reasoning_chunk = StreamChunk {
                                content: reasoning.clone(),
                                is_complete: false,
                                chunk_id: format!(
                                    "{}_reasoning_chunk_{}",
                                    request_id, chunk_counter
                                ),
                            };

                            if let Err(e) = window
                                .emit(&format!("{}_reasoning", stream_event), &reasoning_chunk)
                            {
                                warn!("å‘é€å‰©ä½™æ€ç»´é“¾å—å¤±è´¥: {}", e);
                            }
                        }
                        _ => { /* å¿½ç•¥å…¶ä»–äº‹ä»¶ç±»å‹ï¼ˆDone/ToolCall/Usageç­‰å·²åœ¨ä¸»å¾ªç¯å¤„ç†ï¼‰ */
                        }
                    }
                }
            }
        }

        // æ¸…ç†å–æ¶ˆé€šé“
        self.clear_cancel_channel(stream_event).await;

        // ğŸ”§ [REFACTOR] æ—§çš„å·¥å…·è°ƒç”¨æ‰§è¡Œé€»è¾‘å·²ç§»é™¤
        // å·¥å…·è°ƒç”¨ç°åœ¨ç”± Chat V2 Pipeline ç»Ÿä¸€å¤„ç†ï¼ˆsrc-tauri/src/chat_v2/pipeline.rsï¼‰
        // æ­¤å‡½æ•°åªè´Ÿè´£æµå¼å“åº”çš„æ”¶é›†ï¼Œå·¥å…·è°ƒç”¨é€šè¿‡ LLMStreamHooks å›è°ƒç»™ä¸Šå±‚

        if was_cancelled {
            // P1ä¿®å¤ï¼šç”Ÿå‘½å‘¨æœŸå¯¹é½ - å‘é€ä¸“é—¨çš„cancelledäº‹ä»¶ï¼ŒåŒæ—¶ä¿æŒendäº‹ä»¶
            if let Err(e) = window.emit(
                &format!("{}_cancelled", stream_event),
                &json!({
                    "id": request_id,
                    "reason": "user_cancelled",
                    "timestamp": chrono::Utc::now().format("%Y-%m-%d %H:%M:%S%.3f").to_string()
                }),
            ) {
                warn!("å‘é€å–æ¶ˆäº‹ä»¶å¤±è´¥: {}", e);
            }

            // å–æ¶ˆï¼šä»å‘é€ end äº‹ä»¶ç”¨äºå…¼å®¹æ€§
            let duration_ms = start_instant.elapsed().as_millis();
            if let Err(e) = window.emit(
                &format!("{}_end", stream_event),
                &json!({
                    "reason": "cancelled",
                    "stats": {
                        "chunk_count": chunk_counter,
                        "request_bytes": request_bytes,
                        "response_bytes": response_bytes,
                        "duration_ms": duration_ms,
                        "approx_tokens_in": 0,
                        "approx_tokens_out": 0,
                        "retry_count": 0
                    }
                }),
            ) {
                warn!("å‘é€ç»“æŸäº‹ä»¶å¤±è´¥: {}", e);
            }
        } else {
            // æˆåŠŸï¼šå‘é€å®Œæˆå—ä¸ end(success)
            let final_chunk = StreamChunk {
                content: full_content.clone(),
                is_complete: true,
                chunk_id: format!("final_chunk_{}", chunk_counter),
            };
            if let Err(e) = window.emit(stream_event, &final_chunk) {
                error!("å‘é€æœ€ç»ˆå®Œæˆä¿¡å·å¤±è´¥: {}", e);
            }
            // å¦‚æœæœ‰æ€ç»´é“¾å†…å®¹ï¼Œä¹Ÿå‘é€æ€ç»´é“¾å®Œæˆä¿¡å·
            if enable_chain_of_thought && !reasoning_content.is_empty() {
                let reasoning_final_chunk = StreamChunk {
                    content: reasoning_content.clone(),
                    is_complete: true,
                    chunk_id: format!("reasoning_final_chunk_{}", chunk_counter + 1),
                };
                if let Err(e) = window.emit(
                    &format!("{}_reasoning", stream_event),
                    &reasoning_final_chunk,
                ) {
                    error!("å‘é€æ€ç»´é“¾å®Œæˆä¿¡å·å¤±è´¥: {}", e);
                }
            }
            // end(success) äº‹ä»¶åœ¨ç»Ÿä¸€ç»Ÿè®¡æ®µå‘é€
        }

        // ç»“æŸäº‹ä»¶ï¼ˆé™„å¸¦ç»Ÿè®¡ä¿¡æ¯ï¼‰
        let duration_ms = start_instant.elapsed().as_millis();
        let approx_tokens_out = crate::utils::token_budget::estimate_tokens(&full_content);
        if let Err(e) = window.emit(
            &format!("{}_end", stream_event),
            &json!({
                "reason": "success",
                "stats": {
                    "chunk_count": chunk_counter,
                    "request_bytes": request_bytes,
                    "response_bytes": response_bytes,
                    "duration_ms": duration_ms,
                    "approx_tokens_in": 0,
                    "approx_tokens_out": approx_tokens_out,
                    "retry_count": 0
                }
            }),
        ) {
            warn!("å‘é€ç»“æŸäº‹ä»¶å¤±è´¥: {}", e);
        }

        // ç”¨é‡æ—¥å¿—ï¼šç»“æŸï¼ˆè„±æ•å†™å…¥ï¼Œä½¿ç”¨ FileManagerï¼‰
        {
            let approx_tokens_out = crate::utils::token_budget::estimate_tokens(&full_content);
            let dur = start_instant.elapsed().as_millis();
            let dir = self.file_manager.get_app_data_dir().to_path_buf();
            let logger = crate::debug_logger::DebugLogger::new(dir);

            // ä» API è¿”å›çš„ usage æ•°æ®ä¸­æå–å®é™… token æ•°é‡
            let (actual_prompt_tokens, actual_completion_tokens, reasoning_tokens) =
                Self::extract_usage_tokens(
                    &captured_usage,
                    approx_tokens_out,
                    (request_bytes / 4).max(1),
                );

            let _ = logger
                .log_llm_usage(
                    "end",
                    &config.name,
                    &config.model,
                    &config.model_adapter,
                    request_bytes,
                    response_bytes,
                    actual_prompt_tokens as usize,
                    actual_completion_tokens as usize,
                    Some(dur),
                    None,
                )
                .await;

            crate::llm_usage::record_llm_usage(
                crate::llm_usage::CallerType::ChatV2,
                &config.model,
                actual_prompt_tokens,
                actual_completion_tokens,
                reasoning_tokens,
                None,
                Some(stream_event.to_string()),
                Some(dur as u64),
                !was_cancelled,
                if was_cancelled {
                    Some("cancelled".to_string())
                } else {
                    None
                },
            );
        }

        // æ„å»ºæ€ç»´é“¾è¯¦æƒ…
        let chain_of_thought_details = if enable_chain_of_thought {
            if config.is_reasoning {
                Some(json!({
                    "full_response": full_content,
                    "reasoning_content": if reasoning_content.is_empty() { Value::Null } else { json!(reasoning_content) },
                    "enabled": true,
                    "is_reasoning_model": true,
                    "model_adapter": config.model_adapter
                }))
            } else {
                None
            }
        } else {
            None
        };

        Ok(StandardModel2Output {
            assistant_message: if was_cancelled {
                String::new()
            } else {
                full_content
            },
            raw_response: Some("stream_response".to_string()),
            chain_of_thought_details,
            cancelled: was_cancelled,
        })
    }
    // ç»Ÿä¸€AIæ¥å£å±‚ - æ¨¡å‹äºŒï¼ˆæ ¸å¿ƒè§£æ/å¯¹è¯ï¼‰- éæµå¼ç‰ˆæœ¬ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
    pub async fn call_unified_model_2(
        &self,
        context: &HashMap<String, Value>,
        chat_history: &[ChatMessage],
        subject: &str,
        enable_chain_of_thought: bool,
        image_paths: Option<Vec<String>>,
        task_context: Option<&str>,
        max_input_tokens_override: Option<usize>,
    ) -> Result<StandardModel2Output> {
        info!(
            "è°ƒç”¨ç»Ÿä¸€æ¨¡å‹äºŒæ¥å£: ç§‘ç›®={}, æ€ç»´é“¾={}, å›¾ç‰‡æ•°é‡={}",
            subject,
            enable_chain_of_thought,
            image_paths.as_ref().map(|p| p.len()).unwrap_or(0)
        );

        let _max_input_tokens_override = max_input_tokens_override;

        // è·å–æ¨¡å‹é…ç½®
        // Model Router: choose model by task_context when possible
        let (config, _enable_cot) = {
            let task = match task_context {
                Some(tc) if tc.contains("planner") => "review",
                // ğŸš€ ä¿®å¤ï¼šæ·»åŠ tag_generationçš„è·¯ç”±æ”¯æŒ
                Some(tc) if tc == "tag_generation" || tc.contains("tag") => "tag_generation",
                _ => "default",
            };
            self.select_model_for(task, None, None, None, None, None, None)
                .await
                .unwrap_or((self.get_model2_config().await?, true))
        };

        // å¤„ç†å›¾ç‰‡ï¼ˆå¦‚æœæ¨¡å‹æ”¯æŒå¤šæ¨¡æ€ä¸”æä¾›äº†å›¾ç‰‡ï¼‰
        // ç§»é™¤ä¼šè¯çº§å›¾ç‰‡å›é€€ï¼Œä¸å†ä» image_paths è¯»å–
        let images_base64: Option<Vec<String>> = None;

        let mut messages = vec![];

        // è·å–ç§‘ç›®ä¸“ç”¨çš„Prompt
        let mut subject_prompt = self.get_subject_prompt(subject, "model2");

        // æ·»åŠ ä»»åŠ¡ä¸Šä¸‹æ–‡
        if let Some(context_str) = task_context {
            subject_prompt = format!("{}\n\nä»»åŠ¡ä¸Šä¸‹æ–‡: {}", subject_prompt, context_str);
        }

        // æ„å»ºç³»ç»Ÿæ¶ˆæ¯ï¼ŒåŒ…å«ç ”ç©¶/é¢˜ç›®ä¿¡æ¯ä¸å¯é€‰ç ”ç©¶ç‰‡æ®µ
        let system_content = format!(
            "{}\n\né¢˜ç›®ä¿¡æ¯:\nOCRæ–‡æœ¬: {}\næ ‡ç­¾: {:?}\né¢˜ç›®ç±»å‹: {}\nç”¨æˆ·åŸé—®é¢˜: {}",
            subject_prompt,
            context
                .get("ocr_text")
                .and_then(|v| v.as_str())
                .unwrap_or(""),
            context
                .get("tags")
                .and_then(|v| v.as_array())
                .unwrap_or(&vec![]),
            context
                .get("mistake_type")
                .and_then(|v| v.as_str())
                .unwrap_or(""),
            context
                .get("user_question")
                .and_then(|v| v.as_str())
                .unwrap_or("")
        );

        // ç¦æ­¢ RAG æ–‡æœ¬æ‹¼æ¥

        // ä¸æ³¨å…¥ latest_user_query åˆ° system

        // å¯¹äºæ¨ç†æ¨¡å‹ï¼Œç³»ç»Ÿæ¶ˆæ¯éœ€è¦åˆå¹¶åˆ°ç”¨æˆ·æ¶ˆæ¯ä¸­
        if config.is_reasoning {
            // æ¨ç†æ¨¡å‹ä¸æ”¯æŒç³»ç»Ÿæ¶ˆæ¯ï¼Œéœ€è¦å°†ç³»ç»Ÿæç¤ºåˆå¹¶åˆ°ç”¨æˆ·æ¶ˆæ¯ä¸­
            let combined_content = format!("{}", system_content);

            if config.is_multimodal && images_base64.is_some() && chat_history.is_empty() {
                let mut content = vec![json!({
                    "type": "text",
                    "text": combined_content
                })];

                if let Some(images) = &images_base64 {
                    for image_base64 in images {
                        let image_format = Self::detect_image_format_from_base64(image_base64);
                        debug!("æ£€æµ‹åˆ°å›¾åƒæ ¼å¼: {}", image_format);
                        content.push(json!({
                            "type": "image_url",
                            "image_url": {
                                "url": format!("data:image/{};base64,{}", image_format, image_base64)
                            }
                        }));
                    }
                }

                messages.push(json!({
                    "role": "user",
                    "content": content
                }));
            } else if chat_history.is_empty() {
                messages.push(json!({
                    "role": "user",
                    "content": combined_content
                }));
            }
        } else {
            // éæ¨ç†æ¨¡å‹ä½¿ç”¨æ ‡å‡†çš„ç³»ç»Ÿæ¶ˆæ¯
            messages.push(json!({
                "role": "system",
                "content": system_content
            }));
            // åç»­ä¸¥ç¦å†æ³¨å…¥"ä¼ª system æ–‡æœ¬"æˆ–æç¤º
        }

        // æ·»åŠ èŠå¤©å†å²ï¼ˆåŒ…å«æ¯æ¡ user çš„ image_base64 å¤šæ¨¡æ€ parts æ„å»ºï¼‰
        // ğŸ”§ C3ä¿®å¤ï¼šè¡¥å…… tool_call/tool_result å¤„ç†ï¼ˆä¹‹å‰å®Œå…¨ä¸¢å¼ƒå·¥å…·è°ƒç”¨ä¿¡æ¯ï¼‰
        for msg in chat_history {
            if msg.role == "user" {
                if config.is_multimodal
                    && msg
                        .image_base64
                        .as_ref()
                        .map(|v| !v.is_empty())
                        .unwrap_or(false)
                {
                    let mut parts = vec![json!({"type":"text","text": msg.content})];
                    if let Some(images) = &msg.image_base64 {
                        for image_base64 in images {
                            let image_format = Self::detect_image_format_from_base64(image_base64);
                            parts.push(json!({
                                "type": "image_url",
                                "image_url": {"url": format!("data:image/{};base64,{}", image_format, image_base64)}
                            }));
                        }
                    }
                    messages.push(json!({"role":"user","content": parts}));
                } else {
                    messages.push(json!({"role": "user", "content": msg.content}));
                }
            } else if msg.role == "assistant" {
                if let Some(tc) = &msg.tool_call {
                    let tool_call_obj = json!({
                        "id": tc.id,
                        "type": "function",
                        "function": {
                            "name": tc.tool_name,
                            "arguments": tc.args_json.to_string()
                        }
                    });
                    messages.push(json!({
                        "role": "assistant",
                        "content": msg.content,
                        "tool_calls": [tool_call_obj]
                    }));
                } else {
                    messages.push(json!({"role": "assistant", "content": msg.content}));
                }
            } else if msg.role == "tool" {
                if let Some(tr) = &msg.tool_result {
                    messages.push(json!({
                        "role": "tool",
                        "tool_call_id": tr.call_id,
                        "content": msg.content
                    }));
                } else {
                    // é™çº§å…œåº•
                    messages.push(json!({"role": "assistant", "content": msg.content}));
                }
            } else {
                messages.push(json!({"role": msg.role, "content": msg.content}));
            }
        }

        // ğŸ”§ é˜²å¾¡æ€§åˆå¹¶ï¼šè¿ç»­ assistant tool_callsï¼ˆæ­£å¸¸æµç¨‹ä¸­æ˜¯ no-opï¼‰
        Self::merge_consecutive_assistant_tool_calls(&mut messages);
        // ğŸ”§ é˜²å¾¡æ€§åˆå¹¶ï¼šè¿ç»­ user æ¶ˆæ¯åˆå¹¶
        Self::merge_consecutive_user_messages(&mut messages);

        let mut request_body = json!({
            "model": config.model,
            "messages": messages,
            "stream": false  // éæµå¼ç‰ˆæœ¬
        });

        Self::apply_reasoning_config(&mut request_body, &config, None);

        // æ ¹æ®æ¨¡å‹é€‚é…å™¨ç±»å‹è®¾ç½®ä¸åŒçš„å‚æ•°
        if cfg!(debug_assertions) {
            // debug removed
        }

        // åº”ç”¨ä¾›åº”å•†çº§åˆ«çš„ max_tokens é™åˆ¶
        let max_tokens = effective_max_tokens(config.max_output_tokens, config.max_tokens_limit);
        if config.is_reasoning {
            request_body["max_completion_tokens"] = json!(max_tokens);
            debug!("åº”ç”¨æ¨ç†æ¨¡å‹å‚æ•°: max_completion_tokens={}", max_tokens);
        } else {
            request_body["max_tokens"] = json!(max_tokens);
            request_body["temperature"] = json!(config.temperature);
        }

        // ä½¿ç”¨ ProviderAdapter æ„å»ºè¯·æ±‚ï¼Œç¡®ä¿ Gemini æ¨¡å‹èµ°è½¬æ¢åçš„URL/Headers/Body
        let adapter: Box<dyn ProviderAdapter> = if self.should_use_openai_responses(&config) {
            Box::new(crate::providers::OpenAIResponsesAdapter)
        } else {
            match config.model_adapter.as_str() {
                "google" | "gemini" => Box::new(crate::providers::GeminiAdapter::new()),
                "anthropic" | "claude" => Box::new(crate::providers::AnthropicAdapter::new()),
                _ => Box::new(crate::providers::OpenAIAdapter),
            }
        };
        let preq = adapter
            .build_request(
                &config.base_url,
                &config.api_key,
                &config.model,
                &request_body,
            )
            .map_err(|e| Self::provider_error("èŠå¤©è¯·æ±‚æ„å»ºå¤±è´¥", e))?;

        log_llm_request_audit("CHAT_V2_STREAM", &preq.url, &config.model, &request_body);

        let mut request_builder = self.client
            .post(&preq.url)
            .header("Accept", "text/event-stream, application/json, text/plain, */*")
            .header("Accept-Encoding", "identity")  // ç¦ç”¨å‹ç¼©ï¼Œé¿å…äºŒè¿›åˆ¶å“åº”
            .header("Accept-Language", "zh-CN,zh;q=0.9,en;q=0.8")
            .header("User-Agent", "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36");
        for (k, v) in preq.headers {
            request_builder = request_builder.header(k, v);
        }

        if let Ok(parsed_url) = Url::parse(&config.base_url) {
            if (parsed_url.scheme() == "http" || parsed_url.scheme() == "https")
                && parsed_url.host_str().is_some()
            {
                let origin_val = format!(
                    "{}://{}",
                    parsed_url.scheme(),
                    parsed_url.host_str().unwrap_or_default()
                );
                let referer_val = format!(
                    "{}://{}/",
                    parsed_url.scheme(),
                    parsed_url.host_str().unwrap_or_default()
                );
                request_builder = request_builder
                    .header("Origin", origin_val)
                    .header("Referer", referer_val);
            }
        }

        let response = request_builder
            .json(&preq.body)
            .send()
            .await
            .map_err(|e| AppError::network(format!("æ¨¡å‹äºŒAPIè¯·æ±‚å¤±è´¥: {}", e)))?;

        if !response.status().is_success() {
            let status = response.status();
            let error_text = response.text().await.unwrap_or_default();
            let error_msg = format!("æ¨¡å‹äºŒAPIè¯·æ±‚å¤±è´¥: {} - {}", status, error_text);
            // éæµå¼ç‰ˆæœ¬æ²¡æœ‰ stream_event/window ä¸Šä¸‹æ–‡ï¼Œè¿™é‡Œä»…è¿”å›é”™è¯¯
            error!("æ¨¡å‹äºŒAPIè¯·æ±‚å¤±è´¥(éæµå¼): {}", error_msg);
            return Err(AppError::llm(error_msg));
        }

        let response_text = response
            .text()
            .await
            .map_err(|e| AppError::llm(format!("è¯»å–æ¨¡å‹äºŒå“åº”å¤±è´¥: {}", e)))?;
        let response_bytes = response_text.len();
        let response_json: Value = serde_json::from_str(&response_text)
            .map_err(|e| AppError::llm(format!("è§£ææ¨¡å‹äºŒå“åº”å¤±è´¥: {}", e)))?;

        // Gemini éæµå¼å“åº”ç»Ÿä¸€è½¬æ¢ä¸º OpenAI å½¢çŠ¶
        let openai_like_json = if config.model_adapter == "google" {
            // éæµå¼ï¼šå…ˆæ£€æµ‹å®‰å…¨é˜»æ–­
            if let Some(safety_msg) = Self::extract_gemini_safety_error(&response_json) {
                return Err(AppError::llm(safety_msg));
            }
            match crate::adapters::gemini_openai_converter::convert_gemini_nonstream_response_to_openai(&response_json, &config.model) {
                Ok(v) => v,
                Err(e) => return Err(AppError::llm(format!("Geminiå“åº”è½¬æ¢å¤±è´¥: {}", e))),
            }
        } else if matches!(config.model_adapter.as_str(), "anthropic" | "claude") {
            crate::providers::convert_anthropic_response_to_openai(&response_json, &config.model)
                .ok_or_else(|| AppError::llm("è§£æAnthropicå“åº”å¤±è´¥".to_string()))?
        } else if self.should_use_openai_responses(&config) {
            let mut text_segments: Vec<String> = Vec::new();
            if let Some(output) = response_json.get("output").and_then(|v| v.as_array()) {
                for item in output {
                    if let Some(content_arr) = item.get("content").and_then(|v| v.as_array()) {
                        for entry in content_arr {
                            let entry_type =
                                entry.get("type").and_then(|v| v.as_str()).unwrap_or("");
                            if matches!(entry_type, "output_text" | "text") {
                                if let Some(text) = entry.get("text").and_then(|v| v.as_str()) {
                                    if !text.is_empty() {
                                        text_segments.push(text.to_string());
                                    }
                                }
                            }
                        }
                    }
                }
            }
            if text_segments.is_empty() {
                if let Some(output_text) = response_json.get("output_text").and_then(|v| v.as_str())
                {
                    if !output_text.is_empty() {
                        text_segments.push(output_text.to_string());
                    }
                }
            }
            json!({
                "choices": [{
                    "message": {
                        "content": text_segments.join("")
                    }
                }],
                "usage": response_json.get("usage").cloned()
            })
        } else {
            response_json.clone()
        };

        let content = openai_like_json["choices"][0]["message"]["content"]
            .as_str()
            .ok_or_else(|| AppError::llm("æ— æ³•è§£ææ¨¡å‹äºŒAPIå“åº”"))?;

        // å¦‚æœå¯ç”¨äº†æ€ç»´é“¾ï¼Œå°è¯•æå–æ€ç»´é“¾è¯¦æƒ…
        let chain_of_thought_details = if enable_chain_of_thought {
            // è¿™é‡Œå¯ä»¥æ ¹æ®å“åº”å†…å®¹è§£ææ€ç»´é“¾æ­¥éª¤
            // æš‚æ—¶å°†å®Œæ•´å“åº”ä½œä¸ºæ€ç»´é“¾è¯¦æƒ…
            Some(json!({
                "full_response": content,
                "enabled": true,
                "is_reasoning_model": config.is_reasoning,
                "model_adapter": config.model_adapter
            }))
        } else {
            None
        };

        // ç”¨é‡æ—¥å¿—ï¼šç»“æŸï¼ˆç®€åŒ–æ§åˆ¶å°è¾“å‡ºï¼‰
        let approx_tokens_out = crate::utils::token_budget::estimate_tokens(content);
        debug!(
            "[model2_non_stream] bytes_out={}, approx_tokens_out={}",
            response_bytes, approx_tokens_out
        );

        Ok(StandardModel2Output {
            assistant_message: content.to_string(),
            raw_response: Some(openai_like_json.to_string()),
            chain_of_thought_details,
            cancelled: false,
        })
    }
    pub async fn generate_chat_metadata(
        &self,
        _subject: &str, // subject å·²åºŸå¼ƒ
        user_question: &str,
        conversation_preview: Option<&str>,
        attachment_names: &[String],
    ) -> Result<crate::models::ChatMetadata> {
        let normalized_question = user_question.trim();
        let preview = conversation_preview.unwrap_or("").trim();

        let mut prompt_body = format!(
            "é¦–è½®ç”¨æˆ·è¾“å…¥ï¼š\n{}",
            if normalized_question.is_empty() {
                "(æ— æ–‡æœ¬ï¼Œä»…é™„ä»¶æˆ–å…¶ä»–è¾“å…¥)"
            } else {
                normalized_question
            }
        );

        if !preview.is_empty() {
            prompt_body.push_str("\n\nè¡¥å……ä¸Šä¸‹æ–‡ï¼š\n");
            prompt_body.push_str(preview);
        }

        let attachment_list: Vec<String> = attachment_names
            .iter()
            .map(|name| name.trim())
            .filter(|name| !name.is_empty())
            .map(|name| name.to_string())
            .collect();
        if !attachment_list.is_empty() {
            prompt_body.push_str("\n\né™„ä»¶åˆ—è¡¨ï¼š\n");
            for name in &attachment_list {
                prompt_body.push_str("- ");
                prompt_body.push_str(name);
                prompt_body.push('\n');
            }
        }

        let system_prompt =
            "ä½ æ˜¯ä¸€åå¯¹èŠå¤©ä¼šè¯ç”Ÿæˆç»“æ„åŒ–å…ƒæ•°æ®çš„åŠ©ç†ã€‚åªè¾“å‡ºJSONï¼Œä¸è¦é¢å¤–æ–‡å­—ã€‚\n\n".to_string()
                + "è¯·è¾“å‡ºä¸€ä¸ªJSONå¯¹è±¡ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š\n"
                + "- title: ç®€æ´çš„ä¸­æ–‡æ ‡é¢˜ï¼ˆ<=20å­—ï¼‰ï¼Œæ¦‚æ‹¬èŠå¤©ä¸»é¢˜ï¼›\n"
                + "- summary: 1-2å¥ä¸­æ–‡æ¦‚è¦ï¼Œè‹¥ä¿¡æ¯ä¸è¶³å¯çœç•¥ï¼›\n"
                + "- tags: ä¸­æ–‡æ ‡ç­¾æ•°ç»„ï¼ˆ<=3ä¸ªï¼Œè‹¥æ— åˆé€‚æ ‡ç­¾åˆ™ä¸ºç©ºæ•°ç»„ï¼‰ï¼›\n"
                + "- attributes: å¯é€‰å¯¹è±¡ï¼Œè®°å½•é¢å¤–é”®å€¼å¯¹ï¼Œä¾‹å¦‚{\"intent\":\"è§„åˆ’\"}ã€‚";

        let (config, _) = self
            .select_model_for("chat_title", None, Some(0.1), None, None, None, None)
            .await?;
        let api_key = self.decrypt_api_key_if_needed(&config.api_key)?;

        let request_body = json!({
            "model": config.model,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt_body}
            ],
            "temperature": config.temperature.max(0.1),
            "stream": false
        });

        let adapter: Box<dyn ProviderAdapter> = match config.model_adapter.as_str() {
            "google" | "gemini" => Box::new(crate::providers::GeminiAdapter::new()),
            "anthropic" | "claude" => Box::new(crate::providers::AnthropicAdapter::new()),
            _ => Box::new(crate::providers::OpenAIAdapter),
        };

        let preq = adapter
            .build_request(
                config.base_url.trim_end_matches('/'),
                &api_key,
                &config.model,
                &request_body,
            )
            .map_err(|e| Self::provider_error("ç”ŸæˆèŠå¤©å…ƒæ•°æ®è¯·æ±‚æ„å»ºå¤±è´¥", e))?;

        log_llm_request_audit("METADATA", &preq.url, &config.model, &request_body);

        let mut request_builder = self.client.post(&preq.url);
        for (key, value) in preq.headers.iter() {
            request_builder = request_builder.header(key, value);
        }

        if let Ok(parsed_url) = Url::parse(&config.base_url) {
            if (parsed_url.scheme() == "http" || parsed_url.scheme() == "https")
                && parsed_url.host_str().is_some()
            {
                let origin_val = format!(
                    "{}://{}",
                    parsed_url.scheme(),
                    parsed_url.host_str().unwrap_or_default()
                );
                let referer_val = format!(
                    "{}://{}/",
                    parsed_url.scheme(),
                    parsed_url.host_str().unwrap_or_default()
                );
                request_builder = request_builder
                    .header("Origin", origin_val)
                    .header("Referer", referer_val);
            }
        }

        let response = request_builder
            .json(&preq.body)
            .send()
            .await
            .map_err(|e| AppError::network(format!("èŠå¤©å…ƒæ•°æ®ç”Ÿæˆè¯·æ±‚å¤±è´¥: {}", e)))?;

        if !response.status().is_success() {
            let status = response.status();
            let error_body = response.text().await.unwrap_or_default();
            return Err(AppError::llm(format!(
                "èŠå¤©å…ƒæ•°æ®ç”Ÿæˆå¤±è´¥: {} - {}",
                status, error_body
            )));
        }

        let response_text = response
            .text()
            .await
            .map_err(|e| AppError::llm(format!("è¯»å–èŠå¤©å…ƒæ•°æ®å“åº”å¤±è´¥: {}", e)))?;
        let response_json: Value = serde_json::from_str(&response_text)
            .map_err(|e| AppError::llm(format!("è§£æèŠå¤©å…ƒæ•°æ®å“åº”å¤±è´¥: {}", e)))?;

        let openai_like_json = if config.model_adapter == "google" {
            if let Some(safety_msg) = Self::extract_gemini_safety_error(&response_json) {
                return Err(AppError::llm(safety_msg));
            }
            match crate::adapters::gemini_openai_converter::convert_gemini_nonstream_response_to_openai(&response_json, &config.model) {
                Ok(v) => v,
                Err(e) => return Err(AppError::llm(format!("Geminiå“åº”è½¬æ¢å¤±è´¥: {}", e))),
            }
        } else if matches!(config.model_adapter.as_str(), "anthropic" | "claude") {
            crate::providers::convert_anthropic_response_to_openai(&response_json, &config.model)
                .ok_or_else(|| AppError::llm("è§£æAnthropicå“åº”å¤±è´¥".to_string()))?
        } else {
            response_json.clone()
        };

        let content = openai_like_json["choices"][0]["message"]["content"]
            .as_str()
            .ok_or_else(|| AppError::llm("èŠå¤©å…ƒæ•°æ®æ¨¡å‹è¿”å›å†…å®¹ä¸ºç©º"))?;

        fn extract_json_block(raw: &str) -> Option<String> {
            let trimmed = raw.trim();
            let cleaned = if trimmed.starts_with("```") {
                trimmed
                    .trim_start_matches("```json")
                    .trim_start_matches("```JSON")
                    .trim_start_matches("```")
                    .trim_end_matches("```")
                    .trim()
                    .to_string()
            } else {
                trimmed.to_string()
            };

            if serde_json::from_str::<Value>(&cleaned).is_ok() {
                return Some(cleaned);
            }

            if let (Some(start), Some(end)) = (cleaned.find('{'), cleaned.rfind('}')) {
                if end > start {
                    let candidate = &cleaned[start..=end];
                    if serde_json::from_str::<Value>(candidate).is_ok() {
                        return Some(candidate.to_string());
                    }
                }
            }

            None
        }

        let json_block = extract_json_block(content)
            .ok_or_else(|| AppError::llm("æœªèƒ½ä»èŠå¤©å…ƒæ•°æ®å“åº”ä¸­æå–JSON"))?;

        let metadata_value: Value = serde_json::from_str(&json_block)
            .map_err(|e| AppError::llm(format!("è§£æèŠå¤©å…ƒæ•°æ®JSONå¤±è´¥: {}", e)))?;

        let mut title = metadata_value
            .get("title")
            .and_then(|v| v.as_str())
            .map(|s| s.trim().to_string())
            .unwrap_or_else(|| normalized_question.chars().take(20).collect());
        if title.is_empty() {
            title = normalized_question.chars().take(20).collect();
        }

        let summary = metadata_value
            .get("summary")
            .and_then(|v| v.as_str())
            .map(|s| {
                let trimmed = s.trim();
                if trimmed.is_empty() {
                    None
                } else {
                    Some(trimmed.to_string())
                }
            })
            .flatten();

        let tags: Vec<String> = metadata_value
            .get("tags")
            .and_then(|v| v.as_array())
            .map(|arr| {
                arr.iter()
                    .filter_map(|item| item.as_str())
                    .map(|s| s.trim().to_string())
                    .filter(|s| !s.is_empty())
                    .take(3)
                    .collect::<Vec<String>>()
            })
            .unwrap_or_else(Vec::new);

        let attributes = metadata_value.get("attributes").and_then(|v| {
            if v.is_object() {
                Some(v.clone())
            } else {
                None
            }
        });

        Ok(crate::models::ChatMetadata {
            title,
            summary,
            tags,
            attributes,
            note: None,
        })
    }

    pub async fn test_connection(&self, api_key: &str, base_url: &str) -> Result<bool> {
        self.test_connection_with_model(api_key, base_url, None)
            .await
    }

    // æµ‹è¯•APIè¿æ¥ - å¯ä»¥æŒ‡å®šå…·ä½“æ¨¡å‹
    pub async fn test_connection_with_model(
        &self,
        api_key: &str,
        base_url: &str,
        model_name: Option<&str>,
    ) -> Result<bool> {
        info!("æµ‹è¯•APIè¿æ¥: {} (å¯†é’¥é•¿åº¦: {})", base_url, api_key.len());

        // ç¡®ä¿base_urlæ ¼å¼æ­£ç¡®
        let normalized_url = if base_url.ends_with('/') {
            base_url.trim_end_matches('/').to_string()
        } else {
            base_url.to_string()
        };

        // å¦‚æœæŒ‡å®šäº†æ¨¡å‹åç§°ï¼Œæ£€æŸ¥æ¨¡å‹ç±»å‹å¹¶ä½¿ç”¨é€‚å½“çš„æµ‹è¯•æ–¹æ³•
        if let Some(model) = model_name {
            let lower_model = model.to_lowercase();

            // åµŒå…¥æ¨¡å‹æµ‹è¯•
            if lower_model.contains("embedding")
                || lower_model.contains("bge-")
                || lower_model.contains("embed")
            {
                return self
                    .test_embedding_model(api_key, &normalized_url, model)
                    .await;
            }

            // é‡æ’åºæ¨¡å‹æµ‹è¯•
            if lower_model.contains("rerank") || lower_model.contains("reranker") {
                return self
                    .test_reranker_model(api_key, &normalized_url, model)
                    .await;
            }

            // å¯¹è¯æ¨¡å‹æµ‹è¯•ï¼ˆé»˜è®¤ï¼‰
            return self
                .test_chat_model(api_key, &normalized_url, Some(model))
                .await;
        }

        // æœªæŒ‡å®šæ¨¡å‹æ—¶ï¼Œä½¿ç”¨é€šç”¨æµ‹è¯•
        self.test_chat_model(api_key, &normalized_url, None).await
    }

    // æµ‹è¯•åµŒå…¥æ¨¡å‹
    async fn test_embedding_model(
        &self,
        api_key: &str,
        base_url: &str,
        model: &str,
    ) -> Result<bool> {
        info!("æµ‹è¯•åµŒå…¥æ¨¡å‹: {}", model);

        let request_body = json!({
            "model": model,
            "input": ["æµ‹è¯•åµŒå…¥è¿æ¥"],
            "encoding_format": "float"
        });

        let timeout_duration = std::time::Duration::from_secs(15);
        let request_future = self
            .client
            .post(&format!("{}/embeddings", base_url))
            .header("Authorization", format!("Bearer {}", api_key))
            .header("Content-Type", "application/json")
            .header(
                "Accept",
                "text/event-stream, application/json, text/plain, */*",
            )
            .header("Accept-Encoding", "identity")
            .json(&request_body)
            .send();

        match tokio::time::timeout(timeout_duration, request_future).await {
            Ok(Ok(response)) => {
                let status = response.status();
                debug!("åµŒå…¥æ¨¡å‹æµ‹è¯•å“åº”çŠ¶æ€: {} (æ¨¡å‹: {})", status, model);

                if status.is_success() {
                    info!("åµŒå…¥æ¨¡å‹æµ‹è¯•æˆåŠŸï¼æ¨¡å‹: {}", model);
                    Ok(true)
                } else {
                    let error_text = response.text().await.unwrap_or_default();
                    warn!("åµŒå…¥æ¨¡å‹æµ‹è¯•å¤±è´¥: {} - {}", status, error_text);
                    Ok(false)
                }
            }
            Ok(Err(e)) => {
                error!("åµŒå…¥æ¨¡å‹æµ‹è¯•è¯·æ±‚é”™è¯¯: {}", e);
                Err(AppError::network(format!("åµŒå…¥æ¨¡å‹æµ‹è¯•å¤±è´¥: {}", e)))
            }
            Err(_) => {
                warn!("åµŒå…¥æ¨¡å‹æµ‹è¯•è¶…æ—¶");
                Err(AppError::network("åµŒå…¥æ¨¡å‹æµ‹è¯•è¶…æ—¶"))
            }
        }
    }

    // æµ‹è¯•é‡æ’åºæ¨¡å‹
    async fn test_reranker_model(
        &self,
        api_key: &str,
        base_url: &str,
        model: &str,
    ) -> Result<bool> {
        info!("æµ‹è¯•é‡æ’åºæ¨¡å‹: {}", model);

        let request_body = json!({
            "model": model,
            "query": "æµ‹è¯•æŸ¥è¯¢",
            "documents": ["æµ‹è¯•æ–‡æ¡£1", "æµ‹è¯•æ–‡æ¡£2"],
            "top_k": 2,
            "return_documents": true
        });

        let timeout_duration = std::time::Duration::from_secs(15);
        let request_future = self
            .client
            .post(&format!("{}/rerank", base_url))
            .header("Authorization", format!("Bearer {}", api_key))
            .header("Content-Type", "application/json")
            .header(
                "Accept",
                "text/event-stream, application/json, text/plain, */*",
            )
            .header("Accept-Encoding", "identity")
            .json(&request_body)
            .send();

        match tokio::time::timeout(timeout_duration, request_future).await {
            Ok(Ok(response)) => {
                let status = response.status();
                debug!("é‡æ’åºæ¨¡å‹æµ‹è¯•å“åº”çŠ¶æ€: {} (æ¨¡å‹: {})", status, model);

                if status.is_success() {
                    info!("é‡æ’åºæ¨¡å‹æµ‹è¯•æˆåŠŸï¼æ¨¡å‹: {}", model);
                    Ok(true)
                } else {
                    let error_text = response.text().await.unwrap_or_default();
                    warn!("é‡æ’åºæ¨¡å‹æµ‹è¯•å¤±è´¥: {} - {}", status, error_text);
                    Ok(false)
                }
            }
            Ok(Err(e)) => {
                error!("é‡æ’åºæ¨¡å‹æµ‹è¯•è¯·æ±‚é”™è¯¯: {}", e);
                Err(AppError::network(format!("é‡æ’åºæ¨¡å‹æµ‹è¯•å¤±è´¥: {}", e)))
            }
            Err(_) => {
                warn!("é‡æ’åºæ¨¡å‹æµ‹è¯•è¶…æ—¶");
                Err(AppError::network("é‡æ’åºæ¨¡å‹æµ‹è¯•è¶…æ—¶"))
            }
        }
    }
    // æµ‹è¯•å¯¹è¯æ¨¡å‹
    async fn test_chat_model(
        &self,
        api_key: &str,
        base_url: &str,
        model_name: Option<&str>,
    ) -> Result<bool> {
        // å¦‚æœæŒ‡å®šäº†æ¨¡å‹åç§°ï¼Œä¼˜å…ˆä½¿ç”¨æŒ‡å®šçš„æ¨¡å‹
        let test_models = if let Some(specified_model) = model_name {
            vec![specified_model.to_string()]
        } else {
            // ä½¿ç”¨é€šç”¨çš„æµ‹è¯•æ¨¡å‹åç§°ï¼Œä¸åŒAPIæä¾›å•†å¯èƒ½æ”¯æŒä¸åŒçš„æ¨¡å‹
            vec![
                "gpt-3.5-turbo".to_string(),                 // OpenAI
                "Qwen/Qwen2-7B-Instruct".to_string(),        // SiliconFlow
                "meta-llama/Llama-2-7b-chat-hf".to_string(), // å…¶ä»–
            ]
        };

        // å°è¯•ä¸åŒçš„æ¨¡å‹è¿›è¡Œæµ‹è¯•
        for model in test_models {
            let request_body = json!({
                "model": model,
                "messages": [
                    {
                        "role": "user",
                        "content": "Hi"
                    }
                ],
                "max_tokens": 5,
                "temperature": 0.1
            });

            debug!("å°è¯•æ¨¡å‹: {}", model);

            // ä½¿ç”¨ ProviderAdapter æ„å»ºè¯·æ±‚ï¼ˆæ”¯æŒ Gemini ä¸­è½¬ï¼‰
            let lower_base = base_url.to_lowercase();
            let lower_model = model.to_lowercase();
            let adapter: Box<dyn ProviderAdapter> =
                if lower_model.contains("claude") || lower_model.contains("anthropic") {
                    Box::new(crate::providers::AnthropicAdapter::new())
                } else if lower_model.contains("gemini")
                    || lower_base.contains("generativelanguage.googleapis.com")
                {
                    Box::new(crate::providers::GeminiAdapter::new())
                } else {
                    Box::new(crate::providers::OpenAIAdapter)
                };
            let preq = adapter
                .build_request(base_url, api_key, &model, &request_body)
                .map_err(|e| Self::provider_error("API è¿é€šæ€§æµ‹è¯•è¯·æ±‚æ„å»ºå¤±è´¥", e))?;

            log_llm_request_audit("TEST_CHAT", &preq.url, &model, &request_body);

            let mut request_builder = self.client
                .post(&preq.url)
                .header("Accept", "text/event-stream, application/json, text/plain, */*")
                .header("Accept-Encoding", "identity")  // ç¦ç”¨å‹ç¼©ï¼Œé¿å…äºŒè¿›åˆ¶å“åº”
                .header("Accept-Language", "zh-CN,zh;q=0.9,en;q=0.8")
                .header("User-Agent", "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36");

            // æ·»åŠ adapterè¿”å›çš„headersï¼ˆåŒ…æ‹¬Authorizationï¼‰
            for (k, v) in preq.headers {
                request_builder = request_builder.header(k, v);
            }

            if let Ok(parsed_url) = Url::parse(base_url) {
                if (parsed_url.scheme() == "http" || parsed_url.scheme() == "https")
                    && parsed_url.host_str().is_some()
                {
                    let origin_val = format!(
                        "{}://{}",
                        parsed_url.scheme(),
                        parsed_url.host_str().unwrap_or_default()
                    );
                    let referer_val = format!(
                        "{}://{}/",
                        parsed_url.scheme(),
                        parsed_url.host_str().unwrap_or_default()
                    );
                    request_builder = request_builder
                        .header("Origin", origin_val)
                        .header("Referer", referer_val);
                }
            }

            // ä½¿ç”¨tokioçš„timeoutåŒ…è£…æ•´ä¸ªè¯·æ±‚
            let timeout_duration = std::time::Duration::from_secs(15);
            let request_future = request_builder.json(&preq.body).send();

            // ä½¿ç”¨tokio::time::timeout
            match tokio::time::timeout(timeout_duration, request_future).await {
                Ok(Ok(response)) => {
                    let status = response.status();
                    debug!("APIè¿æ¥æµ‹è¯•å“åº”çŠ¶æ€: {} (æ¨¡å‹: {})", status, model);

                    if status.is_success() {
                        // è§£æä¸€æ¬¡ï¼Œè‹¥ä¸º Gemini åˆ™è½¬æ¢ä¸º OpenAI å½¢çŠ¶å†æ£€æŸ¥ content
                        match response.json::<serde_json::Value>().await {
                            Ok(resp_json) => {
                                let is_google = lower_model.contains("gemini")
                                    || lower_base.contains("generativelanguage.googleapis.com");
                                let openai_like = if is_google {
                                    crate::adapters::gemini_openai_converter::convert_gemini_nonstream_response_to_openai(&resp_json, &model).unwrap_or(resp_json)
                                } else {
                                    resp_json
                                };
                                let _ = openai_like["choices"][0]["message"]["content"]
                                    .as_str()
                                    .unwrap_or("");
                                info!("APIè¿æ¥æµ‹è¯•æˆåŠŸï¼ä½¿ç”¨æ¨¡å‹: {}", model);
                                return Ok(true);
                            }
                            Err(e) => {
                                warn!("APIè¿æ¥æµ‹è¯•è§£æå¤±è´¥: {}", e);
                                return Ok(false);
                            }
                        }
                    } else if status == 400 {
                        // 400é”™è¯¯å¯èƒ½æ˜¯æ¨¡å‹ä¸æ”¯æŒï¼Œå°è¯•ä¸‹ä¸€ä¸ª
                        let error_text = response.text().await.unwrap_or_default();
                        warn!("æ¨¡å‹ {} ä¸æ”¯æŒï¼Œé”™è¯¯: {}", model, error_text);
                        debug!("è¯·æ±‚URL: {}", preq.url);
                        debug!(
                            "è¯·æ±‚ä½“: {}",
                            serde_json::to_string_pretty(&preq.body).unwrap_or_default()
                        );
                        // å¦‚æœæ˜¯ç”¨æˆ·æŒ‡å®šçš„æ¨¡å‹ï¼Œç›´æ¥è¿”å›å¤±è´¥å¹¶æä¾›è¯¦ç»†é”™è¯¯
                        if model_name.is_some() {
                            return Err(AppError::validation(format!(
                                "APIè¯·æ±‚å¤±è´¥ (çŠ¶æ€ç : 400):\nè¯·æ±‚URL: {}\né”™è¯¯å“åº”: {}\nå¯èƒ½åŸå› : æ¨¡å‹ä¸æ”¯æŒæˆ–å‚æ•°é”™è¯¯",
                                preq.url, error_text
                            )));
                        }
                        continue;
                    } else if status == 401 {
                        // 401æ˜¯è®¤è¯é”™è¯¯ï¼Œä¸éœ€è¦å°è¯•å…¶ä»–æ¨¡å‹
                        let error_text = response.text().await.unwrap_or_default();
                        error!("APIå¯†é’¥è®¤è¯å¤±è´¥: {}", status);
                        debug!("è¯·æ±‚URL: {}", preq.url);
                        debug!("è®¤è¯é”™è¯¯è¯¦æƒ…: {}", error_text);
                        return Err(AppError::validation(format!(
                            "APIè®¤è¯å¤±è´¥ (çŠ¶æ€ç : 401):\nè¯·æ±‚URL: {}\né”™è¯¯å“åº”: {}\nè¯·æ£€æŸ¥APIå¯†é’¥æ˜¯å¦æ­£ç¡®",
                            preq.url, error_text
                        )));
                    } else {
                        // å…¶ä»–é”™è¯¯
                        let error_text = response.text().await.unwrap_or_default();
                        error!("APIè¯·æ±‚å¤±è´¥: {} - {}", status, error_text);
                        debug!("è¯·æ±‚URL: {}", preq.url);
                        debug!(
                            "è¯·æ±‚ä½“: {}",
                            serde_json::to_string_pretty(&preq.body).unwrap_or_default()
                        );
                        // å¦‚æœæ˜¯ç”¨æˆ·æŒ‡å®šçš„æ¨¡å‹ï¼Œç›´æ¥è¿”å›å¤±è´¥å¹¶æä¾›è¯¦ç»†é”™è¯¯
                        if model_name.is_some() {
                            return Err(AppError::validation(format!(
                                "APIè¯·æ±‚å¤±è´¥ (çŠ¶æ€ç : {}):\nè¯·æ±‚URL: {}\né”™è¯¯å“åº”: {}",
                                status, preq.url, error_text
                            )));
                        }
                        continue;
                    }
                }
                Ok(Err(e)) => {
                    error!("APIè¿æ¥æµ‹è¯•è¯·æ±‚é”™è¯¯ (æ¨¡å‹: {}): {}", model, e);
                    // å¦‚æœæ˜¯è¿æ¥é”™è¯¯ï¼Œä¸éœ€è¦å°è¯•å…¶ä»–æ¨¡å‹
                    if e.to_string().contains("handshake") || e.to_string().contains("connect") {
                        return Err(AppError::network(format!("è¿æ¥å¤±è´¥: {}", e)));
                    }
                    // å¦‚æœæ˜¯ç”¨æˆ·æŒ‡å®šçš„æ¨¡å‹ï¼Œç›´æ¥è¿”å›å¤±è´¥
                    if model_name.is_some() {
                        return Err(AppError::network(format!("è¯·æ±‚å¤±è´¥: {}", e)));
                    }
                    continue;
                }
                Err(_) => {
                    warn!("APIè¿æ¥æµ‹è¯•è¶…æ—¶ (æ¨¡å‹: {})", model);
                    // å¦‚æœæ˜¯ç”¨æˆ·æŒ‡å®šçš„æ¨¡å‹ï¼Œç›´æ¥è¿”å›å¤±è´¥
                    if model_name.is_some() {
                        return Err(AppError::network("è¯·æ±‚è¶…æ—¶"));
                    }
                    continue;
                }
            }
        }

        warn!("æ‰€æœ‰æµ‹è¯•æ¨¡å‹éƒ½å¤±è´¥äº†");
        Ok(false)
    }

    // === æ— ç³»ç»Ÿæç¤ºçš„ç®€åŒ–æ¨¡å‹äºŒè°ƒç”¨ ===
    /// ç›´æ¥ä½¿ç”¨ç”¨æˆ·æä¾›çš„ promptï¼Œä¸é™„åŠ ä»»ä½•ç³»ç»Ÿæç¤ºï¼Œé€‚ç”¨äºä¸¥æ ¼æ ¼å¼è¾“å‡ºçš„ä»»åŠ¡ï¼ˆå¦‚æ‰¹é‡åˆ†æ”¯é€‰æ‹© / ç²¾ç¡®æ ‡ç­¾æ˜ å°„ï¼‰ã€‚
    pub async fn call_model2_raw_prompt(
        &self,
        user_prompt: &str,
        image_payloads: Option<Vec<ImagePayload>>,
    ) -> Result<StandardModel2Output> {
        let config = self.get_model2_config().await?;
        self.call_raw_prompt_with_config(config, user_prompt, image_payloads).await
    }

    /// ä½¿ç”¨è®°å¿†å†³ç­–æ¨¡å‹è°ƒç”¨ï¼ˆå›é€€é“¾ï¼šmemory_decision_model â†’ model2ï¼‰
    pub async fn call_memory_decision_raw_prompt(
        &self,
        user_prompt: &str,
    ) -> Result<StandardModel2Output> {
        let config = self.get_memory_decision_model_config().await?;
        self.call_raw_prompt_with_config(config, user_prompt, None).await
    }

    /// ä½¿ç”¨æ ‡é¢˜/æ ‡ç­¾ç”Ÿæˆæ¨¡å‹è°ƒç”¨ï¼ˆå›é€€é“¾ï¼šchat_title_model â†’ model2ï¼‰
    pub async fn call_chat_title_raw_prompt(
        &self,
        user_prompt: &str,
    ) -> Result<StandardModel2Output> {
        let config = self.get_chat_title_model_config().await?;
        self.call_raw_prompt_with_config(config, user_prompt, None).await
    }

    /// å†…éƒ¨æ–¹æ³•ï¼šä½¿ç”¨æ˜¾å¼ä¼ å…¥çš„ ApiConfig æ‰§è¡Œ raw prompt è°ƒç”¨
    async fn call_raw_prompt_with_config(
        &self,
        config: ApiConfig,
        user_prompt: &str,
        image_payloads: Option<Vec<ImagePayload>>,
    ) -> Result<StandardModel2Output> {
        // æ„é€ æœ€ç®€æ¶ˆæ¯ï¼Œä»…åŒ…å«ç”¨æˆ·æŒ‡ä»¤
        let mut content_parts = vec![json!({
            "type": "text",
            "text": user_prompt
        })];

        let requested_image_count = image_payloads.as_ref().map(|v| v.len()).unwrap_or(0);
        let mut attached_payloads: Vec<ImagePayload> = Vec::new();

        if let Some(images) = image_payloads {
            if config.is_multimodal {
                for payload in images {
                    content_parts.push(json!({
                        "type": "image_url",
                        "image_url": {
                            "url": format!(
                                "data:{};base64,{}",
                                payload.mime.as_str(),
                                payload.base64.as_str()
                            )
                        }
                    }));
                    attached_payloads.push(payload);
                }
            } else if !images.is_empty() {
                warn!(
                    "å½“å‰æ¨¡å‹({})æœªæ ‡è®°ä¸ºå¤šæ¨¡æ€ï¼Œå¿½ç•¥ {} å¼ å›¾ç‰‡",
                    config.model,
                    images.len()
                );
            }
        }

        let messages = vec![json!({
            "role": "user",
            "content": content_parts
        })];

        // 3. ç»„è£…è¯·æ±‚ä½“
        let mut request_body = json!({
            "model": config.model,
            "messages": messages,
            "stream": false,
            "temperature": config.temperature
        });

        // `max_total_tokens` ä½œä¸ºå…¼å®¹å­—æ®µä¿ç•™ç»™ Gemini é€‚é…å™¨ï¼›åŒæ—¶é’ˆå¯¹ä¸åŒæ¨¡å‹ç±»å‹ä¼ é€’å®˜æ–¹æ¨èçš„ä¸Šé™å‚æ•°ã€‚
        // åº”ç”¨ä¾›åº”å•†çº§åˆ«çš„ max_tokens é™åˆ¶
        let max_tokens = effective_max_tokens(config.max_output_tokens, config.max_tokens_limit);
        request_body["max_total_tokens"] = json!(max_tokens);
        if config.is_reasoning {
            request_body["max_completion_tokens"] = json!(max_tokens);
        } else {
            request_body["max_tokens"] = json!(max_tokens);
        }

        Self::apply_reasoning_config(&mut request_body, &config, None);

        // å¦‚æœæ˜¯ OpenAI GPT æ¨¡å‹ï¼Œå¯ç”¨ JSON strict æ¨¡å¼
        if config.model.starts_with("gpt-") {
            request_body["response_format"] = json!({"type": "json_object"});
        }

        debug!(
            "[RAW_PROMPT] å‘é€ç®€åŒ–è¯·æ±‚åˆ°: {} (ç»é€‚é…å™¨) | è¯·æ±‚å›¾ç‰‡æ•°: {} | å®é™…é™„åŠ : {} | promptå­—ç¬¦æ•°: {}",
            config.base_url,
            requested_image_count,
            attached_payloads.len(),
            user_prompt.chars().count()
        );
        for (idx, payload) in attached_payloads.iter().enumerate().take(3) {
            debug!(
                "[RAW_PROMPT_DEBUG] image[{}]: mime={}, base64_length={}",
                idx,
                payload.mime.as_str(),
                payload.base64.len()
            );
        }

        // 4. é€šè¿‡ ProviderAdapter æ„é€  HTTP è¯·æ±‚
        let adapter: Box<dyn ProviderAdapter> = match config.model_adapter.as_str() {
            "google" | "gemini" => Box::new(crate::providers::GeminiAdapter::new()),
            "anthropic" | "claude" => Box::new(crate::providers::AnthropicAdapter::new()),
            _ => Box::new(crate::providers::OpenAIAdapter),
        };
        let preq = adapter
            .build_request(
                &config.base_url,
                &config.api_key,
                &config.model,
                &request_body,
            )
            .map_err(|e| Self::provider_error("RAW prompt è¯·æ±‚æ„å»ºå¤±è´¥", e))?;

        log_llm_request_audit("RAW_PROMPT", &preq.url, &config.model, &request_body);

        let mut request_builder = self.client
            .post(&preq.url)
            .header("Accept", "text/event-stream, application/json, text/plain, */*")
            .header("Accept-Encoding", "identity")
            .header("Accept-Language", "zh-CN,zh;q=0.9,en;q=0.8")
            .header("User-Agent", "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36");
        for (k, v) in preq.headers {
            request_builder = request_builder.header(k, v);
        }

        // è®¾ç½® Origin/Referer å¤´ï¼ˆä¸å…¶å®ƒè°ƒç”¨ä¿æŒä¸€è‡´ï¼‰
        if let Ok(parsed_url) = Url::parse(&config.base_url) {
            if (parsed_url.scheme() == "http" || parsed_url.scheme() == "https")
                && parsed_url.host_str().is_some()
            {
                let origin_val = format!(
                    "{}://{}",
                    parsed_url.scheme(),
                    parsed_url.host_str().unwrap_or_default()
                );
                let referer_val = format!(
                    "{}://{}/",
                    parsed_url.scheme(),
                    parsed_url.host_str().unwrap_or_default()
                );
                request_builder = request_builder
                    .header("Origin", origin_val)
                    .header("Referer", referer_val);
            }
        }

        // 5. å‘é€è¯·æ±‚
        let response = request_builder
            .json(&preq.body)
            .send()
            .await
            .map_err(|e| AppError::network(format!("RAW_PROMPT APIè¯·æ±‚å¤±è´¥: {}", e)))?;

        // 6. æ£€æŸ¥å“åº”çŠ¶æ€
        if !response.status().is_success() {
            let status = response.status();
            let error_text = response.text().await.unwrap_or_default();
            return Err(AppError::llm(format!(
                "RAW_PROMPT APIè¯·æ±‚å¤±è´¥: {} - {}",
                status, error_text
            )));
        }

        // 7. è§£æå“åº”
        let response_json: serde_json::Value = response
            .json()
            .await
            .map_err(|e| AppError::llm(format!("è§£æRAW_PROMPTå“åº”å¤±è´¥: {}", e)))?;

        // Gemini éæµå¼å“åº”ç»Ÿä¸€è½¬æ¢ä¸º OpenAI å½¢çŠ¶
        let openai_like_json = if config.model_adapter == "google" {
            if let Some(safety_msg) = Self::extract_gemini_safety_error(&response_json) {
                return Err(AppError::llm(safety_msg));
            }
            match crate::adapters::gemini_openai_converter::convert_gemini_nonstream_response_to_openai(&response_json, &config.model) {
                Ok(v) => v,
                Err(e) => return Err(AppError::llm(format!("Geminiå“åº”è½¬æ¢å¤±è´¥: {}", e))),
            }
        } else if matches!(config.model_adapter.as_str(), "anthropic" | "claude") {
            crate::providers::convert_anthropic_response_to_openai(&response_json, &config.model)
                .ok_or_else(|| AppError::llm("è§£æAnthropicå“åº”å¤±è´¥".to_string()))?
        } else {
            response_json.clone()
        };

        let assistant_message = openai_like_json["choices"][0]["message"]["content"]
            .as_str()
            .unwrap_or("")
            .to_string();

        Ok(StandardModel2Output {
            assistant_message,
            raw_response: Some(openai_like_json.to_string()),
            chain_of_thought_details: None,
            cancelled: false,
        })
    }

    /// ä½¿ç”¨ OCR æ¨¡å‹è°ƒç”¨ï¼Œé€‚ç”¨äºå¤šæ¨¡æ€ç´¢å¼•çš„ OCR ä»»åŠ¡
    pub async fn call_ocr_model_raw_prompt(
        &self,
        user_prompt: &str,
        image_payloads: Option<Vec<ImagePayload>>,
    ) -> Result<StandardModel2Output> {
        // 1. è·å– OCR æ¨¡å‹é…ç½®åŠå…¶æœ‰æ•ˆå¼•æ“ï¼Œç¡®ä¿é€‚é…å™¨ä¸å®é™…æ¨¡å‹ä¸€è‡´
        let (config, effective_engine) = self.get_ocr_config_with_effective_engine().await?;
        let ocr_adapter = crate::ocr_adapters::OcrAdapterFactory::create(effective_engine);
        let ocr_mode = crate::ocr_adapters::OcrMode::FreeOcr;
        let prompt_text = ocr_adapter.build_custom_prompt(user_prompt, ocr_mode);

        // 2. æ„é€ æ¶ˆæ¯ï¼ˆâš ï¸ DeepSeek-OCR è¦æ±‚ï¼šå›¾ç‰‡åœ¨å‰ã€æ–‡æœ¬åœ¨åï¼‰
        let mut content_parts: Vec<serde_json::Value> = Vec::new();

        let requested_image_count = image_payloads.as_ref().map(|v| v.len()).unwrap_or(0);
        let mut attached_payloads: Vec<ImagePayload> = Vec::new();

        // å…ˆæ·»åŠ å›¾ç‰‡ï¼ˆå¿…é¡»åœ¨æ–‡æœ¬ä¹‹å‰ï¼‰
        if let Some(images) = image_payloads {
            if config.is_multimodal {
                for payload in images {
                    content_parts.push(json!({
                        "type": "image_url",
                        "image_url": {
                            "url": format!(
                                "data:{};base64,{}",
                                payload.mime.as_str(),
                                payload.base64.as_str()
                            ),
                            "detail": if ocr_adapter.requires_high_detail() { "high" } else { "low" }
                        }
                    }));
                    attached_payloads.push(payload);
                }
            } else if !images.is_empty() {
                warn!(
                    "OCRæ¨¡å‹({})æœªæ ‡è®°ä¸ºå¤šæ¨¡æ€ï¼Œå¿½ç•¥ {} å¼ å›¾ç‰‡",
                    config.model,
                    images.len()
                );
            }
        }

        // å†æ·»åŠ æ–‡æœ¬ prompt
        content_parts.push(json!({
            "type": "text",
            "text": prompt_text
        }));

        let messages = vec![json!({
            "role": "user",
            "content": content_parts
        })];

        // 3. ç»„è£…è¯·æ±‚ä½“
        let mut request_body = json!({
            "model": config.model,
            "messages": messages,
            "stream": false,
            "temperature": ocr_adapter.recommended_temperature()  // OCR ä»»åŠ¡ä½¿ç”¨ç¡®å®šæ€§è¾“å‡º
        });

        // OCR ä»»åŠ¡ï¼šå…ˆåº”ç”¨ä¾›åº”å•†é™åˆ¶ï¼Œå†åº”ç”¨ OCR ä¸“ç”¨çš„ [2048, 8000] èŒƒå›´
        let max_tokens = effective_max_tokens(config.max_output_tokens, config.max_tokens_limit)
            .min(ocr_adapter.recommended_max_tokens(ocr_mode))
            .max(2048)
            .min(8000);
        request_body["max_tokens"] = json!(max_tokens);

        if let Some(extra) = ocr_adapter.get_extra_request_params() {
            if let Some(obj) = request_body.as_object_mut() {
                if let Some(extra_obj) = extra.as_object() {
                    for (k, v) in extra_obj {
                        obj.insert(k.to_string(), v.clone());
                    }
                } else {
                    obj.insert("extra_params".to_string(), extra);
                }
            }
        }

        if let Some(repetition_penalty) = ocr_adapter.recommended_repetition_penalty() {
            if let Some(obj) = request_body.as_object_mut() {
                obj.insert("repetition_penalty".to_string(), json!(repetition_penalty));
            }
        }

        // GLM-4.5+ æ”¯æŒ thinking å‚æ•°ï¼›OCR ä»»åŠ¡é»˜è®¤å…³é—­ä»¥é™ä½å»¶è¿Ÿ
        if crate::llm_manager::adapters::zhipu::ZhipuAdapter::supports_thinking_static(&config.model) {
            let enable = self.is_ocr_thinking_enabled();
            if let Some(obj) = request_body.as_object_mut() {
                obj.insert(
                    "thinking".to_string(),
                    json!({ "type": if enable { "enabled" } else { "disabled" } }),
                );
            }
        }

        debug!(
            "[OCR_MODEL_RAW_PROMPT] å‘é€è¯·æ±‚åˆ°: {} | æ¨¡å‹: {} | è¯·æ±‚å›¾ç‰‡æ•°: {} | å®é™…é™„åŠ : {} | promptå­—ç¬¦æ•°: {}",
            config.base_url,
            config.model,
            requested_image_count,
            attached_payloads.len(),
            user_prompt.chars().count()
        );

        // 4. é€šè¿‡ ProviderAdapter æ„é€  HTTP è¯·æ±‚
        let adapter: Box<dyn ProviderAdapter> = match config.model_adapter.as_str() {
            "google" | "gemini" => Box::new(crate::providers::GeminiAdapter::new()),
            "anthropic" | "claude" => Box::new(crate::providers::AnthropicAdapter::new()),
            _ => Box::new(crate::providers::OpenAIAdapter),
        };
        let preq = adapter
            .build_request(
                &config.base_url,
                &config.api_key,
                &config.model,
                &request_body,
            )
            .map_err(|e| Self::provider_error("OCR RAW prompt è¯·æ±‚æ„å»ºå¤±è´¥", e))?;

        log_llm_request_audit("OCR_RAW", &preq.url, &config.model, &request_body);

        let mut request_builder = self
            .client
            .post(&preq.url)
            .header("Accept", "application/json")
            .header("Accept-Encoding", "identity");

        for (k, v) in preq.headers {
            request_builder = request_builder.header(k, v);
        }

        // 5. å‘é€è¯·æ±‚
        let response = request_builder
            .json(&preq.body)
            .send()
            .await
            .map_err(|e| AppError::network(format!("OCR_MODEL APIè¯·æ±‚å¤±è´¥: {}", e)))?;

        // 6. æ£€æŸ¥å“åº”çŠ¶æ€
        if !response.status().is_success() {
            let status = response.status();
            let error_text = response.text().await.unwrap_or_default();
            return Err(AppError::llm(format!(
                "OCR_MODEL APIè¯·æ±‚å¤±è´¥: {} - {}",
                status, error_text
            )));
        }

        // 7. è§£æå“åº”
        let response_json: serde_json::Value = response
            .json()
            .await
            .map_err(|e| AppError::llm(format!("è§£æOCR_MODELå“åº”å¤±è´¥: {}", e)))?;

        // Gemini éæµå¼å“åº”ç»Ÿä¸€è½¬æ¢ä¸º OpenAI å½¢çŠ¶
        let openai_like_json = if config.model_adapter == "google" {
            if let Some(safety_msg) = Self::extract_gemini_safety_error(&response_json) {
                return Err(AppError::llm(safety_msg));
            }
            match crate::adapters::gemini_openai_converter::convert_gemini_nonstream_response_to_openai(&response_json, &config.model) {
                Ok(v) => v,
                Err(e) => return Err(AppError::llm(format!("Geminiå“åº”è½¬æ¢å¤±è´¥: {}", e))),
            }
        } else if matches!(config.model_adapter.as_str(), "anthropic" | "claude") {
            crate::providers::convert_anthropic_response_to_openai(&response_json, &config.model)
                .ok_or_else(|| AppError::llm("è§£æAnthropicå“åº”å¤±è´¥".to_string()))?
        } else {
            response_json.clone()
        };

        let assistant_message = openai_like_json["choices"][0]["message"]["content"]
            .as_str()
            .unwrap_or("")
            .to_string();

        Ok(StandardModel2Output {
            assistant_message,
            raw_response: Some(openai_like_json.to_string()),
            chain_of_thought_details: None,
            cancelled: false,
        })
    }

    /// å•å¼ å›¾ç‰‡è½¬ Markdown æ–‡æœ¬ï¼ˆå¤ç”¨ DeepSeek-OCR é…ç½®ï¼‰
    /// ç¿»è¯‘åœºæ™¯ä½¿ç”¨ Free OCR æ¨¡å¼ï¼Œæ— éœ€è¾“å‡ºåæ ‡ï¼ˆé¢˜ç›®é›†è¯†åˆ«ä½¿ç”¨ grounding æ¨¡å¼ï¼‰
    ///
    /// âš ï¸ DEPRECATED: æ‰€æœ‰è°ƒç”¨è€…å·²è¿ç§»åˆ° `call_ocr_free_text_with_fallback`ï¼ˆå¸¦ fallback + è¶…æ—¶ + ç†”æ–­ï¼‰ã€‚
    /// æœ¬æ–¹æ³•ä¿ç•™ä»…ä¾›å…¼å®¹ï¼Œæ–°ä»£ç è¯·å‹¿ä½¿ç”¨ã€‚
    #[allow(dead_code)]
    pub async fn convert_image_to_markdown(&self, image_path: &str) -> Result<String> {
        let config = self.get_exam_segmentation_model_config().await?;
        let api_key = self.decrypt_api_key_if_needed(&config.api_key)?;

        let mime = Self::infer_image_mime(image_path);
        let (data_url, _) = self
            .prepare_segmentation_image_data(image_path, mime)
            .await?;

        let prompt_text = "Free OCR.";
        let messages = vec![json!({
            "role": "user",
            "content": [
                { "type": "image_url", "image_url": { "url": data_url, "detail": "high" } },
                { "type": "text", "text": prompt_text }
            ]
        })];

        // OCR ä»»åŠ¡ï¼šå…ˆåº”ç”¨ä¾›åº”å•†é™åˆ¶ï¼Œå†åº”ç”¨ OCR ä¸“ç”¨çš„ [2048, 8000] èŒƒå›´
        let max_tokens = effective_max_tokens(config.max_output_tokens, config.max_tokens_limit)
            .max(2048)
            .min(8000);
        let request_body = json!({
            "model": config.model,
            "messages": messages,
            "temperature": 0.0,
            "max_tokens": max_tokens,
            "stream": false,
        });

        let adapter: Box<dyn ProviderAdapter> = match config.model_adapter.as_str() {
            "google" | "gemini" => Box::new(crate::providers::GeminiAdapter::new()),
            "anthropic" | "claude" => Box::new(crate::providers::AnthropicAdapter::new()),
            _ => Box::new(crate::providers::OpenAIAdapter),
        };

        let preq = adapter
            .build_request(&config.base_url, &api_key, &config.model, &request_body)
            .map_err(|e| Self::provider_error("OCRè¯·æ±‚æ„å»ºå¤±è´¥", e))?;

        log_llm_request_audit("OCR_PAGES", &preq.url, &config.model, &request_body);

        let mut header_map = reqwest::header::HeaderMap::new();
        for (k, v) in preq.headers.iter() {
            if let (Ok(name), Ok(val)) = (
                reqwest::header::HeaderName::from_bytes(k.as_bytes()),
                reqwest::header::HeaderValue::from_str(v),
            ) {
                header_map.insert(name, val);
            }
        }

        let response = self
            .client
            .post(&preq.url)
            .headers(header_map)
            .json(&preq.body)
            .send()
            .await
            .map_err(|e| AppError::llm(format!("OCRè¯·æ±‚å¤±è´¥: {}", e)))?;

        if !response.status().is_success() {
            let status = response.status();
            let error_text = response.text().await.unwrap_or_default();
            return Err(AppError::llm(format!(
                "OCR APIè¿”å›é”™è¯¯ {}: {}",
                status, error_text
            )));
        }

        let response_text = response
            .text()
            .await
            .map_err(|e| AppError::llm(format!("è¯»å–OCRå“åº”å¤±è´¥: {}", e)))?;

        let response_json: Value = serde_json::from_str(&response_text).map_err(|e| {
            AppError::llm(format!(
                "è§£æOCRå“åº”JSONå¤±è´¥: {}, åŸå§‹å†…å®¹: {}",
                e, response_text
            ))
        })?;

        response_json["choices"][0]["message"]["content"]
            .as_str()
            .ok_or_else(|| AppError::llm("OCRæ¨¡å‹è¿”å›å†…å®¹ä¸ºç©º"))
            .map(|s| s.to_string())
    }

    /// ä» API è¿”å›çš„ usage æ•°æ®ä¸­æå–å®é™… token æ•°é‡
    ///
    /// æ”¯æŒå¤šç§æ ¼å¼ï¼š
    /// - OpenAI: prompt_tokens, completion_tokens
    /// - Anthropic: input_tokens, output_tokens
    /// - Gemini: promptTokenCount, candidatesTokenCount, thoughtsTokenCount
    ///
    /// å¦‚æœ API æ²¡æœ‰è¿”å› usage æ•°æ®ï¼Œåˆ™ä½¿ç”¨ä¼°ç®—å€¼ä½œä¸º fallback
    fn extract_usage_tokens(
        usage: &Option<serde_json::Value>,
        fallback_completion_tokens: usize,
        fallback_prompt_tokens: usize,
    ) -> (u32, u32, Option<u32>) {
        if let Some(usage_value) = usage {
            // æå– prompt_tokensï¼ˆè¾“å…¥ï¼‰
            // å¦‚æœ API è¿”å› 0 æˆ–æœªè¿”å›ï¼Œå°è¯•ä» total_tokens - completion_tokens æ¨ç®—
            let raw_prompt = usage_value
                .get("prompt_tokens")
                .or_else(|| usage_value.get("input_tokens"))
                .or_else(|| usage_value.get("promptTokenCount"))
                .and_then(|v| v.as_u64())
                .unwrap_or(0) as u32;

            // æå– completion_tokensï¼ˆè¾“å‡ºï¼‰
            let completion_tokens = usage_value
                .get("completion_tokens")
                .or_else(|| usage_value.get("output_tokens"))
                .or_else(|| usage_value.get("candidatesTokenCount"))
                .and_then(|v| v.as_u64())
                .unwrap_or(fallback_completion_tokens as u64)
                as u32;

            // å¦‚æœ prompt_tokens ä¸º 0 ä½†æœ‰ total_tokensï¼Œå°è¯•æ¨ç®—
            let prompt_tokens = if raw_prompt == 0 {
                let total = usage_value
                    .get("total_tokens")
                    .or_else(|| usage_value.get("totalTokenCount"))
                    .and_then(|v| v.as_u64())
                    .unwrap_or(0) as u32;
                if total > completion_tokens {
                    total - completion_tokens
                } else {
                    fallback_prompt_tokens as u32
                }
            } else {
                raw_prompt
            };

            // æå– reasoning_tokensï¼ˆæ€ç»´é“¾ï¼Œå¯é€‰ï¼‰
            let reasoning_tokens = usage_value
                .get("reasoning_tokens")
                .or_else(|| usage_value.get("thoughtsTokenCount"))
                .and_then(|v| v.as_u64())
                .map(|v| v as u32);

            debug!(
                "[LLM Usage] ä» API æå–: prompt={}, completion={}, reasoning={:?}",
                prompt_tokens, completion_tokens, reasoning_tokens
            );

            (prompt_tokens, completion_tokens, reasoning_tokens)
        } else {
            // æ²¡æœ‰ API usage æ•°æ®ï¼Œä½¿ç”¨ä¼°ç®—å€¼
            let estimated_prompt = fallback_prompt_tokens as u32;
            debug!(
                "[LLM Usage] API æœªè¿”å› usageï¼Œä½¿ç”¨ä¼°ç®—å€¼: prompt={}, completion={}",
                estimated_prompt, fallback_completion_tokens
            );
            (estimated_prompt, fallback_completion_tokens as u32, None)
        }
    }
}
// è·å–é€šç”¨Promptæ¨¡æ¿ï¼ˆsubject å·²åºŸå¼ƒï¼‰
impl LLMManager {
    pub fn get_subject_prompt(&self, _subject: &str, task_type: &str) -> String {
        // subject å·²åºŸå¼ƒï¼Œç›´æ¥ä½¿ç”¨é€šç”¨æç¤ºè¯
        self.get_fallback_prompt(task_type)
    }

    // é€šç”¨æç¤ºè¯
    fn get_fallback_prompt(&self, task_type: &str) -> String {
        match task_type {
            "ocr" | "classification" => {
                "ä½ æ˜¯ä¸€ä¸ªé¢˜ç›®åˆ†æä¸“å®¶ã€‚è¯·è¯†åˆ«å›¾ç‰‡ä¸­çš„é¢˜ç›®æ–‡å­—å†…å®¹ï¼Œå¹¶åˆ†æé¢˜ç›®ç±»å‹å’Œç›¸å…³çŸ¥è¯†ç‚¹æ ‡ç­¾ã€‚\n\nã€é‡è¦ã€‘OCRæ–‡æœ¬æå–è¦æ±‚ï¼š\n1. æå–çº¯æ–‡æœ¬å†…å®¹ï¼Œä¸è¦ä½¿ç”¨LaTeXæ ¼å¼\n2. æ•°å­¦å…¬å¼ç”¨æ™®é€šæ–‡å­—æè¿°\n3. ä¿æŒæ–‡æœ¬ç®€æ´æ˜“è¯»\n4. é¿å…ä½¿ç”¨ç‰¹æ®ŠLaTeXå‘½ä»¤\n\nè¯·ä»¥JSONæ ¼å¼è¿”å›ç»“æœï¼š{{\"ocr_text\": \"é¢˜ç›®æ–‡å­—\", \"tags\": [\"æ ‡ç­¾1\", \"æ ‡ç­¾2\"], \"mistake_type\": \"é¢˜ç›®ç±»å‹\"}}".to_string()
            }
            "model2" | "analysis" => {
                "ä½ æ˜¯ä¸€ä¸ªæ•™å­¦ä¸“å®¶ã€‚è¯·ä»”ç»†åˆ†æè¿™é“é”™é¢˜ï¼Œæä¾›è¯¦ç»†çš„è§£é¢˜æ€è·¯å’ŒçŸ¥è¯†ç‚¹è®²è§£ã€‚\n\nã€é‡è¦ã€‘å…¬å¼æ ¼å¼è¦æ±‚ï¼ˆKaTeX å…¼å®¹ï¼‰:\n1. è¡Œå†…å…¬å¼ä½¿ç”¨ $...$ï¼›å—çº§å…¬å¼ä½¿ç”¨ $$...$$ï¼Œåˆ†éš”ç¬¦å¿…é¡»æˆå¯¹é—­åˆã€‚\n2. åˆ†æ•°ä¸€å¾‹ä½¿ç”¨ \\frac{{åˆ†å­}}{{åˆ†æ¯}}ï¼›ç¦æ­¢ä½¿ç”¨ \\over/\\atop/\\chooseã€‚\n3. æ ¹å·å¿…é¡»å†™æˆ \\sqrt{{...}}ï¼Œä¸è¦çœç•¥èŠ±æ‹¬å·ã€‚\n4. ä¸Šä¸‹æ ‡å¤šå­—ç¬¦éœ€åŠ èŠ±æ‹¬å·ã€‚\n5. ä¸­æ–‡/é ASCII æ–‡æœ¬ç½®äº \\text{{...}}ã€‚\n6. ä»…ä½¿ç”¨ KaTeX æ”¯æŒçš„å‘½ä»¤ã€‚".to_string()
            }
            "review" => {
                "ä½ æ˜¯ä¸€ä¸ªå­¦ä¹ åˆ†æä¸“å®¶ã€‚è¯·åˆ†æè¿™äº›é”™é¢˜çš„å…±åŒé—®é¢˜å’Œæ”¹è¿›å»ºè®®ã€‚".to_string()
            }
            "consolidated_review" => {
                "ä½ æ˜¯ä¸€ä¸ªå­¦ä¹ åˆ†æä¸“å®¶ã€‚è¯·å¯¹æä¾›çš„é”™é¢˜è¿›è¡Œç»¼åˆå¤ä¹ åˆ†æï¼ŒåŒ…æ‹¬çŸ¥è¯†ç‚¹æ€»ç»“ã€å¸¸è§é”™è¯¯æ¨¡å¼è¯†åˆ«å’Œå­¦ä¹ å»ºè®®ã€‚".to_string()
            }
            "chat" => {
                "åŸºäºè¿™é“é¢˜ç›®ï¼Œè¯·å›ç­”å­¦ç”Ÿçš„é—®é¢˜ã€‚\n\nã€é‡è¦ã€‘å…¬å¼æ ¼å¼è¦æ±‚ï¼ˆKaTeX å…¼å®¹ï¼‰:\n1. è¡Œå†… $...$ã€å—çº§ $$...$$ï¼›ç¡®ä¿æˆå¯¹é—­åˆã€‚\n2. åˆ†æ•°ç”¨ \\frac{{åˆ†å­}}{{åˆ†æ¯}}ï¼›ç¦æ­¢ \\over/\\atop/\\chooseã€‚\n3. \\sqrt{{...}} ä¸å¾—çœç•¥èŠ±æ‹¬å·ã€‚\n4. ä¸­æ–‡æ–‡æœ¬æ”¾ \\text{{...}}ã€‚\n5. ä»…ç”¨ KaTeX æ”¯æŒæŒ‡ä»¤ã€‚".to_string()
            }
            "anki_generation" => {
                "è¯·æ ¹æ®ä»¥ä¸‹å­¦ä¹ å†…å®¹ï¼Œç”Ÿæˆé€‚åˆåˆ¶ä½œAnkiå¡ç‰‡çš„é—®é¢˜å’Œç­”æ¡ˆå¯¹ã€‚æ¯å¼ å¡ç‰‡åº”æµ‹è¯•ä¸€ä¸ªå•ä¸€çš„æ¦‚å¿µã€‚è¯·ä»¥JSONæ•°ç»„æ ¼å¼è¿”å›ç»“æœï¼Œæ¯ä¸ªå¯¹è±¡å¿…é¡»åŒ…å« \"front\" (å­—ç¬¦ä¸²), \"back\" (å­—ç¬¦ä¸²), \"tags\" (å­—ç¬¦ä¸²æ•°ç»„) ä¸‰ä¸ªå­—æ®µã€‚".to_string()
            }
            _ => {
                "è¯·æ ¹æ®æä¾›çš„é¢˜ç›®ä¿¡æ¯ï¼Œè¯¦ç»†è§£ç­”é—®é¢˜ã€‚".to_string()
            }
        }
    }

    /// ç”Ÿæˆ Anki å¡ç‰‡ - æ ¸å¿ƒåŠŸèƒ½
    pub async fn generate_anki_cards_from_document(
        &self,
        document_content: &str,
        subject_name: &str,
        options: Option<&crate::models::AnkiGenerationOptions>,
    ) -> Result<Vec<crate::models::AnkiCard>> {
        info!(
            "å¼€å§‹ç”Ÿæˆ Anki å¡ç‰‡: ç§‘ç›®={}, æ–‡æ¡£é•¿åº¦={}",
            subject_name,
            document_content.len()
        );

        // 1. è·å– Anki åˆ¶å¡æ¨¡å‹é…ç½®
        let config = self.get_anki_model_config().await?;

        // 2. è·å–ç§‘ç›®ç‰¹å®šçš„ Anki åˆ¶å¡ Prompt
        let subject_prompt = self.get_subject_prompt(subject_name, "anki_generation");

        // 3. æ„å»ºæœ€ç»ˆçš„AIæŒ‡ä»¤
        let final_prompt = format!("{}\n\næ–‡æ¡£å†…å®¹ï¼š\n{}", subject_prompt, document_content);

        // 4. å‡†å¤‡AIæ¨¡å‹è¯·æ±‚ï¼ˆåº”ç”¨ä¾›åº”å•†çº§åˆ«çš„ max_tokens é™åˆ¶ï¼‰
        let max_tokens = options
            .as_ref()
            .and_then(|opt| opt.max_tokens)
            .map(|v| effective_max_tokens(v, config.max_tokens_limit))
            .unwrap_or_else(|| {
                effective_max_tokens(config.max_output_tokens, config.max_tokens_limit)
            });
        let temperature = options
            .as_ref()
            .and_then(|opt| opt.temperature)
            .unwrap_or(0.3);

        let mut request_body = json!({
            "model": config.model,
            "messages": [
                {
                    "role": "user",
                    "content": final_prompt
                }
            ],
            "max_tokens": max_tokens,
            "temperature": temperature
        });

        Self::apply_reasoning_config(&mut request_body, &config, None);

        // å¦‚æœæ”¯æŒJSONæ¨¡å¼ï¼Œæ·»åŠ response_format
        if config.model.starts_with("gpt-") {
            request_body["response_format"] = json!({"type": "json_object"});
        }

        debug!("å‘é€ Anki åˆ¶å¡è¯·æ±‚åˆ°: {} (ç»é€‚é…å™¨)", config.base_url);

        // 5. é€šè¿‡ ProviderAdapter å‘é€HTTPè¯·æ±‚ï¼ˆæ”¯æŒ Gemini ä¸­è½¬ï¼‰
        let adapter: Box<dyn ProviderAdapter> = match config.model_adapter.as_str() {
            "google" | "gemini" => Box::new(crate::providers::GeminiAdapter::new()),
            "anthropic" | "claude" => Box::new(crate::providers::AnthropicAdapter::new()),
            _ => Box::new(crate::providers::OpenAIAdapter),
        };
        let preq = adapter
            .build_request(
                &config.base_url,
                &config.api_key,
                &config.model,
                &request_body,
            )
            .map_err(|e| Self::provider_error("Anki åˆ¶å¡è¯·æ±‚æ„å»ºå¤±è´¥", e))?;

        log_llm_request_audit("ANKI_CARD", &preq.url, &config.model, &request_body);

        let mut request_builder = self.client
            .post(&preq.url)
            .header("Accept", "text/event-stream, application/json, text/plain, */*")
            .header("Accept-Encoding", "identity")  // ç¦ç”¨å‹ç¼©ï¼Œé¿å…äºŒè¿›åˆ¶å“åº”
            .header("Accept-Language", "zh-CN,zh;q=0.9,en;q=0.8")
            .header("User-Agent", "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36");
        for (k, v) in preq.headers {
            request_builder = request_builder.header(k, v);
        }

        if let Ok(parsed_url) = Url::parse(&config.base_url) {
            if (parsed_url.scheme() == "http" || parsed_url.scheme() == "https")
                && parsed_url.host_str().is_some()
            {
                let origin_val = format!(
                    "{}://{}",
                    parsed_url.scheme(),
                    parsed_url.host_str().unwrap_or_default()
                );
                let referer_val = format!(
                    "{}://{}/",
                    parsed_url.scheme(),
                    parsed_url.host_str().unwrap_or_default()
                );
                request_builder = request_builder
                    .header("Origin", origin_val)
                    .header("Referer", referer_val);
            }
        }

        let response = request_builder.json(&preq.body).send().await.map_err(|e| {
            let error_msg = if e.to_string().contains("timed out") {
                format!("Ankiåˆ¶å¡APIè¯·æ±‚è¶…æ—¶: {}", e)
            } else if e.to_string().contains("connect") {
                format!("æ— æ³•è¿æ¥åˆ° Anki åˆ¶å¡ API æœåŠ¡å™¨: {}", e)
            } else {
                format!("Ankiåˆ¶å¡APIè¯·æ±‚å¤±è´¥: {}", e)
            };
            AppError::network(error_msg)
        })?;

        // 6. å¤„ç†HTTPå“åº”
        if !response.status().is_success() {
            let status = response.status();
            let error_text = response.text().await.unwrap_or_default();
            return Err(AppError::llm(format!(
                "Ankiåˆ¶å¡APIè¯·æ±‚å¤±è´¥: {} - {}",
                status, error_text
            )));
        }

        let response_json: Value = response
            .json()
            .await
            .map_err(|e| AppError::llm(format!("è§£æ Anki åˆ¶å¡å“åº”å¤±è´¥: {}", e)))?;

        // Gemini éæµå¼å“åº”ç»Ÿä¸€è½¬æ¢ä¸º OpenAI å½¢çŠ¶
        let openai_like_json = if config.model_adapter == "google" {
            // éæµå¼ï¼šå…ˆæ£€æµ‹å®‰å…¨é˜»æ–­
            if let Some(safety_msg) = Self::extract_gemini_safety_error(&response_json) {
                return Err(AppError::llm(safety_msg));
            }
            match crate::adapters::gemini_openai_converter::convert_gemini_nonstream_response_to_openai(&response_json, &config.model) {
                Ok(v) => v,
                Err(e) => return Err(AppError::llm(format!("Geminiå“åº”è½¬æ¢å¤±è´¥: {}", e))),
            }
        } else if matches!(config.model_adapter.as_str(), "anthropic" | "claude") {
            crate::providers::convert_anthropic_response_to_openai(&response_json, &config.model)
                .ok_or_else(|| AppError::llm("è§£æAnthropicå“åº”å¤±è´¥".to_string()))?
        } else {
            response_json.clone()
        };

        // 7. æå–AIç”Ÿæˆçš„å†…å®¹
        let content_str = openai_like_json["choices"][0]["message"]["content"]
            .as_str()
            .ok_or_else(|| AppError::llm("æ— æ³•è§£æ Anki åˆ¶å¡ API å“åº”"))?;

        // éšç§ä¿æŠ¤ï¼šä»…è®°å½•å“åº”é•¿åº¦ï¼Œä¸æ‰“å°åŸå§‹å†…å®¹
        debug!("Anki åˆ¶å¡å“åº”é•¿åº¦: {} å­—ç¬¦", content_str.len());

        // 8. æ¸…ç†å’Œè§£æAIè¿”å›çš„JSONæ•°æ®
        let cleaned_content = self.clean_anki_json_response(content_str)?;
        debug!("æ¸…ç†åçš„JSONé•¿åº¦: {} å­—ç¬¦", cleaned_content.len());

        // 9. ååºåˆ—åŒ–ä¸ºAnkiCardå‘é‡ï¼ˆå¸¦å®¹é”™å¤„ç†ï¼‰
        let cards: Vec<crate::models::AnkiCard> =
            self.parse_anki_cards_with_fallback(&cleaned_content, content_str)?;

        info!("æˆåŠŸç”Ÿæˆ {} å¼ ANKIå¡ç‰‡", cards.len());
        Ok(cards)
    }

    /// æ¸…ç†AIè¿”å›çš„ANKIå¡ç‰‡JSONå“åº”
    fn clean_anki_json_response(&self, content: &str) -> Result<String> {
        let mut cleaned = content.trim().to_string();

        // ç§»é™¤markdownä»£ç å—
        cleaned = regex::Regex::new(r"```(?:json)?\s*")
            .unwrap()
            .replace_all(&cleaned, "")
            .to_string();
        cleaned = regex::Regex::new(r"```\s*$")
            .unwrap()
            .replace_all(&cleaned, "")
            .to_string();

        // ç§»é™¤å¸¸è§å‰ç¼€
        let prefixes = [
            "ä»¥ä¸‹æ˜¯ç”Ÿæˆçš„Ankiå¡ç‰‡ï¼š",
            "Ankiå¡ç‰‡ï¼š",
            "JSONç»“æœï¼š",
            "å¡ç‰‡æ•°æ®ï¼š",
            "Here are the Anki cards:",
            "Cards:",
            "JSON:",
            "Result:",
        ];

        for prefix in &prefixes {
            if cleaned.starts_with(prefix) {
                cleaned = cleaned
                    .strip_prefix(prefix)
                    .unwrap_or(&cleaned)
                    .trim()
                    .to_string();
                break;
            }
        }

        // ç¡®ä¿æ˜¯æœ‰æ•ˆçš„JSONæ•°ç»„æ ¼å¼
        if !cleaned.starts_with('[') {
            // å°è¯•æ‰¾åˆ°ç¬¬ä¸€ä¸ª'['
            if let Some(start) = cleaned.find('[') {
                cleaned = cleaned[start..].to_string();
            } else {
                return Err(AppError::llm("æ— æ³•æ‰¾åˆ°JSONæ•°ç»„å¼€å§‹æ ‡è®°"));
            }
        }

        if !cleaned.ends_with(']') {
            // å°è¯•æ‰¾åˆ°æœ€åä¸€ä¸ª']'
            if let Some(end) = cleaned.rfind(']') {
                cleaned = cleaned[..=end].to_string();
            } else {
                return Err(AppError::llm("æ— æ³•æ‰¾åˆ°JSONæ•°ç»„ç»“æŸæ ‡è®°"));
            }
        }

        Ok(cleaned)
    }

    /// è§£æANKIå¡ç‰‡JSONï¼Œå¸¦å®¹é”™å¤„ç†ï¼ˆè‡ªåŠ¨è¡¥å……ç¼ºå¤±çš„imageså­—æ®µå’Œå…¼å®¹questionå­—æ®µï¼‰
    fn parse_anki_cards_with_fallback(
        &self,
        json_str: &str,
        original_content: &str,
    ) -> Result<Vec<crate::models::AnkiCard>> {
        // å°è¯•å°†JSONå­—ç¬¦ä¸²è§£æä¸ºé€šç”¨çš„Valueæ•°ç»„
        let mut card_values: Vec<Value> = match serde_json::from_str(json_str) {
            Ok(v) => v,
            Err(e) => {
                // å¦‚æœè¿åŸºæœ¬JSONéƒ½è§£æä¸äº†ï¼Œç›´æ¥è¿”å›é”™è¯¯
                return Err(AppError::llm(format!(
                    "è§£æANKIå¡ç‰‡JSONå¤±è´¥: {} - åŸå§‹å†…å®¹: {}",
                    e, original_content
                )));
            }
        };

        // éå†æ¯ä¸ªå¡ç‰‡å¯¹è±¡ï¼Œè¿›è¡Œå­—æ®µå…¼å®¹æ€§å¤„ç†
        for card_value in &mut card_values {
            if let Some(obj) = card_value.as_object_mut() {
                // å…¼å®¹ "question" å­—æ®µ -> "front"
                if obj.contains_key("question") && !obj.contains_key("front") {
                    if let Some(question_val) = obj.remove("question") {
                        obj.insert("front".to_string(), question_val);
                    }
                }
                // è‡ªåŠ¨è¡¥å……ç¼ºå¤±çš„ "images" å­—æ®µ
                if !obj.contains_key("images") {
                    obj.insert("images".to_string(), json!([]));
                }
            }
        }

        // å°†å¤„ç†è¿‡çš„Valueè½¬æ¢å›JSONå­—ç¬¦ä¸²
        let processed_json_str = match serde_json::to_string(&card_values) {
            Ok(s) => s,
            Err(e) => return Err(AppError::llm(format!("é‡æ–°åºåˆ—åŒ–å¡ç‰‡æ•°æ®å¤±è´¥: {}", e))),
        };

        // ä½¿ç”¨å¤„ç†è¿‡çš„JSONå­—ç¬¦ä¸²è¿›è¡Œæœ€ç»ˆçš„ååºåˆ—åŒ–
        match serde_json::from_str::<Vec<crate::models::AnkiCard>>(&processed_json_str) {
            Ok(cards) => Ok(cards),
            Err(e) => {
                // å¦‚æœä»ç„¶å¤±è´¥ï¼Œè¯´æ˜æœ‰å…¶ä»–ç»“æ„é—®é¢˜
                Err(AppError::llm(format!(
                    "æœ€ç»ˆè§£æANKIå¡ç‰‡å¤±è´¥: {} - å¤„ç†åJSON: {}",
                    e, processed_json_str
                )))
            }
        }
    }

    fn should_use_openai_responses(&self, config: &ApiConfig) -> bool {
        if config.model_adapter != "general" {
            return false;
        }
        if !(config.is_reasoning || config.supports_reasoning) {
            return false;
        }
        let lower = config.model.to_lowercase();
        lower.contains("o1") || lower.contains("o3") || lower.contains("gpt-5")
    }
}
